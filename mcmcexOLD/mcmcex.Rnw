\documentclass{article}
\usepackage{graphicx, color, amssymb, amsmath, bm, rotating, graphics,
epsfig, multicol}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{caption}
\begin{document}

% \SweaveOpts{fig.path='figure/', fig.align='center', fig.show='hold'}

<<setup, include=FALSE, cache=FALSE>>=
options(replace.assign=TRUE,width=90)
@


\title{MCMC Examples for DLMs}


\author{Matt Simpson}

\maketitle

\section{Introduction}

In this note, I compare the performance of two samplers
for exploring the posterior distribution of a local level state space
model. Suppose we have a univariate time series $y_t$ for
$t=1,2,...,T$. The local level model says
\begin{align*}
  y_t &= \theta_t + v_t\\
  \theta_t & = \theta_{t-1} + w_t
\end{align*}
for $t=1,2,...,T$ with
\[
\begin{bmatrix} v_t \\ w_t \end{bmatrix} \stackrel{iid}{\sim}
N\left(\begin{bmatrix} 0\\0\end{bmatrix}, \begin{bmatrix} V & 0 \\ 0 &
    W \end{bmatrix}\right)
\]
and
\[
\theta_0\sim N(m_0, C_0)
\]

$V$ and $W$ are treated as unkown, and we put independent inverse gamma
priors on each, i.e. $V\sim IG(\alpha_1, \beta_1)$ and $W\sim
IG(\alpha_2, \beta_2)$. The goal, then, is to simulate from the posterior
distribution of $(V,W,\theta_{0:T} | y_{0:T})$.

\subsection{Algorithm 1: The State Sampler}

The standard algorithm for doing this, which I'm calling the ``state
sampler,'' relies on Forward Filtering Backward Sampling (FFBS). FFBS
is an algorithm for sampling from the latent states, $\theta_{0:T}$
conditional on any unknown parameters. The algorithm samples the
states condition on $(V,W)$, then samples $(V,W)$ conditional on the
states, hence the name. In detail, the algorithm is as follows.

\begin{enumerate}
  \item Simulate $\theta_{0:T}$ from $\pi(\theta_{0:T}|V,W,y_{1:T})$
    using FFBS
  \item Simulate $(V,W)$ from $\pi(V,W|\theta_{0:T},y_{1:T})$:
    $V$ and $W$ are conditionally independent (conditional on
    $\theta_{0:T},y_{1:T}$), with distributions
    \[
    V|\theta_{0:T},y_{1:T} \sim IG(a_V, b_V)
    \]
    \[
    W|\theta_{0:T},y_{1:T} \sim IG(a_W, b_W)
    \]
    where
    \begin{align*}
      a_V =& \alpha_1 + T/2\\
      b_V = & \beta_1 + \sum_{t=1}^T(y_t-\theta_t)^2/2
    \end{align*}
    and
    \begin{align*}
      a_W =& \alpha_2 + T/2\\
      b_W = & \beta_1 + \sum_{t=1}^T(\theta_t-\theta_{t-1})^2/2
    \end{align*}
\end{enumerate}

\subsection{Algorithm 2: The Scaled Disturbance Sampler}

A second algorithm worth considering  samples $V$ and $W$ conditional
on $\gamma_{0:T}$ instead of conditional on $\theta_{0:T}$,  with
$\gamma_t = (\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,...,T$ and
$\gamma_0=\theta_0$. Note that for $t>0$, $\gamma_t = w_t/\sqrt{W}$,
i.e. the $\gamma$'s are the disturbances from the system equation, but
scaled so that they have a $N(0,1)$ distribution a priori. The basic
idea here is to sample the scaled disturbances conditional on $(V,W)$,
then sample $V$ conditional on the scaled disturbances and $W$, then
$W$ conditional on the scaled disturbances and $V$. Sampling the
scaled disturbances can be accomplished using FFBS to sample the
states, then transform the states to the scaled disturbances using the
formulas above, or by sampling them directly using the disturbance
smoother. The full algorithm is as follows:

\begin{enumerate}
  \item Simulate $\theta_{0:T}$ from $\pi(\theta_{0:T}|V,W,y_{0:T})$
    using FFBS and form $\gamma_{0:T}$.
  \item Simulate $V$ from $\pi(V|\gamma_{0:T},W,y_{1:T})$:
    \[
    V|\gamma_{0:T},W,y_{1:T} \sim IG(a_V, b_V)
    \]
    where
    \begin{align*}
      a_V =& \alpha_1 + T/2\\
      b_V = & \beta_1 + \sum_{t=1}^T(y_t-\gamma_0 -
      \sqrt{W}\sum_{j=1}^t\gamma_j)^2/2
    \end{align*}
  \item Simulate $W$ from $\pi(W|\gamma_{0:T},V,y_{1:T})$:
    \[
    \mathcal{L} \equiv \log \pi(W|\gamma_{0:T},V,y_{1:T}) = -aW +
    b\sqrt{W} - (\alpha_2 + 1)\log W -\beta_2/W +C
    \]
    where $C$ is some constant we can ignore for sampling purposes,
    $a=\sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$ and
    $b=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$.
\end{enumerate}

Step 3 of this algorithm is of some interest since the distribution to
be simulated from isn't standard. One possibility (suggested by
Dr. Roy) is to use an adaptive rejection sampling algorithm that
depends on the density being log-concave. This doesn't always work
because the target density isn't log-concave in general (Dr. Roy made
a mistake confusing $W$ and $\sqrt{W}$ in his note that led him to
derive an incorrect conditional density for $W$ which was
log-concave. The correct density, here, is clearly not guaranteed to
be log-concave). When $W/V$, the signal-to-noise ratio, is above 1 the
density tends to be log-concave and adaptive rejection sampling works
well. When the signal-to-noise ratio is below 1, the density is often not
log concave. In this case a proposal density from the location-scale
family of $t$ distributions works, albeit with some additional
computational cost. In this case, setting the location parameter equal
to the target density's mode and the scale parameter equal to the
square root of the negative second derivative of the log of the target
density results in a family of proposal densities defined by the
degrees of freedom parameter, $df$. Then it's just a matter of
choosing $df$ to minimize the rejection rate.

\subsection{Algorithm 3: The Scaled Error Sampler}

A final sampler samples $V$ and $W$ conditional
on $\psi_{0:T}$ instead of conditional on $\theta_{0:T}$,  with
$\psi_t = (y_t - \theta_{t})/\sqrt{V}$ for $t=1,2,...,T$ and
$\psi_0=\theta_0$. Note that for $t>0$, $\psi_t = v_t/\sqrt{V}$,
i.e. the $\psi$'s are the errors from the observation equation, but
scaled so that they have a $N(0,1)$ distribution a priori. The basic
idea here is to sample the scaled errors conditional on $(V,W)$,
then sample $W$ conditional on the scaled errors and $V$, then
$V$ conditional on the scaled errors and $W$. Sampling the
scaled errors, like the scaled disturbances, can be accomplished using
FFBS to sample the states, then transform the states to the scaled
errors using the formulas above, or by sampling them directly using
the disturbance smoother. The full algorithm is as follows:

\begin{enumerate}
  \item Simulate $\theta_{0:T}$ from $\pi(\theta_{0:T}|V,W,y_{0:T})$
    using FFBS and form $\psi_{0:T}$.
  \item Simulate $W$ from $\pi(W|\psi_{0:T},V,y_{1:T})$:
    \[
    W|\psi_{0:T},V,y_{1:T} \sim IG(a_W, b_W)
    \]
    where
    \begin{align*}
      a_W =& \alpha_2 + T/2\\
      b_W = & \beta_2 + \sum_{t=1}^T(y_t- y_{t-1} -
      \sqrt{V}(\psi_t-\psi_{t-1}))^2/2
    \end{align*}
    (defining $y_0=0$)
  \item Simulate $V$ from $\pi(V|\psi_{0:T},W,y_{1:T})$:
    \[
    \mathcal{L} \equiv \log \pi(V|\psi_{0:T},W,y_{1:T}) = -aV +
    b\sqrt{V} - (\alpha_1 + 1)\log V -\beta_1/V +C
    \]
    where $C$ is some constant we can ignore for sampling purposes,
    $a=\sum_{t=1}^T(L\psi_t)^2/2W$ and
    $b=\sum_{t=1}^T(L\psi_tLy_t)/W$ where
    \begin{align*}
      L\psi_t &= \begin{cases} \psi_t - \psi_{t-1} &\text{for } t=2,3,...,T\\
                                \psi_1 & \text{for } t=1\end{cases}\\
      Ly_t & =\begin{cases} y_t - y_{t-1} &\text{for } t=2,3,...,T\\
                                y_1 - \psi_0 & \text{for } t=1\end{cases}\\
    \end{align*}
\end{enumerate}

Step 3 of this algorithm is again of some interest since the
distribution to be simulated from isn't standard. Notice that the
target density has the same form as the target in algorithm 2, so we
can use the same methods to directly simulate from it.

\section{Results}

<<plotsetup, echo=FALSE>>=
library(ggplot2)
library(scales)
load("scors.Rdata")
load("dcors.Rdata")
load("ecors.Rdata")
cnam <- colnames(scors[[2]][[1]])
rnam <- rownames(scors[[2]][[1]])
Vs <- c(.01, .1, 1, 10, 100, 1000)
Ws <- Vs
Ts <- c(10, 100, 1000)
cordat <- data.frame(AC=NULL, V=NULL, W=NULL, sampler=NULL, T=NULL, variable=NULL)
for(i in 1:3){
  cordat <- rbind(cordat,  data.frame(AC=c(scors[[2]][[i]]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="state", T=Ts[i], variable="V"))
  cordat <- rbind(cordat,  data.frame(AC=c(scors[[3]][[i]]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="state", T=Ts[i], variable="W"))
  cordat <- rbind(cordat,  data.frame(AC=c(dcors[[2]][[i]]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="dist", T=Ts[i], variable="V"))
  cordat <- rbind(cordat,  data.frame(AC=c(dcors[[3]][[i]]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="dist", T=Ts[i], variable="W"))
  cordat <- rbind(cordat,  data.frame(AC=c(ecors[[2]][[i]]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="error", T=Ts[i], variable="V"))
  cordat <- rbind(cordat,  data.frame(AC=c(ecors[[3]][[i]]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="error", T=Ts[i], variable="W"))
}
@


In order to test these algorithms, I simulated a fake dataset from
the local level model for various choices of $V$, $W$, and $T$ ---
i.e. all possible combinations of $V=0.01,\ 0.1,\ 1,\ 10,\ 100,\
1000$, $W=0.01,\ 0.1,\ 1,\ 10,\ 100,\ 1000$, and $T=10,\ 100,\
1000$. Then for each dataset, I fit the local level model using each
sampler. For both each sampler, I used the same priors: $m_0=0$,
$C_0=10^7$, $\alpha_1=\alpha_2=5$, $\beta_1=(\alpha_1-1)V$ and
$\beta_2=(\alpha_2-1)W$ where $(V,W)$ were the true values of $V$ and
$W$ used to simulate the time series. This ensures all priors and thus
the posterior are proper and results in independent inverse gamma
priors on $V$ and $W$ with prior means equal to the respective true
values and moderately large prior variances. For each sampler, I
obtained a sample of size $n=2000$ and threw away the first 500
samples as burn in. In the scaled disturbance sampler, I used adaptive
rejection sampling to sample $W$ when log-concavity was satisfied, and
the rejection sampling scheme with $t$ proposal mention above in all
other situations. Similarly, in the scaled error sampler, I used
adaptive rejection sampling to sample $V$ when it's density was
log-concave and the $t$-proposal rejection sampling scheme in all
other situations. Once the sampling was complete, I computed the
autocorrelation in the chain for each parameter, including each of the
$\theta$'s. The following tables show what I found.

\subsection{Results for $(V,W)$}

Figure \ref{corplotT10} shows the autocorrelation for $V$ and $W$ for
all three samplers in all of the fitted models for $T=10$. Note that
moving from the lower left corner of a plot to the upper right corner
(moving ``up a diagonal'') holds the signal-to-noise ratio ($W/V$)
constant, while moving from the upper left hand corner to the lower
right hand corner (moving ``down a diagonal'') decreases the
signal-to-noise ratio. We can see that for the state sampler when the
the signal-to-noise ratio is high, $V$ has rather high autocorrelation
in the posterior sampler while $W$ has very low autocorrelation. On
the other hand, when the signal to noise ratio is low, $V$ has low
autocorrelation while $W$ has high autocorrelation in the posterior
sampler. Neither of these
facts seem affected by the actual values of $W$ and $V$, just their
relative values, i.e. the signal-to-noise ratio. When the signal to
noise ratio is constant (moving up any diagonal in the a plot) the
first order autocorrelation for both $V$ and $W$ looks about constant
even while $W$ and $V$ are changing (by the same factor).  Note that
the autocorrelation is never all that bad even when it becomes high in
one of the chains --- never above $0.6$ or so.

The scaled disturbance sampler had significantly different
results. When the signal-to-noise ratio is low, autocorrelation is
low for both $V$ and $W$. However, whent he signal-to-noise ratio is
high, autocorrelation is high for both $V$ and $W$ --- albeit it gets
much worse for $W$ than for $V$, maxing out at $0.99$ and $0.6$ or
so respectively. Again for constant signal-to-noise ratio, the values
of $V$ and $W$ respectively don't seem to matter much for
autocorrelation. Only the signal-to-noise ratio matters.

Finally, the scaled error sampler is essentially the opposite of the
scaled disturbance sampler. When the signal-to-noise ratio is high,
autocorrelation is low for both $V$ and $W$ and when the ratio is
low, autocorrelation is high for both $V$ and $W$. The scaled error
sampler also seems to have bigger problems with $V$ than with $W$ ---
the autocorrelation gets as high as about 0.99 for $V$ but only about
0.65 for $W$.

Now we can look at Figure \ref{corplotT100} and see essentially the
same pattern when $T=100$. The state sampler has low autocorrelation for $W$ and
high autocorrelation for $V$ when the signal-to-noise ratio is high and
high autocorrelation for $W$ and low autocorrelation for $V$ when the
signal-to-noise ratio is low. Note, however, that this time the
autocorrelation problem becomes much worse when the signal-to-noise
ratio is significantly different from 1 - reaching to about $0.9$ in
the worst cases.

The scaled disturbance sampler is also repeating the
same pattern --- low signal-to-noise ratio begets low autocorrelation
in both $V$ and $W$, while high signal-to-noise ratio begets high
autocorrelation in both $V$ and $W$. However now the autocorrelation
for $V$ now gets as high as $0.94$ while the autocorrelation for $W$
still gets to about $0.99$, but it gets there for somewhat lower
(farther away from 1) signal-to-noise ratios than for the smaller
value of $T$. The scaled error sampler once again has the opposite
pattern with high autocorrelation for a low signal-to-noise ratio and low
autocorrelation for a high signal-to-noise ratio. Similar to the scaled
disturbance sampler, the scaled error sampler also has worse
autocorelation problems when the problems do arise under the longer
time series. The autocorrelation for $V$ gets as high as $0.99$ again
while the autocorrelation for $W$ gets up to $0.94$ is the worst
case.

Finally in Figure \ref{corplotT1000} we see the same pattern repeated
again for $T=1000$. The trend also seems to continue with respect to
the length of the time series --- as $T$ increases, problems with
autocorrelation only become worse. The error and disturbance samplers
continue to be exact opposites --- though note that the scaled error
sampler appears to have a slightly larger range of the parameter space
under which it has acceptable autocorrelation than the scaled
disturbance sampler (compare the top right corners of the error
sampler tables with the bottom left corners of the disturbance sampler
tables).

To sum up the results for $V$ and $W$, the state sampler is good for
$V$ when the signal-to-noise ratio is low; good for $W$ when the
signal-to-noise ratio is high; and good for $(V,W)$ when the
signal-to-noise ratio is near 1. The scaled disturbance sampler is
good for $(V,W)$ or either individually when the signal-to-noise ratio
is low and the scaled error sampler is good for $(V,W)$ or either
individually when the signal-to-noise ratio is high. Increasing $T$,
the length of the time series, only exacerbates autocorrelation
problems and restricts further the range of the parameter space where
any of the samplers work well.

\begin{figure}[htb]
  \centering
<<corplotT10, echo=FALSE>>=
ggplot(data=cordat[cordat$T==10,], aes(x=V, y=W, fill=AC)) + #$
    geom_tile() +
    scale_fill_gradient2("Corr", low=muted("blue"), high=muted("red"),
                         limits=c(-1,1), mid="white",
                         guide=guide_colorbar(barheight=20)) +
    facet_grid(sampler~variable, scales="free", labeller=label_both) +
    scale_x_log10("V = noise", breaks=Vs) + scale_y_log10("W = signal", breaks=Vs) +
    ggtitle("Autocorrelation for V and W, T=10") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=10$ for $V$ and $W$ and for the state, scaled
  disturbance, and scaled error samplers. $X$ and $Y$ axes
  indicate the true values of $V$ and $W$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  moving up any diagonal. In the upper left the signal is high, in the
  lower right the noise is high.}
\label{corplotT10}
\end{figure}

\begin{figure}[htb]
  \centering
<<corplotT100, echo=FALSE>>=
ggplot(data=cordat[cordat$T==100,], aes(x=V, y=W, fill=AC)) + #$
    geom_tile() +
    scale_fill_gradient2("Corr", low=muted("blue"), high=muted("red"),
                         limits=c(-1,1), mid="white",
                         guide=guide_colorbar(barheight=20)) +
    facet_grid(sampler~variable, scales="free", labeller=label_both) +
    scale_x_log10("V = noise", breaks=Vs) + scale_y_log10("W = signal", breaks=Vs) +
    ggtitle("Autocorrelation for V and W, T=100") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=100$ for $V$ and $W$ and for the state, scaled
  disturbance, and scaled error samplers. $X$ and $Y$ axes
  indicate the true values of $V$ and $W$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  moving up any diagonal. In the upper left the signal is high, in the
  lower right the noise is high.}
\label{corplotT100}
\end{figure}

\begin{figure}[htb]
  \centering
<<corplotT1000, echo=FALSE>>=
ggplot(data=cordat[cordat$T==1000,], aes(x=V, y=W, fill=AC)) + #$
    geom_tile() +
    scale_fill_gradient2("Corr", low=muted("blue"), high=muted("red"),
                         limits=c(-1,1), mid="white",
                         guide=guide_colorbar(barheight=20)) +
    facet_grid(sampler~variable, scales="free", labeller=label_both) +
    scale_x_log10("V = noise", breaks=Vs) + scale_y_log10("W = signal", breaks=Vs) +
    ggtitle("Autocorrelation for V and W, T=1000") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=1000$ for $V$ and $W$ and for the state, scaled
  disturbance, and scaled error samplers. $X$ and $Y$ axes
  indicate the true values of $V$ and $W$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  moving up any diagonal. In the upper left the signal is high, in the
  lower right the noise is high.}
\label{corplotT1000}
\end{figure}




\subsection{Results for the $\theta$'s}
<<thetcor, echo=FALSE>>=
mscor <- round(max(scors[[1]][[1]], scors[[1]][[2]], scors[[1]][[3]]),2)
mdcor <- round(max(dcors[[1]][[1]], dcors[[1]][[2]], dcors[[1]][[3]]),2)
mecor <- round(max(ecors[[1]][[1]], ecors[[1]][[2]], ecors[[1]][[3]]),2)

thetacordat <- data.frame(AC=NULL, V=NULL, W=NULL, sampler=NULL, T=NULL, variable=NULL)
for(i in 1:3){
  thetacordat <- rbind(thetacordat, data.frame(AC=c(scors[[1]][[i]][,,1]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="state", T=Ts[i], variable="theta[0]"))
  thetacordat <- rbind(thetacordat, data.frame(AC=c(scors[[1]][[i]][,,Ts[i] + 1]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="state", T=Ts[i], variable="theta[T]"))
  thetacordat <- rbind(thetacordat, data.frame(AC=c(dcors[[1]][[i]][,,1]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="dist", T=Ts[i], variable="theta[0]"))
  thetacordat <- rbind(thetacordat, data.frame(AC=c(dcors[[1]][[i]][,,Ts[i] + 1]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="dist", T=Ts[i], variable="theta[T]"))
  thetacordat <- rbind(thetacordat, data.frame(AC=c(ecors[[1]][[i]][,,1]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="error", T=Ts[i], variable="theta[0]"))
  thetacordat <- rbind(thetacordat, data.frame(AC=c(ecors[[1]][[i]][,,Ts[i] + 1]), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="error", T=Ts[i], variable="theta[T]"))
}
library(plyr) ## needed for the following function to work

## passed into ggplot functions to make facet labels have variable name
## and math parsing.
label_both_parsed <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), function(x) parse(text = x))
}
@

We can do the same thing for some of the $\theta$'s. Tables
\ref{thetaplotT10}, \ref{thetaplotT100}, and \ref{thetaplotT1000}
contain the first order autocorrelation for $\theta_0$ and
$\theta_{T}$, for $T=10$, $T=100$, and $T=1000$ respectively. The key
thing we learn here is that autocorrelation in the chains for the
$\theta$'s doesn't appear to be a problem - it's always pretty low and
usually is essentially 0. I picked the first and last states of the
time series to illustrate this, but the result holds across all states
in all time series. The maximum autocorrelation for any of the states
in any of the fitted models for the the state sampler is
\Sexpr{mscor}. Likewise for the scaled disturbance sampler the maximum
autocorrelation is \Sexpr{mdcor} and the for scaled error sampler the
maximum autocorrelation is \Sexpr{mecor}. Autocorrelation for the
states doesn't seem to be a problem, at least for the local level
model.

\begin{figure}[htb]
  \centering
<<thetaplotT10, echo=FALSE>>=
ggplot(data=thetacordat[thetacordat$T==10,], aes(x=V, y=W, fill=AC)) + #$
    geom_tile() +
    scale_fill_gradient2("Corr", low=muted("blue"), high=muted("red"),
                         limits=c(-1,1), mid="white",
                         guide=guide_colorbar(barheight=20)) +
    facet_grid(sampler~variable, scales="free",
               labeller=label_both_parsed) +
    scale_x_log10("V = noise", breaks=Vs) +
    scale_y_log10("W = signal", breaks=Vs) +
    ggtitle(expression(paste("Autocorrelation for ", theta[0], " and ", theta[T], ", T=10"))) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=10$ for $\theta_0$ and $\theta_T$ and for the state, scaled
  disturbance, and scaled error samplers. $X$ and $Y$ axes
  indicate the true values of $V$ and $W$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  moving up any diagonal. In the upper left the signal is high, in the
  lower right the noise is high.}
\label{thetaplotT10}
\end{figure}

\begin{figure}[htb]
  \centering
<<thetaplotT100, echo=FALSE>>=
ggplot(data=thetacordat[thetacordat$T==100,], aes(x=V, y=W, fill=AC)) + #$
    geom_tile() +
    scale_fill_gradient2("Corr", low=muted("blue"), high=muted("red"),
                         limits=c(-1,1), mid="white",
                         guide=guide_colorbar(barheight=20)) +
    facet_grid(sampler~variable, scales="free",
               labeller=label_both_parsed) +
    scale_x_log10("V = noise", breaks=Vs) +
    scale_y_log10("W = signal", breaks=Vs) +
    ggtitle(expression(paste("Autocorrelation for ", theta[0], " and ", theta[T], ", T=100"))) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=100$ for $\theta_0$ and $\theta_T$ and for the state, scaled
  disturbance, and scaled error samplers. $X$ and $Y$ axes
  indicate the true values of $V$ and $W$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  moving up any diagonal. In the upper left the signal is high, in the
  lower right the noise is high.}
\label{thetaplotT100}
\end{figure}

\begin{figure}[htb]
  \centering
<<thetaplotT1000, echo=FALSE>>=
ggplot(data=thetacordat[thetacordat$T==1000,], aes(x=V, y=W, fill=AC)) + #$
    geom_tile() +
    scale_fill_gradient2("Corr", low=muted("blue"), high=muted("red"),
                         limits=c(-1,1), mid="white",
                         guide=guide_colorbar(barheight=20)) +
    facet_grid(sampler~variable, scales="free",
               labeller=label_both_parsed) +
    scale_x_log10("V = noise", breaks=Vs) +
    scale_y_log10("W = signal", breaks=Vs) +
    ggtitle(expression(paste("Autocorrelation for ", theta[0], " and ", theta[T], ", T=1000"))) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=1000$ for $\theta_0$ and $\theta_T$ and for the state, scaled
  disturbance, and scaled error samplers. $X$ and $Y$ axes
  indicate the true values of $V$ and $W$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  moving up any diagonal. In the upper left the signal is high, in the
  lower right the noise is high.}
\label{thetaplotT1000}
\end{figure}


\subsection{Implications for Interweaving}

There are 4 different possible interweaving algorithms available ---
interweaving any 2 of the 3 algorithms above or all
three. Interweaving the error and disturbance samplers is most
promising in terms of convergence since they have low autocorrelation
in exactly the opposite situations. However, as we'll see in the next
section, both samplers are rather slow so interweaving them might not
be very efficient. It may the case that each of the 3 available
interweaving algorithms is fastest to converge (or more efficient) for
different parameter values. In that case, there may be an algorithm
the somehow hybridizes the interweaving algorithms in order to come up
with something better. Another possibility worth noting is some sort
of 3-way interweaving algorithm.

It's worth noting that while the state sampler generalizes easily to
generic dynamic linear (gaussian) models, the the error sampler and
disturbance sampler do not necessarily. In order for the error sampler
to generalize, there has to be at least as many observation errors as
system disturbances, i.e. $dim(v_t)\geq dim(w_t)$. Similarly, in order
for the disturbance sampler to generalize, there has to be at least as
many system disturbances than observation errors, i.e. $dim(w_t)\geq
dim(v_t)$. This restricts the applicability of these two samplers and
interweaving algorithms using them somewhat, especially the error
sampler since in most use-cases $dim(w_t)\geq dim(v_t)$. However, both
the error sampler and the disturbance sampler can be extended to these
cases by appending a portion of the state vector to the scaled
errors/disturbances in order to make the transformation one-to-one (in
the same way that the initial state $\theta_0$ is appended for both
samplers) --- though this raises the question: {\it which} portion
should be appended?

\section{Computation Time}
<<computation, echo=FALSE>>=
load("stimes.Rdata")
load("dtimes.Rdata")
load("etimes.Rdata")
stimes <- stimes[[3]]
dtimes <- dtimes[[3]]
etimes <- etimes[[3]]
svtimes <- matrix(0,6,6)
colnames(svtimes) <- cnam
rownames(svtimes) <- rnam
swtimes <- svtimes
sstimes <- svtimes
dvtimes <- svtimes
dwtimes <- svtimes
dstimes <- svtimes
evtimes <- svtimes
ewtimes <- svtimes
estimes <- svtimes
darsprop <- svtimes
earsprop <- svtimes
for(i in 1:6){
  for(j in 1:6){
    stim <- apply(stimes[[i]][[j]], 2, sum)
    dtim <- apply(dtimes[[i]][[j]][,-4], 2, sum)
    etim <- apply(etimes[[i]][[j]][,-4], 2, sum)
    earsprop[i,j] <- mean(etimes[[i]][[j]][,4]=="ARS")
    darsprop[i,j] <- mean(dtimes[[i]][[j]][,4]=="ARS")
    sstimes[i,j] <- stim[1]
    svtimes[i,j] <- stim[2]
    swtimes[i,j] <- stim[3]
    dstimes[i,j] <- dtim[1]
    dvtimes[i,j] <- dtim[2]
    dwtimes[i,j] <- dtim[3]
    estimes[i,j] <- etim[1]
    evtimes[i,j] <- etim[2]
    ewtimes[i,j] <- etim[3]
  }
}
dsreltimes <- (dstimes - sstimes)/sstimes*100
esreltimes <- (estimes - sstimes)/sstimes*100
reltimedat <- data.frame(time=NULL, V=NULL, W=NULL, sampler=NULL)
reltimedat <- rbind(reltimedat, data.frame(time=c(dsreltimes), V=rep(Vs, 6), W=rep(Vs, each=6), sampler="dist"))
reltimedat <- rbind(reltimedat, data.frame(time=c(esreltimes), V=rep(Vs, 6), W=rep(Vs, each=6), sampler="error"))

@

It's also worth comparing actual computation time for the three
algorithms above. We'll focus on two comparisons. First, it's worth
noting how costly it is to simulate the states via FFBS then transform
them to either the the erros or the disturbances. This will suggest
whether there are subtantial gains to be made by simulating the errors
or disturbances directly. Second, we'll consider the relative speeds
of the rejection sampling step in the error and disturbance samplers.

Figure \ref{reltimeplot} contains the relative amount of time spent
sampling the states and transforming them appropriately in the scaled
disturbance and scaled error samplers, as a proportion of the amount
of time spent sampling the states in the state sampler for the
$T=1000$ simulations. The scaled
disturbance sampler tends to spend around $5-10\%$ more time sampling
the states while the scaled error sampler spends around $5-15\%$ more
time sampling the states than the state sampler.  Clearly, it appears
the scaled error sampler could benefit more from directly sampling the
errors than the scaled disturbance sampler could from directly
sampling the disturbances, though it's probably worthwhile in both
cases.

Figure \ref{rejtimeplot} contains the average amount of time in
thousandths of a second per iteration spent on the rejection sampling
step in scaled disturbance and scaled error samplers. in addition to
proportion of iterations using the adaptive rejection sampler, once
again only for the $T=1000$ simulations. The disturbance sampler
appears to spend about about $0.7$ thousandths of a second on the
rejection sampling step, except as the signal-to-noise ratio gets
low. In this case it spends as much as $38$ thousandths of a second on
the rejection sampling step. Notice
that when this happens, the adaptive rejection sampling algorithm
isn't being used much --- this is almost assuredly the
cause. Similarly, the scaled error sampler spends about $0.7$
thousandths of a second on the rejection step, except when the
signal-to-noise ratio is high, in which case it spends as much as $48$
thousandths of a second on the rejection step. This, again, appears to
be because the sampler can't use adaptive rejection sampling in this case.

The big things to note here are 1) the scaled disturbance sampler is
slightly faster than the scaled error sampler in the state sampling
and transformation steps (as long as we're using FFBS to sample the
states, then transforming them appropriately). 2) The scaled
disturbance sampler and the scaled error sampler fail to be able to
use adaptive rejection sampling in opposite ends of the parameter
space, but the scaled disturbance sampler is able to use adaptive
rejection sampling in a wider range. Also 3) the regions of the
parameter space where the scaled disturbance and scaled error samplers
don't use adaptive rejection sampling are the extreme ends of the
regions where they don't have autocorrelation problems, i.e. precisely
in the region where we might want to use them. This is unfortunate,
but motivates us to find a better sampling algorithm when
log-concavity and thus adaptive rejection sampling fails.



\begin{figure}[htb]
  \centering
<<reltimeplot, echo=FALSE, fig.height=4>>=
ggplot(data=reltimedat, aes(x=V, y=W, fill=time)) + #$
    geom_tile() +
    scale_fill_gradient("Time",
                        low="white", high=muted("blue")) +
    facet_grid(.~sampler, scales="free",
               labeller=label_both) +
    scale_x_log10("V = noise", breaks=Vs) +
    scale_y_log10("W = signal", breaks=Vs) +
    ggtitle("Time Spent Sampling and Transforming States") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@
\caption{Extra amount of time spent sampling the states and
  transforming them to the scaled disturbances or scaled errors,
  respectively, in the scaled disturbance sampler and scaled error
  sampler relative to the state sampler as a percentage of the amount
  of time spent sampling the states in the state sampler. I.e. the
  formula is $100\times \frac{scaled\ time\ -\ state\ time}{state\
    time}$. $X$ and $Y$ axes
  indicate the true values of $V$ and $W$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  moving up any diagonal. In the upper left the signal is high, in the
  lower right the noise is high.}
\label{reltimeplot}
\end{figure}





% rejection sampling times
\begin{figure}[htb]
\centering
<<tablerejdtime, echo=FALSE, results='asis', fig.height=4>>=
rejtimes <- data.frame(time=NULL, V=NULL, W=NULL, sampler=NULL)
rejtimes <- rbind(rejtimes, data.frame(time=c(dwtimes/2), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="dist"))
rejtimes <- rbind(rejtimes, data.frame(time=c(ewtimes/2), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="error"))
ggplot(data=rejtimes, aes(x=V, y=W, fill=time)) + #$
    geom_tile() +
    scale_fill_gradient("Time", trans="log", low="white",
                        high=muted("blue"), breaks=c(.14, 1, 7.4)) +
    facet_grid(.~sampler, scales="free",
               labeller=label_both) +
    scale_x_log10("V = noise", breaks=Vs) +
    scale_y_log10("W = signal", breaks=Vs) +
    ggtitle("Average Time Spent in the Rejection Sampling Step") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
arstimes <- data.frame(prop=NULL, V=NULL, W=NULL, sampler=NULL)
arstimes <- rbind(arstimes, data.frame(prop=c(darsprop), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="dist"))
arstimes <- rbind(arstimes, data.frame(prop=c(earsprop), V=rep(Vs, 6), W=rep(Ws, each=6), sampler="error"))
ggplot(data=arstimes, aes(x=V, y=W, fill=prop)) + #$
    geom_tile() +
    scale_fill_gradient("Proportion", low="white", limits=c(0,1),
                        high=muted("blue")) +
    facet_grid(.~sampler, scales="free",
               labeller=label_both) +
    scale_x_log10("V = noise", breaks=Vs) +
    scale_y_log10("W = signal", breaks=Vs) +
    ggtitle("Proportion of Iterations Adaptive Rejection Sampling is Used") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@
\caption{Average amount of time in thousandths of a second per
  iteration spent on the rejection sampling step (for $W$ in the
  scaled disturbance sampler and for $V$ in the scaled error sampler)
  and the proportion of iterations spent using the adaptive rejection
  sampler for $T=1000$. $X$ and $Y$ axes
  indicate the true values of $V$ and $W$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  moving up any diagonal. In the upper left the signal is high, in the
  lower right the noise is high.}
\label{rejtimeplot}
\end{figure}


\end{document}
