%\documentclass[xcolor=dvipsnames, handout]{beamer}
\documentclass[xcolor=dvipsnames]{beamer}
\makeatletter\def\Hy@xspace@end{}\makeatother 
\usepackage{graphicx, color, amssymb, amsmath, bm, rotating, 
graphics, epsfig, multicol, amsthm, bbm}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}
\usepackage[authoryear]{natbib}
\usepackage{bibentry}
\nobibliography*
%\newcommand{\newblock}{}  %needed to make beamer and natbib play nice
\usepackage{tikz}
\usetikzlibrary{positioning,shapes.geometric}
\usetikzlibrary{fit}  % fitting shapes to coordinates
\usetheme{Boadilla}
\usecolortheme[named=Red]{structure}
\setbeamercovered{transparent=0}
\beamertemplatenavigationsymbolsempty
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\N{\mathrm{N}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\vect}{vec}
\newcommand{\indicator}[1]{\mathbbm{1}{\left\{ {#1} \right\} }}

\title[Oral Prelim]{Oral Prelim}
%\subtitle{}
\author[Matt Simpson]{Matthew Simpson}
\date{}
\institute[]{Departments of Statistics and Economics, Iowa State University}


%\title[short title]{long title}
%\subtitle[short subtitle]{long subtitle}
%\author[short name]{long name}
%\date[short date]{long date}
%\institution[short name]{long name}

% very important to use option [fragile] for frames containing code output!
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
Three papers:\\~\\
\begin{itemize}
\item Interweaving in Dynamic Linear Models \\~\\
\begin{itemize}
\item Submitted to JCGS with Jarad Niemi and Vivekananda Roy\\~\\
\end{itemize}
\item Computationally conveninent covariance matrix priors under multiple parameterizations\\~\\
\item Evaluating the national school lunch program using Bayesian partial identification
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Interweaving in DLMs: A motivating example}
Adapted from \citet{yu2011center}, suppose:\\
\begin{align*}
y|\theta_1, \mu & \sim N(\theta_1, 1) \\
\theta_1|\mu & \sim N(\mu, \sigma^2) 
\end{align*}\\~\\
with $\sigma^2$ known and $p(\mu)\propto 1$.\\~\\~\\

\pause Posterior of $\mu$: $\mu|y \sim N(y,1+\sigma^2)$

\end{frame}

\begin{frame}
\frametitle{Interweaving in DLMs: A motivating example}

DA algorithm based on $\theta_1$:
\begin{align*}
\theta_1|\mu,y &\sim N\left(\frac{\mu + \sigma^2y}{1+\sigma^2}, \frac{\sigma^2}{1+\sigma^2}\right)\\
\mu |\theta_1, y &\sim N(\theta_1, \sigma^2)
\end{align*}\\~\\

\pause Let $\theta_2 = \theta_1 - \mu$. Then DA algorithm based on $\theta_2$:
\begin{align*}
\theta_2|\mu,y &\sim N\left(\frac{\sigma^2(y - \mu)}{1+\sigma^2}, \frac{\sigma^2}{1+\sigma^2}\right)\\
\mu |\theta_2, y &\sim N(y-\theta_2, 1)
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Interweaving in DLMs: A motivating example}
$\sigma=\frac{1}{\sqrt{1000}}$
\begin{center}
\includegraphics[width=0.9\textwidth]{trace1}\\
\end{center}
$\sigma=\sqrt{1000}$
\begin{center}
\includegraphics[width=0.9\textwidth]{trace2}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Interweaving in DLMs: The basic idea}
Alternate between two DAs -- alternating algorithm:
\begin{align*}
{\color{blue}[\theta_1|\mu,y]} \to {\color{blue}[\mu|\theta_1,y]} \to [\theta_2|\mu,y]\phantom{\theta_1} \to [\mu|\theta_2,y]
\end{align*}\\~\\
\pause Weave two DAs together -- global interweaving strategy (GIS):
\begin{align*}
{\color{blue}[\theta_1|\mu,y]} \to {\color{blue}[\mu|\theta_1,y]} \to {\color{red}[\theta_2|\mu,\theta_1,y]} \to [\mu|\theta_2,y]
\end{align*}\\~\\\pause
The GIS algorithm obtains {\bf IID} draws from the posterior of $\mu$.
\end{frame}

\begin{frame}
\frametitle{Interweaving in DLMs: Ancillary--sufficiency interweaving strategy (ASIS)}
GIS where one DA is a sufficient augmentation (SA) and the other is an ancillary augmentation (AA).\\~\\
\begin{itemize}
\item$\theta$ is an SA if $p(y|\theta,\phi)=p(y|\theta)$ (AKA centered augmentation)\\~\\
\item$\theta$ is an AA if $p(\theta|\phi)=p(\theta)$ (AKA non-centered augmentation)\\~\\
\end{itemize}
\pause Basu's theorem suggests the form and amount of dependence between an SA and an AA will be limited.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Interweaving in DLMs: The dynamic linear model} 
  For $t=1,2,...,T$
  \begin{align*}
    y_t  =&F_t\theta_t +  v_t  \qquad \mbox{(observation equation)}\\
    \theta_t =& G_t\theta_{t-1} + w_t \qquad \mbox{(system equation)}
  \end{align*} 
  with $v_t\stackrel{iid}{\sim}N_k(0,V)$ independent of $w_t\stackrel{iid}{\sim}N_p(0,W)$.\\~\\

\begin{figure}
  \centering
    \tikzstyle{state}=[circle, thick, minimum size=1.2cm, draw=black!80]
    \tikzstyle{obs}=[circle, thick, minimum size=1.2cm, draw=black!80]
  \begin{tikzpicture}[>=latex,text height=1.5ex,text depth=0.25ex]
    \matrix[row sep=0.5cm,column sep=0.5cm]{
    % First line: Observations
    &
    \node (y_t-1) [obs]{$y_{t-1}$}; &
    &
    \node (y_t) [obs]{$y_{t}$}; &
    &
    \node (y_t+1) [obs]{$y_{t+1}$}; &
    \\
    % Second line: States
    \node (theta_t-2) {$\cdots$}; &
    \node (theta_t-1) [state]{$\theta_{t-1}$}; &
    &
    \node (theta_t) [state]{$\theta_{t}$}; &
    &
    \node (theta_t+1) [state]{$\theta_{t+1}$}; &
    \node (theta_t+2) {$\cdots$}; \\
    };
    
    % The diagram elements are now connected through arrows:
    \path[->]
    (theta_t-2) edge (theta_t-1)
    (theta_t-1) edge (theta_t)
    (theta_t) edge (theta_t+1)
    (theta_t+1) edge (theta_t+2)
    (theta_t-1) edge (y_t-1)
    (theta_t) edge (y_t)
    (theta_t+1) edge (y_t+1)
    ;
  \end{tikzpicture}
  \end{figure}

For convenience define $y\equiv(y_1',\cdots,y_T')'$ and $\theta\equiv(\theta_0',\cdots,\theta_T)'$.

\end{frame}

\begin{frame}
  \frametitle{Interweaving in DLMs: The dynamic linear model} 
Let $\phi=(V,W)$ denote the unknown parameter.\\~\\

Priors: independently \\~\\
\begin{itemize}
\item[]$\theta_0\sim N_p(m_0,C_0)$, $V\sim IW_k(\Lambda_V,\lambda_V)$, and $W\sim IW_p(\Lambda_W,\lambda_W)$\\~\\
\end{itemize}

\pause Want to apply GIS and ASIS in particular, but suppose $\eta$ is a SA and $p(y,\eta|\phi)$ is Gaussian.\\~
\begin{itemize}
\item[] Then under weak conditions, $p(\phi|\eta,y)$ and $p(\phi|y)$ have a similar form.\\~\\
\pause\item[] $\implies$ ASIS requires sampling from a density just as difficult as the posterior.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Interweaving in DLMs: Data augmentations for the DLM}
Standard DA: states $\theta$. In terms of $\theta$, the model is:
\begin{align*}
y_t|\theta,V,W \stackrel{ind}{\sim} & N_k(F_t\theta_t,V)\\ 
\theta_t|\theta_{0:(t-1)},V,W \sim & N_p(G_t\theta_{t-1},W)
\end{align*} 
{\color{blue}$\theta$ is an SA for $W|V$ and an AA for $V|W$.}\\~\\

\pause Scaled disturbances $\gamma\equiv(\gamma_0',\cdots,\gamma_T)'$ \citep{fruhwirth2004efficient}.
\begin{itemize}
\item[]$\gamma_0=\theta_0$ and $\gamma_t=L_W^{-1}(\theta_t - G_t\theta_{t-1})=L_W^{-1}w_t$ where $L_W$ is the Cholesky decomposition of $W$.
\end{itemize}
\pause Then {\color{blue}$\gamma$ is an AA for $(V,W)$.}\\~\\


\pause Scaled errors $\psi\equiv(\psi_0',\cdots,\psi_T')'$:
\begin{itemize}
\item[]$\psi_0=\theta_0$ and $\psi_t=L_V^{-1}(y_t - F_t\theta_t)=L_V^{-1}v_t$ where $L_V$ is the Cholesky decomposition of $V$.
\end{itemize}
\pause Then {\color{blue}$\psi$ is an AA for $(V,W)$.}
\end{frame}

\begin{frame}
\frametitle{Interweaving in DLMs: New DAs for the DLM}
Wrongly-scaled disturbances $\tilde{\gamma}\equiv(\tilde{\gamma}_0',\cdots,\tilde{\gamma}_T')'$:\\~\\
\begin{itemize}
\item[]$\tilde{\gamma}_0=\theta_0$ and $\tilde{\gamma}_t=L_V^{-1}(\theta_t - G_t\theta_{t-1})=L_V^{-1}w_t$\\~\\
\end{itemize}

Wrongly-scaled errors $\tilde{\psi}\equiv(\tilde{\psi}_0',\cdots,\tilde{\psi}_T')'$:\\~\\
\begin{itemize}
\item[]$\tilde{\psi}_0=\theta_0$ and $\tilde{\psi}_t=L_W^{-1}(y_t - F_t\theta_{t})=L_W^{-1}v_t$\\~\\
\end{itemize}

{\color{blue} $\tilde{\gamma}$ is an SA for $W|V$ and $\tilde{\psi}$ is an SA for $V|W$.}
\end{frame}

\begin{frame}[fragile]
\frametitle{Interweaving in DLMs: Evaluating the strategies}
Model: for $t=1,2,...,T$
\begin{align*}
    y_t|\theta  \stackrel{ind}{\sim}&N(\theta_t,V) \qquad (\mbox{observation equation})\\
    \theta_t|\theta_{0:(t-1)} \sim& N(\theta_{t-1},W) \qquad (\mbox{system equation})
  \end{align*} 

Let $V^*$ and $W^*$ denote the true values used to simulate the time series.\\~\\

Independent priors:
\begin{itemize}
\item $\theta_0\sim N(0, 10^7)$, $V\sim IG(5, 4V^*)$ and $W\sim IG(5, 4W^*)$.\\~\\
\end{itemize}

Simulation Setup:
\begin{itemize}
\item Simulated data: $T=10$, $T=100$ \& $T=1000$ and $V^*$, $W^*$ $=10^{i/2}$ with $i=-4,-3,\cdots,4$.
\item Each sampler was used to fit the model to each dataset using one Markov chain started at $(V^*,W^*)$.
\end{itemize}


\end{frame}

\begin{frame}
\frametitle{Interweaving in DLMs: LLM Results -- ESP for $T=100$}
\centering
\includegraphics[width=0.59\textwidth]{../dlmasis/doc/plots/basecisESplot100}\\
\includegraphics[width=0.53\textwidth]{../dlmasis/doc/plots/altintESplotV100}
\includegraphics[width=0.45\textwidth]{../dlmasis/doc/plots/altintESplotW100}
\end{frame}

\begin{frame}
\frametitle{Interweaving in DLMs: Proceedings paper}

Presented at the second Bayesian young statisticians meeting in Vienna, Austria, September 18-19, 2014.\\~\\

Submitted proceedings paper: example using interweaving in a DLM for one treatment of an economic experiment.\\~\\

\pause Model:
\begin{align*}
y_{j,t} =& \mu_t + \theta_{j,t} + v_{j,t}&&\mbox{(observation equation)} \nonumber\\
\theta_{j,t}=&\theta_{j,t-1} + w_{j,t}&&\mbox{(replication level system equation)}\nonumber\\
\mu_t=&\mu_{t-1} + u_t&&\mbox{(treatment level system equation)}
\end{align*}
with
\begin{align*}
v_{j,t}\sim N(0,V_j), && w_{j,t}\sim N(0,W_j), && u_t\sim N(0,U)
\end{align*}
independently.

\end{frame}




\begin{frame}
\frametitle{Covariance matrix priors: Motivation}
Consider the model
\begin{align*}
 y_t &\stackrel{ind}{\sim} N(\theta_t, V), &\theta_t &\stackrel{iid}{\sim} N(\mu,W)
\end{align*}
\pause
Let $\gamma_t=(\theta_t-\mu)/\sqrt{W}$. Then
\begin{align*}
 y_t &\stackrel{ind}{\sim} N(\mu + \sqrt{W}\gamma_t, V), & \theta_t &\stackrel{iid}{\sim} N(0,1)
\end{align*}
$\gamma_{1:T}$ are called the scaled disturbances or noncentered disturbances.\\~\\

\pause Want priors for $V$ and $W$ that yield fast MCMC {\it and} have good properties:
\begin{itemize}
\item Conditionally conjugate:
\begin{itemize}
\item under $\theta_{1:T}$: $IG(\alpha,\beta)$ on both $V$ and $W$.
\item under $\gamma_{1:T}$: $IG(\alpha,\beta)$  on $V$ and $N(0,Q)$ on $\pm\sqrt{W}$
\end{itemize}
\item \citet{fruhwirth2008bayesian}: $N(0,Q)$ on $\pm\sqrt{V}$ and $\pm\sqrt{W}$
\item \citet{gelman2006prior}: half-$t$ on $\sqrt{V}$ and $\sqrt{W}$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Covariance matrix priors: Motivation -- DAs}
Consider the scaled disturbances:
\begin{align*}
 y_t &\stackrel{iid}{\sim} N(\mu + \sqrt{W}\gamma_t, V), &\gamma_t &\stackrel{ind}{\sim} N(0,1)
\end{align*}
Suppose we put $N(0,Q)$ priors on $\pm\sqrt{V}$ and $\pm\sqrt{W}$.
\begin{itemize}
\item[]( Equivalent to $G(1/2, Q/2)$ on $V$ and $W$ under the shape-scale parameterization)
\end{itemize}

\pause
Full conditionals:
\begin{align*}
&\pm\sqrt{W}\sim N(.,.), &&V\sim GIG(.,.,.)
\end{align*}
i.e. 
\[
p(V|\cdots)\propto V^{p-1}\exp\left[-\frac{1}{2}\left(a V + \frac{b}{V}\right)\right].
\]
 So the full conditionals are relatively easy to sample from, but the $GIG$ draw is costly.
\end{frame}
%% 1.751 seconds for 100,000 iid GIG(1,1,1) draws, 0.015 seconds for 100,000 iid G(1,1) draws
%% using rgig() from GeneralizedHyperbolic R package

%where 
%\begin{align*}
%&\hat{\mu}_W=\hat{\Sigma}_W\frac{\sum_{t=1}^T(y_t-\mu)\gamma_t}{V}, &&\hat{\Sigma}_W=\left(\frac{\sum_{t=1}^T\gamma_t^2}{V} + \frac{1}{Q_W}\right)^{-1}
%\end{align*}
%\begin{align*}
%&\hat{\alpha}_V=-T/2 + 1, &&\hat{\beta}_V=1/Q_V, &&&\hat{\gamma}_V=\sum_{t=1}^T(y_t - \mu - \sqrt{W}\gamma_t)^2
%\end{align*}


\begin{frame}
\frametitle{Covariance matrix priors: Motivation -- Mixing gammas}
How does the half$-t$ prior relate to the others? Suppose
\[
W\sim G(a,Qk) \mbox{ and } Q\sim IG(\alpha,\beta)
\]
or
\[
W\sim IG(\alpha, B) \mbox{ and } B\sim G(a,\beta k)
\]
where $k$ is a known constant. \pause Then
\[
p(W) \propto W^{a-1}(W + \beta k)^{-(a + \alpha)}
\]

and for $S=\pm\sqrt{W}$
\[
p(S) \propto S^{2a-1}\left(1 + \frac{S}{\beta k}\right)^{-(a + \alpha)}
\]
$a=1/2 \implies $ $S\sim t$ and $|S| \sim$ half-$t$.\\~

\pause \citet{huang2013simple}: $IG(\alpha,\beta)$ on $V$ and $W$ with $\beta\sim G(a,b)$.
\end{frame}

\begin{frame}
\frametitle{Covariance matrix priors: Motivation -- Equivalent priors}
In particular the following result in equivalent marginal distributions for $W$:
\begin{align*}
W|B &\sim IG(n/2,B), &&B\sim G(1/2,\beta/2)\\
W|Q &\sim G(1/2,Q/2), &&Q\sim IG(n/2,\beta)\\
\pm\sqrt{W}|Q &\sim N(0,Q), &&Q\sim IG(n/2,\beta)\\
\sqrt{W}|Q &\sim \mbox{half-}N(0,Q), &&Q\sim IG(n/2,\beta)
\end{align*}
with marginal distribution
\begin{align*}
\pm\sqrt{W} &\sim t_n(0,2\beta/n) &\mbox{and}&& \sqrt{W} \sim \mbox{half-}t_n(0,2\beta/n)
\end{align*}

\pause Unites the many of the available priors for variance parameters under a mixture distribution.
\end{frame}

\begin{frame}
\frametitle{Covariance matrix priors: Motivation -- full conditionals}
Using the scaled disturbances:
\begin{align*}
 y_t &\stackrel{ind}{\sim} N(\mu + \sqrt{W}\gamma_t, V), & \theta_t &\stackrel{iid}{\sim} N(0,1)
\end{align*}
Suppose we put $t$ priors on $S_V=\pm\sqrt{V}$ and $S_W=\pm\sqrt{W}$. Full conditionals?\\~ \pause

Complicated, but if we use the auxillary variables $Q_W$ and $B_V$:
\begin{align*}
V|\cdots  &\sim IG(.,.), & B_V|\cdots  & \sim G(.,.) &&\mbox{\citep{huang2013simple}}\\% \propto V^{-\frac{T + n_V}{2} - 1}\exp\left[-\frac{1}{V}\left(\frac{\sum_{t=1}^T(y_t - \mu - S_W\gamma_t)^2}{2} + B_V\right)\right]\\ %\propto B_V^{-\frac{1}{2}}\exp\left[-B_V\left(\frac{1}{V} + \frac{2}{\beta_V}\right)\right]
\intertext{and}
S_W|\cdots  &\sim N(.,.),  &Q_W|\cdots  &\sim IG(.,.) &&\mbox{[Unknown in literature]}%\propto \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\frac{(y_t - \mu - S_W\gamma_t)^2}{V} + \frac{S^2_W}{Q_W}\right)\right]\\ %\propto Q_W^{-\frac{n_W}{2} - 1}\exp\left[-\frac{1}{Q_W}\left(\frac{S_W^2}{2} + \beta_W\right)\right]\\
\end{align*}
\pause $\implies$ The $t$ prior is computationally convenient for either DA.
\end{frame}

\begin{frame}
\frametitle{Covariance matrix priors: Multivariate case}
Now $V$ and $W$ are covariance matrices -- want to generalize.\\
Let $\gamma_t= Chol(W)^{-1}(\theta_t - \mu)$. \pause Under $\gamma_{1:T}$:
\begin{enumerate}
\item Conditionally conjugate for $V$: inverse Wishart.
\item \citet{alvarez2014cov}: IW extremely informative for correlations.
\item Conditionally conjugate for $W$: $\pm\vech(Chol(W))\sim N(0,Q)$ \citep{fruhwirth2008bayesian}.\\
\[
\mbox{$\vech()$ is the half-vectorization:  }\vech\left(\begin{bmatrix} a & b \\ c & d \end{bmatrix}\right) = \begin{bmatrix}a\\c\\d\end{bmatrix}
\]

\item \citet{gelman2006prior}: something with half-$t$'s on the standard deviations.
\item \citet{huang2013simple}: inverse Wishart with scale matrix $\Sigma$, then independent gammas on $\sigma_{ii}$'s.
\end{enumerate}
\pause Possible generalizations:\\
\begin{enumerate}
\item Wishart mixture of inverse Wisharts (or vice versa)
\item inverse Wishart mixture of normals on $\pm\vech(Chol(W))$, i.e. $t$.
\end{enumerate}
\pause These two are not the same by the Bartlett decomposition! 
\end{frame}

% \begin{frame}
% \frametitle{Covariance Matrix Priors -- Bartlett decomposition}
% Suppose $W\sim W_p(n, I_{p\times p})$ and $Chol(W)=[s_{ij}]$.\\~\\

% Jacobian: 
% \[
% dW = 2^p\prod_{j=1}^ps_{jj}^{p+1-j} dChol(W)
% \]

% Then the nonzero $s_{ij}$ are independent with 
% \begin{align*}
% s_{ij}\sim N(0,1)\mbox{ for }i<j&&\mbox{ and }&& s_{ii}^2\sim\chi^2_{n-i+1} = G((n-i+1)/2, 2)
% \end{align*}

% \pause But \ \ $\pm \sqrt{s}|Q \sim N(0,Q) \iff s|Q\sim G(1/2,Q/2)$.\\~

% In general if $W\sim W_p(n,\Sigma)$ or $W\sim IW_p(n,\Sigma)$ and $n$ is an integer, then $W$'s distribution depends on $n\times p$ scalar normal random variables.\\~

% \end{frame}

\begin{frame}
  \frametitle{Covariance matrix priors: Mixing Wisharts}
Suppose either 
\begin{align*}
X|Y \sim W_p(n_1,Y), &&Y\sim IW_p(n_2,\Sigma)
\end{align*}
or
\begin{align*}
X|Y \sim IW_p(n_2,Y), &&Y\sim W_p(n_1,\Sigma)
\end{align*}
where $n_1,n_2>p-1$ and $\Sigma$ is $p\times p$ and positive definite.\\~\\
\pause 
Then the marginal distribution of $X$ is $X\sim F_p(n_1,n_2,\Sigma)$ where
\begin{align*}
 p(X|\Sigma) = \frac{\left|\Sigma\right|^{n_2/2}}{\B_p(n_1/2,n_2/2)}|X|^{(n_1 - p - 1)/2}|X + \Sigma|^{-(n_1 + n_2)/2}
\end{align*}
Called the matrix-$F$ and generalized matrix-variate beta distribution of the second kind.
\end{frame}

% \begin{frame}
% \frametitle{Covariance Matrix Priors -- Multivariate beta function}
% \begin{align*}
%   p(X|\Sigma) = \frac{\left|\Sigma\right|^{n_2/2}}{\B_p(n_1/2,n_2/2)}|X|^{(n_1 - p - 1)/2}|X + \Sigma|^{-(n_1 + n_2)/2}
% \end{align*}
% \pause
% \begin{align*}
% \B_p(\alpha_1, \alpha_2) &= \int_{X>0}|X|^{\alpha_1 - (p + 1)/2}|X + I|^{-(\alpha_1 + \alpha_2)}dX\\
% &= \int_{0<X<I}|X|^{\alpha_1 - (p + 1)/2}|I-X|^{\alpha_2 - (p + 1)/2}dX\\
% &=\frac{\Gamma_p(\alpha_1)\Gamma_p(\alpha_2)}{\Gamma_p(\alpha_1 + \alpha_2)}
% \end{align*}
% \pause where
% \begin{align*}
% \Gamma_p(\alpha) &= \int_{X>0}|X|^{\alpha - (p+1)/2}\exp\left[-\tr(X)\right]dX\\
% &=\pi^{p(p-1)/4}\prod_{i=1}^p\Gamma(\alpha - (i-1)/2)
% \end{align*}
% \end{frame}

\begin{frame}
\frametitle{Covariance matrix priors: Marginal SD priors}
If $X\sim W_p(n_1,\Sigma)$ then the marginal distribution of $s_{ii}=\sqrt{x_{ii}}$ is the Nakagami distribution with density:
\[
p(s_{ii})\propto s_{ii}^{n_1-1}\exp\left[-\frac{s_{ii}^2}{2\sigma_{ii}}\right]
\]

\pause If $X\sim F_p(n_1, n_2 + p - 1, \Sigma)$ then the marginal distribution of $s_{ii}=\sqrt{x_{ii}}$ is the ``scale mixed Nakagami'' distribution with density:
\[
p(s_{ii})\propto s_{ii}^{n_1 - 1}\left(1 + \frac{s_{ii}^2}{\sigma_{ii}}\right)^{-(n_1 + n_2)/2}
\]

\pause {\color{blue}{\it The degrees of freedom problem}}: in both cases $n_1 > p - 1$ for the covariance matrix to be almost surely positive definite, but $n_1=1$ results in marginal half-normals or half-$t$'s.
\end{frame}

\begin{frame}
\frametitle{Covariance matrix priors: Multivariate full conditionals}
\citet{huang2013simple}: $W|\Sigma\sim W_p(n,\Sigma)$, $\sigma_{ii}\stackrel{ind}{\sim} G(\alpha_i,\beta_i)$. \\
This yields:
\begin{enumerate}
\item Marginal half-$t$ distributions for the SDs.
\item Easy full conditional draws for $V$.\\
\end{enumerate}
\pause The full conditonals for $W$ are more complicated. \\~

\pause Using the normal priors on $\pm\vech(Chol(W))$ and $\pm\vech(Chol(V))$, the full conditional for $V$ is complicated.\\

\pause Instead of $Chol(W)$, could use any matrix $S_W$ such that $S_WS_W'=W$ with $\gamma_t=S_W^{-1}(\theta_t - \mu)$:
\begin{align*}
  p(W&|\gamma_{0:T},\cdots)\propto \exp\left[-\frac{1}{2}(S_W\delta - \eta)'\Omega(S_W\delta - \eta)\right]\times \mbox{prior}
\end{align*}
where  $\delta$ and $\eta$ are $p\times 1$, $\Omega$ is $p\times p$  and positive definite.\\
\cite{fruhwirth2008bayesian}: normal kernel...
\begin{itemize}
\item[] but their method for computing the parameters is complicated.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Covariance matrix priors: Elimination and duplication}

 If $S_W=Chol(W)$ then $S_W\delta = (\delta' \otimes I_p)L_p'\vech(S_W)$.
\begin{itemize}
\item $L_p$ is the elimination matrix, the unique $p(p+1)/2\times p^2$ matrix such that $\vect(A)=L_p'\vech(A)$ for all $p\times p$ lower triangular $A$.\\~\\
\end{itemize}
\pause If $S_W=S_W'=W^{1/2}$ then $S_W\delta = (\delta' \otimes I_p)D_p\vech(S_W)$.
\begin{itemize}
\item $D_p$ is the duplication matrix, the unique $p^2\times p(p+1)/2$ matrix such that $\vect(A)=D_p\vech(A)$ for all $p\times p$ symmetric $A$.
\end{itemize}

\pause\begin{align*}
L_2 = \left[\begin{tabular}{cc|cc} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\\hline 0 & 0 & 0 & 1 \end{tabular}\right], && D_2 = \left[\begin{tabular}{cc|c} 1 & 0 & 0 \\ 0 & 1 &  0 \\\hline 0 & 1 & 0\\ 0 & 0 & 1 \end{tabular}\right]
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Covariance matrix priors: Goals}
Goal: find a covariance matrix prior $p(W)$ with two mixture representations:\\~
\begin{enumerate}
\item $p(V|B)p(B)$ where $p(V|\gamma_{0:T},\cdots)$ is inverse Wishart and $p(B|\gamma_{0:T},\cdots)\propto p(B)p(V|B)$ is easy to sample from.\\~
\item $p(S_W|Q)p(Q)$ where $S_WS_W'=W$, $p(S_W|\gamma_{0:T},\cdots)$ is Normal and $p(Q|\gamma_{0:T},\cdots)\propto p(Q)p(S_W|Q)$ is easy to sample from.\\~
\end{enumerate}
\pause More generally: find a prior that...
\begin{itemize}
\item yields easy full conditionals for both $V$ and $W$ under $\gamma_t$.
\item is not highly informative for either the correlations or the SDs.\\~
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Evaluating the National School Lunch Progam (NSLP)}
The NSLP gave free or reduced price lunches to over 31 million U.S. children each school day in 2012 at a cost of about \$11.6 billion for fiscal year 2012.\\~\\

Housholds which are under 130\% of the poverty line receive free school lunches for their children, while households between 130\% and 185\% of the poverty line pay a small price --- 40 cents in 2001--2004, the period our data comes from.\\~\\

NSLP $\to$ better health outcomes for children? Mixed evidence --- missing counterfactual problem and underreporting of participation.\\~\\

\citet{gundersen2012impact} attempts to deal with both of these issues.

\end{frame}

\begin{frame}
\frametitle{Evaluating the NSLP: Data}
Source: 2001--2004 National Health and Nutrition Examination Survey (NHANES), conducted by the National Center for Health Statistics, Centers for Disease Constrol (NCHS/CDC).\\~\\

Detailed measures on a variety of health related outcomes are included in the NHANES.\\~\\

\pause Focus on one outcome: food security.

\begin{itemize}
\item Measured with a series of 18 questions about food-related needs and resources in the household, e.g. ``I worried whether our food would run out before we got money to buy more.'' 
\item The household is considered to be food insecure if the respondent answers affirmatively to three or more of these questions.
\item $y_i=1$ denotes that household $i$ is food secure; $y_i=0$ denotes that household $i$ is not food secure.
\end{itemize}
\end{frame}
    
\begin{frame}
  \frametitle{Evaluating the NSLP: Base model}
  For household $i=1,2,...,n$:\\~\\
\begin{itemize}
  \item $y_i$ is the food security indicator for household $i$. 
  \item $z_i$ is treatment status indicator for household $i$.\pause\\~
  \item $y_i(0)$ is what the food security indicator for $i$ would be if they were not participating in the NSLP.
  \item $y_i(1)$ is what the food security indicator for $i$ would be if they were participating in the NSLP.\pause\\~
  \item Thus $y_i = y_i(z_i)$ is observed and $y_i(1-z_i)$ is unobserved. 
  \item We call them both ``counterfactual outcomes'' or ``potential outcomes.''
\end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Evaluating the NSLP: Base model}
Assume the household's treatment decisions are independent:
    \[
    z_i \stackrel{iid}{\sim}Ber(p_z)
    \]
    where $p_z=P(z_i=1)$.\\~\\
\pause Presumably household $i$'s two outcomes are correlated, conditional on their treatment choice. 
    \begin{itemize}
      \item Let $  p_{ab|c} = P(y_i(0)=a, y_i(1)=b|z_i=c)$.
      \item Define 
        \begin{align*}
          &p_0\equiv (p_{00|0}, p_{10|0}, p_{01|0}, p_{11|0}), &&p_1\equiv (p_{00|1}, p_{10|1}, p_{01|1}, p_{11|1}).
        \end{align*}
      \item Now define $y_i^*=\indicator{y_i(0)=1} + 2\indicator{y_i(1)=1}$. \\~
      \end{itemize}
\pause Then assume:
    \[
    y_i^*|z_i=t \stackrel{ind}{\sim}Multinomial(1,p_t)
    \]
\end{frame}

\begin{frame}
  \frametitle{Evaluating the NSLP: Base model -- identification}
$p_z$ is the only identified parameter in the model.\\~\\
 Some functions of the parameters are identified: \\~\\
    \begin{itemize}
      \item $P(y_i(1)=1|z_i=1) = p_{11|1} + p_{01|1}$\\~\\  
      \item $P(y_i(0)=1|z_i=0) =p_{11|0} + p_{10|0}$\\~\\
      \item $P(y_i(1)=0|z_i=1) =p_{10|1} + p_{00|1}$\\~\\ 
      \item $P(y_i(0)=0|z_i=0) =p_{01|0} + p_{00|0}$\\~\\
      \end{itemize}
But treatment effects parameters are not, e.g.
\begin{align*}
E[ATE|y] &= E[y_{new}(1) - y_{new}(0)|y] \\
&=E[(p_{01|1} - p_{10|1})p_z + (p_{01|0} - p_{10|0})(1-p_z)|y] 
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Evaluating the NSLP: Partial identification}
Imagine every household has the same treatment condition.\\
\ \ \ Probably $P(\mbox{food secure})$ higher for those who chose not to participate.\\~

\pause Monotone treatment selection (MTS): the treatment variable ($z_i$) is a monotone instrumental variable (MIV), i.e.
\[
P[y_i(k)=1|z_i=0] > P[y_i(k)=1|z_i=1]
\]
for $k=0,1$.\\~\\

``Households choose to participate the NSLP for good reason, on average.''\\~\\

\pause In terms of the model parameters:
\begin{align*}
  p_{10|0} + p_{11|0} > p_{10|1} + p_{11|1} &&\mbox{and}&&  p_{01|0} + p_{11|0} > p_{01|1} + p_{11|1}.
\end{align*}
\pause Then estimate the model subject to these constraints.
\end{frame}


\begin{frame}
\frametitle{Evaluating the NSLP: ``Bayesian partial identification''}
Suppose the likelihood function is $p(y|\phi,\theta)=p(y|\theta)$ so that $\phi$ is unidentified.\\~

Further suppose we have a prior $p(\theta,\phi)=p(\phi|\theta)p(\theta)$. \pause We can still learn about $\phi$ despite it being unidentified:
    \begin{align*}
    p(\theta,\phi|y) &\propto p(y|\theta)p(\theta)p(\phi|\theta) \propto p(\phi|\theta)p(\theta|y).
  \end{align*}
\pause Then the marginal prior and marginal posterior of $\phi$ are:
    \begin{align*}
    p(\phi) = \int p(\phi|\theta)p(\theta)d\theta  &&\mbox{and}&& p(\phi|y) = \int p(\phi|\theta)p(\theta|y)d\theta.
  \end{align*}
Since $\phi$ and $\theta$ are dependent a priori, the marginal prior and posterior are not the same \citep{poirier1998revising}.\\~

\pause Cannot learn about the dependence between $\phi$ and $\theta$: $p(\phi|\theta,y)=p(\phi|\theta)$.

\end{frame}


\begin{frame}
  \frametitle{Evaluating the NSLP: Classification error}
There are ``impossible'' households in the data --- need to allow for misclassification.\\~

\pause Assume we observe $x_i$ which is a signal of $z_i$:
\begin{align*}
  x_i|z_i=0 \sim Ber(\alpha),&&  x_i|z_i=1 \sim Ber(1-\beta)
\end{align*}
\pause Now we observe $(x_i,y_i(z_i))$, but not $z_i$.\\
\ \ $\implies$Every structural parameter in the base model is now unidentified. \\~

\pause Solution: more partial identification.
\pause \begin{itemize}
\item[] Following \cite{bollinger2009bayesian} we'll assume $\alpha + \beta <1$.\\
\ \ \ $\implies$ $Cov(x_i,z_i)>0$ so that $x_i$ tells us something useful about $z_i$.\\~\\
\end{itemize}


\pause Assume a uniform prior for $\alpha$, $\beta$, $p_0$, $p_1$, and $p_z$ subject to the imposed constraints.
\end{frame}


\begin{frame}
\frametitle{Evaluating the NSLP: Computation}
One approach: data augmentation (DA) \citep{tanner1987calculation}.\\~\\

Missing data:
\begin{itemize}
\item Basic model: $y_i(1-z_i)$ for $i=1,2,\cdots,I$
\item Classification error model: $(y_i(0), y_i(1), z_i)$ for $i=1,2, \cdots,I$.\\~
\end{itemize}

\pause Side effect: we obtain the posterior distribution of each potential outcome for each household in the sample.\\~

\pause Potentially major issue: unidentified parameters mix very poorly. \\~\\
\ \ \ Another option: MCMC for identified $\theta$, then draw unidentified $\phi$ from
\[
p(\phi|\theta,y)=p(\phi|\theta).
\]
\end{frame}


\begin{frame}
    \frametitle{Evaluating the NSLP: Posterior predictive TEs}
Define $TE_{new}=y_{new}(1) - y_{new}(0)$ as the {\it outcome gain} or {\it treatment effect} of an as yet unobserved household.\\~\\

\cite{poirier2003predictive}:
\begin{itemize}
\item[] Average treatment effect ($ATE$) distribution: 
  \[
  p(TE_{new}|y) = \int p(TE_{new}|\theta)p(\theta|y)d\theta
  \]
\item[] Average treatment on the treated ($ATT$) distribution: 
  \[
  p(TE_{new}|y,z_{new}=1) = \int p(TE_{new}|\theta, z_{new}=1)p(\theta|y)d\theta
  \]
\end{itemize}
Both are discrete distributions on $\{-1,0,1\}$.
\end{frame}



\begin{frame}[fragile]
\frametitle{Evaluating the NSLP: Results for TE parameters}
  
% latex table generated in R 2.15.2 by xtable 1.7-1 package
% Fri Nov 15 14:38:44 2013
\begin{table}[ht]
\centering
{\footnotesize
\begin{tabular}{|r|rrr|rrr|}
  \hline
 & Mean & SD & NSE & P(TE=-1) & P(TE=0) & P(TE=1) \\ 
  \hline
Prior ATE & -0.0058 & 0.6633 & 6.63E-03 & 0.223 & 0.560 & 0.217 \\ 
  Posterior ATE & 0.0434 & 0.5904 & 4.64E-03 & 0.153 & 0.650 & 0.197 \\ 
  Prior ATT & 0.0101 & 0.6652 & 6.65E-03 & 0.216 & 0.557 & 0.226 \\ 
  Posterior ATT & 0.0106 & 0.5978 & 3.00E-03 & 0.173 & 0.643 & 0.184 \\ 
   \hline
\end{tabular}
}
\end{table}
{\footnotesize Prior and posterior predictive means, standard deviations, and 95\% credible intervals for various treatment effect parameters of the classification error model. NSE is an estimate of the numerical (Monte Carlo) standard error associated with estimating the posterior mean that attempts to take into account the dependence in the Markov chain.}
\end{frame}



\begin{frame}[fragile]
\frametitle{Evaluating the NSLP: Further work}
  \begin{enumerate}
\item Better priors, e.g. for $\alpha$ and $\beta$.
\item Covariates, particularly income. GLM with potentially unknown link function.
\item Households above but near the income threshold likely similar to households below but near the threshold -- can help identification.
\item Possible measurement error on income.
\item Once income is in the model: policy simulation. What happens when income eligibility threshold changes?
\item Separate out free and reduced lunches.
\item Take into account the sample weights in the TE distributions and elsewhere.
\item Impact of NSLP on other health outcomes.
\item Interactions between NSLP and other major programs.
  \end{enumerate}
  
\end{frame}


\appendix
\newcounter{finalframe}
\setcounter{finalframe}{\value{framenumber}}

\begin{frame}

      \begin{center}

        \font\endfont = cmss10 at 25.40mm
        \color{Red}
        \endfont 
        \baselineskip 20.0mm

        Thank you!

      \end{center}    


\end{frame}

\begin{frame}
\frametitle{Covariance matrix priors: Elimination matrix}
Elimination Matrix: $\vect(A)=L_p'\vech(A)$ for any lower triangular $p\times p$ matrix $A$.
\begin{itemize}
\item $L_p$ has full row rank $p(p+1)/2$.
\item $L_pL_p'=I_{p(p+1)/2}$.
\item $L_p^{+}=L_p'(L_pL_p')^{-1}=L_p'$ (Moore-Penrose pseudoinverse).
\item $\vech(A)=L_p\vect(A)$ for any $p\times p$ matrix $A$.
\end{itemize}
\pause\begin{align*}
L_2 = \left[\begin{tabular}{cc|cc} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\\hline 0 & 0 & 0 & 1 \end{tabular}\right], && L_3 =  \left[\begin{tabular}{ccc|ccc|ccc}
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\ 
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\hline
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\hline
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{tabular}\right]\\~\\
\end{align*}
\end{frame}


\begin{frame}
\frametitle{Covariance matrix priors: Duplication matrix}
Duplication Matrix: $\vect(A)=D_p\vech(A)$ for any symmetric $p\times p$ matrix $A$.
\begin{itemize}
\item $D_p$ has full column rank $p(p+1)/2$.
\item $D_p^{+}=(D_p'D_p)^{-1}D_p'$ (Moore-Penrose pseudoinverse).
\item $\vech(A)=D_p^+\vect(A)$ for any symmetric $p\times p$ matrix $A$.
\end{itemize}
\pause\begin{align*}
D_2 = \left[\begin{tabular}{cc|c} 1 & 0 & 0 \\ 0 & 1 &  0 \\\hline 0 & 1 & 0\\ 0 & 0 & 1 \end{tabular}\right], && D_3 =  \left[\begin{tabular}{ccc|cc|c}
1 & 0 & 0 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 & 0 & 0 \\ 
0 & 0 & 1 & 0 & 0 & 0 \\\hline
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\\hline
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1
\end{tabular}\right]\\~\\
\end{align*}
\end{frame}


\begin{frame}
\frametitle{Evaluating the NSLP -- Sketch of a Larger Model}
 Binary response, binary treatment, one real covariate with a
fuzzy threshold and many other covariates.\\~\\

\pause Counterfactual outcome $\tilde{Y}(k)$ for $k=1,2$ and observed outcome $y$.\\~

\pause Income to poverty level ratio $\tilde{x}$ and other covariates $\bm{z}$. Observe $x$ a signal of $\tilde{x}$.\\~

\pause Treatment status $\tilde{t}$, $\tilde{t}=1$ indicates participation in the NSLP. Observed treatment status $t$.\\~

\pause $\tilde{t}=\tilde{s}\tilde{a}$ where $\tilde{s}=1$ if the household would choose to participate, if allowed, and $\tilde{a}=1$ if the household is allowed to participate.\\~

\pause Officially $\tilde{a}=\indicator{\tilde{x}<x_c}$ but it is possible that:
\begin{enumerate}
\item The relevant authorities mismeasure the covariate
  $\tilde{x}$.
\item The relevant authorities are ``nice'' to people near the
  threshold.
\end{enumerate}

\end{frame}

\begin{frame}
\frametitle{Evaluating the NSLP -- Sketch of a Larger Model}

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|}
\hline
Variable & Meaning \\
\hline\hline
$\bm{z}'$ & Vector of additional covariates.\\
$\tilde{x}$ & Unobserved variable determining treatment eligibility.\\
$x$ & Measurement of treatment eligibility variable.\\
$x_c$ & Cutoff value for $\tilde{x}$ that determines treatment
eligibility.\\
$\tilde{c}$ & Indicates whether $\tilde{x}$ is below cuttoff
$x_c$. $\tilde{c}=\indicator{\tilde{x}\leq x_c}$.\\
$c$ & Indicates whether $x$ is below cutoff $x_c$. $c=\indicator{x\leq x_c}$.\\
$\tilde{s}$ & Indicates whether individual would choose the treatment.\\
$\tilde{a}$ & Indicates whether individual is allowed to select the
treatment.\\
$\tilde{t}$ & Indicates actual treatment status. $\tilde{t}=\tilde{a}\tilde{s}$.\\
$t$ & Indicates measured treatment status.\\ 
$\tilde{Y}(k)$ & The potential outcome under counterfactual $k$.\\
$y$ & The actual observed outcome. $y=\tilde{Y}(\tilde{t})$.\\
\hline
\end{tabular}
\end{table}

\end{frame}


\begin{frame}
\frametitle{Evaluating the NSLP -- Sketch of a Larger Model}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}[%
->,
shorten >=2pt,
>=stealth,
node distance=1cm,
pil/.style={
->,
thick,
shorten =2pt,}
]
\node[circle, draw, minimum size=0.80cm] (Xt) {$\tilde{x}$};
\node[circle, draw, minimum size=0.80cm] [right= of Xt] (Ct) {$\tilde{c}$};
\node[circle, draw, minimum size=0.80cm] [below=2 of Ct] (C) {$c$};
\node[above= of Xt] (AXt) {};
\node[below= of Xt] (BXt) {};
\node[circle, draw, minimum size=0.80cm] [right=0.1 of BXt] (X) {$x$};
\node[circle, draw, minimum size=0.80cm] [right=2 of BXt] (at) {$\tilde{a}$};
\node[circle, draw, minimum size=0.80cm] [right=2 of AXt] (St) {$\tilde{s}$};
\node[circle, draw, minimum size=0.80cm] [right= of Ct] (tt) {$\tilde{t}$};
\node[circle, draw, minimum size=0.80cm] [right= of at] (t){$t$};
\node[above=2 of tt] (Att){};
\node[circle, draw, minimum size=0.80cm] [right=.025 of Att] (Yt)
{\begin{tabular}{c}$\tilde{Y}(0)$\\
$\tilde{Y}(1)$\end{tabular}};
\node[circle, draw, minimum size=0.80cm] [right= of tt] (y) {$y$};

\draw[->] (Xt) --(X);
\draw[->, color=blue, thick, dashed] (Xt) --(Ct);
\draw[->, color=blue, thick, dashed] (X) --(C);
\draw[->] (Xt) --(at);
\draw[->] (Ct) --(at);
\draw[->] (C) --(at);
\draw[->] (X) --(at);
\draw[->] (Xt) --(St);
\draw[->] (Ct) --(St);
\draw[->, color=blue, thick, dashed] (St) --(tt);
\draw[->, color=blue, thick, dashed] (at) --(tt);
\draw[->] (tt) --(t);
\draw[->] (St) --(Yt);
%\draw[->] (at) --(Yt);
%\draw[->] (Xt) --(Yt);
\draw[->, color=blue, thick, dashed] (Yt) --(y);
\draw[->, color=blue, thick, dashed] (tt) --(y);
\draw[->] (Xt) to [out=90,in=180] (Yt);
%\draw[->] (1) to [out=150,in=165] (4);
\end{tikzpicture}
\end{figure}

\end{frame}



\begin{frame}[allowframebreaks]
        \frametitle{References}
        \bibliographystyle{plainnat}
        \bibliography{../dlmasis/doc/dlmasis}
\end{frame} 
\setcounter{framenumber}{\value{finalframe}}
\end{document}










\begin{frame}[fragile]
\frametitle{Evaluating the NSLP: Data summary}

Includes 2693 children ages 6 to 17 who reside in households with income less than 185\% of the federal poverty line and are reported to be attending schools with the NSLP.\\~\\
  
% latex table generated in R 2.15.2 by xtable 1.7-1 package
% Fri Nov 15 15:26:21 2013
\begin{table}[!ht]
\centering
{\scriptsize
\begin{tabular}{llll}
  \hline
  & Income-eligible children & Recipients & Nonrecipients \\ 
  \hline
Age in years & 11.88 & 11.41 & 13.47 \\ 
   & (3.33) & (3.21) & (3.21) \\ 
  NSLP recipient & 0.77 & 1 & 0 \\ 
   & (0.42) & (0) & (0) \\ 
  Ratio of income to the poverty line & 0.92 & 0.88 & 1.06 \\ 
   & (0.47) & (0.46) & (0.49) \\ 
  Food secure household & 0.6 & 0.58 & 0.67 \\ 
   & (0.49) & (0.49) & (0.47) \\ 
   \hline
\end{tabular}
}
\caption{Summary of key variables by National School Lunch Program participation. Note that these statistics do not take into account the sample weights.} 
\label{dattab}
\end{table}
\end{frame}




\begin{frame}
\frametitle{Evaluating the NSLP -- Sketch of a Larger Model}
Starting with the threshold covariate:
\begin{align*}
\tilde{x}_i\stackrel{ind}{\sim} N(\bm{z}_i'\bm{\beta}_{\tilde{x}},\sigma_{\tilde{x}}^2)&&\mbox{and}&&x_i\stackrel{ind}{\sim} N(\tilde{x}_i,\sigma_{x}^2)
\end{align*}

\pause Let
\begin{align*}
\tilde{a}_i=\indicator{a^*_i>0} &&\mbox{and}&&\tilde{s}_i=\indicator{s^*_i>0}
\end{align*}
where
\begin{align*}
a_i^*\stackrel{ind}{\sim} & N(\bm{z}_i'\bm{\beta}_{a^*} +
\tilde{c}_i\lambda_{a^*} + c_i\gamma_{a^*} + \tilde{x}_i\theta_{a^*} +
x_i\phi_{a^*}, 1)\\
s_i^*\stackrel{ind}{\sim} & N(\bm{z}_i'\bm{\beta}_{s^*} +
\tilde{c}_i\lambda_{s^*} + \tilde{x}_i\theta_{s^*} , 1)\\
\end{align*}
Then $\tilde{t}_i=\tilde{a}_i\tilde{s}_i$ \pause and let
\begin{align*}
t_i | \tilde{t}_i=0 \stackrel{ind}{\sim} & Ber(p)\\
t_i | \tilde{t}_i=1 \stackrel{ind}{\sim} & Ber(1-q)
\end{align*}
where $p$ is the false positive rate and $q$ is the false negative
rate. 
\end{frame}

\begin{frame}
\frametitle{Evaluating the NSLP -- Sketch of a Larger Model}
Define $Y^*_i(0)$ and $Y^*_i(1)$ such that $\tilde{Y}_i(k)=\indicator{Y^*_i(k)>0}$ where 
\[
\begin{bmatrix} Y_i^*(0) \\ Y_i^*(1) \end{bmatrix}
\stackrel{ind}{\sim} N \left( \begin{bmatrix}
    \bm{z}_i'\bm{\beta}_{y^*} + \tilde{x}_i\theta_{y^*} +
    s^*_i\psi_{y_0^*} \\
    \bm{z}_i'\bm{\beta}_{y^*} + \tilde{x}_i\theta_{y^*} +
    s^*_i\psi_{y_1^*} \end{bmatrix},
\begin{bmatrix}1 & \rho \\ \rho & 1 \end{bmatrix} \right)
\]
and $y_i=\tilde{Y}_i(\tilde{t}_i)$.\\~

$Y_i^*(k)$ represents an unobserved continuous measure of food security in counterfactual $k$.

\end{frame}

\begin{frame}
\frametitle{Evaluating the NSLP -- Large Model Identification Heuristics}
$\sigma_x^2$ and $\sigma_{\tilde{x}}^2$ both unidentified, but $\sigma_x^2 + \sigma_{\tilde{x}}^2$ is identified. \pause If 
\begin{align*}
Corr(x,\tilde{x}|\bm{z})>K&&\mbox{or equivalently}&&\frac{\sigma_x^2}{\sigma_x^2 + \sigma_{\tilde{x}}^2} > K^2
\end{align*}
then we can learn about both $\sigma_x^2$ and $\sigma_{\tilde{x}}^2$. \\~\\

\pause To learn about the components of the $a_i^*$ and $s_i^*$ equations, we need to learn about $a_i^*$ and $s_i^*$ $\implies$ need to learn about $p$ and $q$. \pause If $p + q > 1$ in the prior then we can learn about $p$, $q$, and $\tilde{t}_i$.\\~\\

\pause When $\tilde{t}_i=1$ we know $\tilde{a}_i=\tilde{s}_i=1$ which lets us update our priors on the parameters in the $a^*_i$ and $s_i^*$ equations.\\~\\

\pause $\rho=Corr(Y^*(0),Y^*(1)|s^*)$ is also unidentified.\\
\pause\ \ $\implies$ can learn about $E[TE|y]$ but maybe not about $p(TE|y)$...\\
\pause But we do learn about $Corr(Y^*(0),Y^*(1)|\tilde{x},\bm{z})$ through positive definite restrictions $\implies$ learning about $p(TE|y)$.

\end{frame}

\begin{frame}
p\frametitle{Evaluating the NSLP -- Open Questions for the Large Model}
\begin{enumerate}
\item Priors that acknowledge that households near the income cutoff but on opposite sides are similar.
\item Use outside information of classification error rates in the priors.
\item Check whether partial identification heuristics are correct.
\item Nonparametric link functions.
\item Nonparametric
\item Alternative specifications.
\item Still need to take into account the survey weights.
\end{enumerate}
\end{frame}



