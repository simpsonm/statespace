<<set-parent-combalg, echo=FALSE, cache=FALSE>>=
set_parent('mcmcex.Rnw')
@


\section{Combining Algorithms}

There are several ways to combine the above MCMC algorithms into a hybrid algorithm. The most straightforward method is an ``alternating'' algorithm. An alternating algorithm combines two or more samplers by drawing each sampler sequentially in a single iteration. For example, the alternating sampler that combines the state and scaled disturbance sampler is as follows. Let $(\theta_{0:T}^{(k)},V^k,W^k)$ denote the $k$'th iteration of the chain. To obtain the $k+1$'st iteration, run the following:

\begin{enumerate}
  \item Simulate $(\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T})$
  \item Simulate $(V,W|\theta_{0:T},y_{1:T})$
  \item Simulate $(\gamma_{0:T}|V,W,y_{0:T})$
  \item Simulate $(V^{(k+1)}|W^{(k+1)},\gamma_{0:T},y_{1:T})$
  \item Simulate $(W^{(k+1)}|V^{(k+1)},\gamma_{0:T},y_{1:T})$
  \item Form $\theta^{(k+1)}_{0:T}$ from $(\gamma_{0:T}^{(k+1)}, V^{(k+1)},W^{(k+1)}, y_{0:T})$
\end{enumerate}


In other words to get from $(\theta_{0:T}^{(k)},V^{(k)},W^{(k)})$ to $(\theta_{0:T}^{(k+1)},V^{(k+1)},W^{(k+1)})$, you run one iteration of the state sampler to obtain an intermediate iteration, then run one iteration from the scaled disturbance sampler to obtain the desired sample sample. In this way, any two or all three of the original three algorithms can be combined into an alternating algorithm. 

Another approach is given in \citet{yu2011center}. In their paper, Yu and Meng define an interweaving strategy. In the language and notation of the local level model, a single iteration of an interweaving strategy has the following steps, e.g. when interweaving between the state and scaled disturbance samplers:

\begin{enumerate}
  \item Simulate $(\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T})$
  \item Simulate $(V,W|\theta_{0:T},y_{1:T})$
  \item Form $\gamma_{0:T}$ from $(\theta_{0:T},V,W,y_{0:T})$
  \item Simulate $(V^{(k+1)}|W^{(k+1)},\gamma_{0:T},y_{1:T})$
  \item Simulate $(W^{(k+1)}|V^{(k+1)},\gamma_{0:T},y_{1:T})$
  \item Form $\theta^{(k+1)}_{0:T}$ from $(\gamma_{0:T}^{(k+1)}, V^{(k+1)},W^{(k+1)}, y_{0:T})$
\end{enumerate}

The key difference here is in what happens in step 3. Here instead of drawing $\gamma_{0:T}$ from $p(\gamma_{0:T}|V,W,y_{0:T})$, we're drawing it from $p(\gamma_{0:T}|\theta_{0:T},V,W,y_{0:T})$ which happens to be a degenerate distribution. Changing this one step speeds up the computation since drawing the full vector $\gamma_{0:T}$ (or $\theta_{0:T}$) is costly, but computing the transformation is cheap. In addition, \citeauthor{yu2011center} found that when one of the two samplers being interweaved uses an ancillary augmentation (AA) while the other uses a sufficient augmentation (SA), the interweaving sampler has much better convergence properties than the corresponding alternating sampler or either of the two samplers the interweaving and alternating samplers are composed of. In this case, combining any two of the base samplers (parameterizations) doesn't result in an AA-SA pair, but these are the natural parameterizations to consider. Note that when there is more than two parameterizations to choose between, there's a possibility of a hybrid alternating/interweaving sampler --- i.e. it alternates between the first two samplers but interweaves between the second one and the third one, or vice versa. We won't consider these samplers here.

A final way to combine the original three samplers is a random kernel sampler. For each iteration, the random kernel sampler chooses one of the three basic samplers randomly, then obtains the next iteration using that parameter vector. The probability of choosing any given sampler must remain constant throughout the sampling process --- at least after burn in. This makes it possible to somehow learn the optimal probabilities during the burn in, then use the probabilties to obtain the desired chain. We focus on random kernel samplers with a uniform distribution over possible parameterizations. Combined with the previous samplers, this gives us three base samplers and three samplers that are composed of any two or three of the base samplers for a total of 15 samplers. Table \ref{table:sams} displays all possible sampling schemes.

\begin{table}[h]
  \centering
  \scalebox{.9}{
  \begin{tabular}{|l|cccc|}\hline
    Base: & State (S) & Scaled Disturbance (SD) & Scaled Error (SE) & \\
    Alternating: & S + SD & S + SE & SD + SE & S + SD + SE\\
    Interweaving: & S + SD & S + SE & SD + SE & S + SD + SE\\
    Random Kernel: & $\frac{1}{2}$S + $\frac{1}{2}$SD & $\frac{1}{2}$S + $\frac{1}{2}$SE & $\frac{1}{2}$SD + $\frac{1}{2}$SE & $\frac{1}{3}$S + $\frac{1}{3}$SD + $\frac{1}{3}$SE\\
    \hline
  \end{tabular}
  }
  \caption{All posterior samplers considered for the local level model. The fractions in the random kernel sampler indicate the probability that each base sampler is chosen in a given iteration. Note that there are two samplers omitted which are a sort of hybrid between alternating and interweaving: the sampler that alternates between S and SD, then interweaves between SD and SE, and the sampler which reverses which steps are interweaving and which are alternating.}
  \label{table:sams}
\end{table}


