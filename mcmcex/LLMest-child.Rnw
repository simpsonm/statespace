<<set-parent-LLMest, echo=FALSE, cache=FALSE>>=
set_parent('mcmcex.Rnw')
@


\section{Application: The Local Level Model}

{\it INSERT A PARAGRAPH MOTIVATING WHY WE LOOK AT THE LOCAL LEVEL MODEL}

The local level model (LLM) is a DLM with univariate data $y_t$ for $t=1,2,\cdots,T$ and a univariate latent state $\theta_t$ for $t=0,2,\cdots,T$ that satisfies
\begin{align}
  y_t |\theta_{0:T}& \stackrel{ind}{\sim} N(\theta_t,V) \label{llmobseq}\\
  \theta_t |\theta_{0:t-1}& \sim N(\theta_{t-1},W) \label{llmsyseq}
\end{align}
with $\theta_0\sim N(m_0,C_0)$. Here $\theta_t=E[y_t|\theta_{0:T}]$, i.e. the average value of $y_t$. The states are $\theta_{0:T}$, the scaled disturbances are $\gamma_{0:T}$ with $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$, and the scaled errors are $\psi_{0:T}$ with $\psi_0=\theta_0$ and $\psi_t=(y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$. The independent inverse Wishart priors on $V$ and $W$ in Section \ref{modelsec} cash out to independent inverse gamma priors for the local level model, i.e. $V\sim IG(\alpha_V,\beta_V)$ and $W\sim IG(\alpha_W,\beta_W)$. 

\subsection{Base Samplers}\label{sec:llmbase}

The joint density of $(V,W,\theta_{0:T},y_{1:T})$ is:
\begin{align*}
  p(&V_,W,\theta_{0:T},y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)} \exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \theta_{t})^2\right)\right]\nonumber\\
  &W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(\theta_t - \theta_{t-1})^2\right) \right] \exp\left[-\frac{1}{2C_0}(\theta_0 - m_0)^2\right]%\label{llmstatejoint}
\end{align*}
This immediately gives the state sampler:
\begin{alg}[State Sampler for LLM]\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{llmstatealg}
\item Draw $\theta_{0:T}$ from $p(\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T})$ using FFBS.
\item Draw $(V^{(k+1)},W^{(k+1)})$ from $p(V,W|\theta_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
In step 2, $V$ and $W$ are independent with $V\sim IG(a_V,b_V)$ and $W\sim IG(a_W, b_W)$ where $a_V = \alpha_V + T/2$, $b_V = \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2$, $a_W = \alpha_W + T/2$, and $b_W = \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2$.

The scaled disturbance sampler, the DA algorithm based on the scaled disturbances, is a bit more complicated. In this context $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$, and thus $\theta_t = \sqrt{W}\sum_{t=1}^T\gamma_t + \gamma_0$ for $t=1,2,\cdots,T$. Following \eqref{dlmdistjoint}, we can write the joint posterior of $(V,W,\gamma_{0:T})$ as
\begin{align}
  p(&V,W,\gamma_{0:T}|y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)}\exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \gamma_0 - \sqrt{W}\textstyle\sum_{s=1}^t\gamma_s)^2\right)\right] \nonumber\\
  & \times W^{-(\alpha_W + 1)}\exp\left[-\frac{\beta_W}{W}\right] \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\gamma_t^2\right]\exp\left[-\frac{1}{2C_0}(\gamma_0-m_0)^2\right]\label{llmdistpost}
\end{align}
Now $V$ and $W$ are no longer conditionally independent given $\gamma_{0:T}$ and $y_{1:T}$. Instead of attempting the usual DA algorithm, we'll add an extra Gibbs step and draw $V$ and $W$ separately. This gives us the scaled disturbance sampler:
\begin{alg}[Scaled Disturbance Sampler for LLM]\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{llmdistalg}
\item Draw $\gamma_{0:T}$ from $p(\gamma_{0:T}|V^{(k)},W^{(k)},y_{1:T})$, possibly using FFBS to sample $\theta_{0:T}$ then transforming.
\item Draw $V^{(k+1)}$ from $p(V|W^{(k)},\gamma_{0:T},y_{1:T})$.
\item Draw $W^{(k+1)}$ from $p(W|V^{(k+1)},\gamma_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
In step 2, $V$ is drawn from the same inverse gamma distribution as in step 2 of Algorithm \ref{llmstatealg}. In step 3, the draw of $W$ is more complicated. The density can be written as
\begin{align*}
  p(W|V,\gamma_{0:T},y_{1:T}) \propto & W^{-\alpha_w - 1}\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \gamma_0 - \sqrt{W}\sum_{s=1}^t\gamma_s\right)^2\right].
\end{align*}
This density isn't any known form and is difficult to sample from. The log density can be written as
\begin{align*}
\log p(W|V,\gamma_{0:T},y_{1:T}) =& -aW + b\sqrt{W} - (\alpha_W + 1)\log W -\beta_W/W + C 
\end{align*}
where $C$ is some constant, $a=\sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$ and $b=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$. It can be shown that $b^2 > \frac{32}{9\beta_w}(\alpha_w+1)^3(1 - 2sgn(b)/3)$ implies that the density is log concave where
\begin{align*}
  sgn(b) & = \begin{cases} 1 &\text{if } b>0\\
    0 & \text{if } b=0\\
    -1 & \text{if } b<0.\\
    \end{cases}
\end{align*}
This condition is equivalent to $\partial^2\log p(W|.)/\partial W^2 < 0$ at the $W^*$ that maximizes $\partial^2\log p(W|.)/\partial W^2$ and hence guarantees the density is globally log-concave. It turns out that this tends to hold over a wide region of the parameter space --- so long as $V$ is smaller or isn't much larger than $W$. This allows for the use of adaptive rejection sampling in order to sample from this distribution in many cases, e.g. using \citet{gilks1992adaptive}. An alternative is to use a $t$ approximation to the conditional density as a proposal in a rejection sampler, but this is much more computationally expensive when necessary.

The scaled error sampler is similar to the scaled disturbance sampler, and this is easy to see in the local level model. Here $\psi_0=\theta_0$ and $\psi_t = (y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$ so that $\theta_t = y_t - \sqrt{V}\psi_t$ for $t=1,2,\cdots,T$. From \eqref{dlmerrorjoint} we can write $p(V,W,\psi_{0:T}|y_{1:T})$ as
\begin{align*}
    p(&V,W,\psi_{0:T},y_{1:T}) \propto W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(Ly_t + \sqrt{V}L\psi_t)^2\right)\right]\nonumber \\
 & V^{-(\alpha_V + 1)}\exp\left[-\frac{\beta_V}{V}\right]  \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\psi_t^2\right] \exp\left[-\frac{1}{2C_0}(\psi_0-m_0)^2 \right] 
\end{align*}
where we define $Ly_t=y_t-y_{t-1}$ for $t=2,3,...,T$ \& $Ly_1= y_1 - \psi_0$ and $L\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$ \& $L\psi_1=\psi_1-0$. Once again, $V$ and $W$ are no longer conditionally independent given $\psi_{0:T}$ and $y_{1:T}$. In fact, the density seems analgous to \eqref{llmdistpost} with $V$ and $W$ ``switching places'' so to speak. The scaled error sampler obtained from drawing $V$ and $W$ separately is:
\begin{alg}[Scaled Error Sampler for LLM]\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{llmerroralg}
\item Draw $\psi_{0:T}$ from $p(\psi_{0:T}|V^{(k)},W^{(k)},y_{1:T})$, possibly using FFBS to sample $\theta_{0:T}$ then transforming.
\item Draw $V^{(k+1)}$ from $p(V|W^{(k)},\psi_{0:T},y_{1:T})$.
\item Draw $W^{(k+1)}$ from $p(W|V^{(k+1)},\psi_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
In step 3, $W$ is drawn from the same inverse gamma distribution as in step 2 of Algorithm \ref{llmstatealg}. Drawing $V$ in step 2 is more complicated, but exactly analogous to drawing $W$ in Algorithm \ref{llmdistalg}. The log density of $V|W,\psi_{0:T},y_{1:T}$ can be written as
\begin{align*}
\log p(V|W,\psi_{0:T},y_{1:T}) =& -aV + b\sqrt{V} - (\alpha_V + 1)\log W -\beta_V/V + C 
\end{align*}
where again $C$ is some constant, but now $a=\sum_{t=1}^T(L\psi_t)^2/2W$ and $b=\sum_{t=1}^T(L\psi_tLy_t)/W$. So we can use the same methods to sample from this density -- adaptive rejection sampling, as in \citet{gilks1992adaptive}, will work as long as $b^2 > \frac{32}{9\beta_V}(\alpha_V+1)^3(1 - 2sgn(b)/3)$, and otherwise a $t$ proposal in a rejection sampler will work but will be substantially slower.

\subsection{Hybrid Samplers: Interweaving, Alternation and Random Kernel}
Section \ref{sec:dlminter} contains the details for the interweaving algorithms in the general DLM. In the local level model, there is little to add. We'll consider all four GIS samplers based on any two or three of the base samplers and one CIS sampler. In the GIS samplers, the order of the parameterizations will always be the states $(\theta_{0:T})$, then the scaled disturbances $(\gamma_{0:T})$, then the scaled errors $(\psi_{0:T})$. All of the GIS algorithms and the CIS algorithm are below in Table \ref{GISalgorithms}. Note the distributional forms for each of these steps (in some cases a transformation) are in Section \ref{sec:llmbase}. In Section \ref{sec:dlminter} we saw that one version of the CIS algorithms is the same as the ``Error-Dist'' GIS algorithm (i.e. the Dist-Error algorithm but flipping the order in which the DAs are used). The CIS algorithm below is not the same as the CIS algorith which is equivalent to the Error-Dist GIS algorithm, but the difference is only the order in which the DAs are used within each Gibbs step. Thus we don't expect it to perform much differently from the Dist-Error GIS algorithm, but we include it for completeness.
\begin{table}[!h]
  \centering
\begin{enumerate}
  \item State-Dist GIS algorithm:\\
    $[\theta_{0:T}|V,W]\to [V,W|\theta_{0:T}] \to [\gamma_{0:T}|V,W,\theta_{0:T}] \to [V|W,\gamma_{0:T}] \to [W|V,\gamma_{0:T}]$
  \item State-Error GIS algorithm:\\
    $[\theta_{0:T}|V,W]\to [V,W|\theta_{0:T}] \to [\psi_{0:T}|V,W,\theta_{0:T}] \to [V|W,\psi_{0:T}] \to [W|V,\psi_{0:T}]$
  \item Dist-Error GIS algorithm:\\
    $[\gamma_{0:T}|V,W]\to [V|W, \gamma_{0:T}] \to [W|V, \gamma_{0:T}] \to [\psi_{0:T}|V,W,\gamma_{0:T}] \to [V|W,\psi_{0:T}] \to [W|V,\psi_{0:T}]$
  \item Triple GIS algorithm:\\
    $[\theta_{0:T}|V,W]\to [V,W|\theta_{0:T}] \to [\gamma_{0:T}|V,W,\theta_{0:T}] \to [V|W,\gamma_{0:T}] \to [W|V,\gamma_{0:T}] \to [\psi_{0:T}|V,W,\gamma_{0:T}] \to [V|W,\psi_{0:T}] \to [W|V,\psi_{0:T}]$
  \item Full CIS algorithm:\\
        $[\theta_{0:T}|V,W]\to [V|W,\theta_{0:T}] \to [\psi_{0:T}|V,W,\theta_{0:T}] \to [V|W,\psi_{0:T}] \to [\theta_{0:T}|V,W] \to [W|V,\theta_{0:T}] \to [\gamma_{0:T}|V,W] \to [W|V,\gamma_{0:T}]$
\end{enumerate}
\caption{GIS Algorithms for the local level model}
\label{GISalgorithms}
\end{table}

Interweaving algorithms are conceptually very similar to alternating algorithms. For every GIS algorithm, there's a corresponding alternating algorithm  where each $[DA_2|V,W,DA_1]$ step is replaced by a $[DA_2|V,W]$ step (here $DA_i$ is a data augmentation for $i=1,2$.). Table \ref{altalgorithms} contains each alternating algorithm. Note that there are two possible ``hybrid triple'' algorithms that we don't consider here where the move from $\theta_{0:T}$ to $\gamma_{0:T}$ interweaves and while the move from $\gamma_{0:T}$ to $\psi_{0:T}$ alternates and vice versa.

\begin{table}[!h]
  \centering
\begin{enumerate}
  \item State-Dist alternating algorithm:\\
    $[\theta_{0:T}|V,W]\to [V,W|\theta_{0:T}] \to [\gamma_{0:T}|V,W] \to [V|W,\gamma_{0:T}] \to [W|V,\gamma_{0:T}]$
  \item State-Error alternating GIS algorithm:\\
    $[\theta_{0:T}|V,W]\to [V,W|\theta_{0:T}] \to [\psi_{0:T}|V,W] \to [V|W,\psi_{0:T}] \to [W|V,\psi_{0:T}]$
  \item Dist-Error alternating GIS algorithm:\\
    $[\gamma_{0:T}|V,W]\to [V|W, \gamma_{0:T}] \to [W|V, \gamma_{0:T}] \to [\psi_{0:T}|V,W] \to [V|W,\psi_{0:T}] \to [W|V,\psi_{0:T}]$
  \item Triple alternating GIS algorithm:\\
    $[\theta_{0:T}|V,W]\to [V,W|\theta_{0:T}] \to [\gamma_{0:T}|V,W] \to [V|W,\gamma_{0:T}] \to [W|V,\gamma_{0:T}] \to [\psi_{0:T}|V,W] \to [V|W,\psi_{0:T}] \to [W|V,\psi_{0:T}]$
\end{enumerate}
\caption{Alternating algorithms for the local level model}
\label{altalgorithms}
\end{table}

Finally, we also consider random kernel algorithms. In this context, a random kernel algorithm randomly chooses from the state sampler, scaled disturbance sampler, and scaled error sampler in each iteration where the selection probabilities are constant with respect to the iteration. We consider four random kernel algorithms based on any two or three of the base samplers with an equal probability of selecting each base sampler included in the algorithm. For example, the State-Dist random kernel algorithm selects either the state sampler or the scaled disturbance sampler with equal probability at every iteration, while the triple random kernel algorithm selections from the state sampler, scaled disturbance sampler, or the scaled error sampler with equal probability at every iteration.

Table \ref{LLMalgorithms} contains each algorithm we considered for the local level model. The basic idea here is that the alternating algorithms and the random kernel algorithms should serve as a sort of baseline to compare the corresponding interweaving algorithms against. The GIS algorithm should be slightly faster than the alternating algorithm since the only difference is one step becoming a transformation instead of a random draw, but the difference shouldn't be large. So we'd like the GIS algorithms to have at least as quick mixing as the corresponding alternating algorithms. The random kernel algorithms, however, have do do half as much computation to obtain a single draw (or a third as much computation in the case of the triple random kernel algorithm). Thus in some vague sense, we would like the GIS algorithms to have mixing which is twice as fast as the corresponding random kernel algorithm, or three times as fast in the case of the triple algorithms. We can make this notion precise by considering the effective sample size (ESS) of the Markov chain -- we'd like the GIS algorithms to have an ESS about twice as large as the corresponding random kernel algorithms, or three times as large for the triple algorithms.

\begin{table}[!h]
  \centering
  \begin{tabular}{|l||l|l|l|l|}\hline
    Type & & & & \\\hline
    Base & State & Scaled Disturbance & Scaled Error & \\\hline
    GIS & State-Dist & State-Error & Dist-Error & Triple \\
    Alt & State-Dist & State-Error & Dist-Error & Triple \\
    RandKern & $\frac{1}{2}$State $+\frac{1}{2}$Dist & $\frac{1}{2}$State $+\frac{1}{2}$Error & $\frac{1}{2}$Dist $+\frac{1}{2}$Error & $\frac{1}{3}$State $+\frac{1}{3}$Dist $+\frac{1}{3}$Error \\\cline{1-5}
    CIS & \multicolumn{3}{l|}{State-Error for $V|W$; State-Dist for $W|V$} & \\
    \hline
  \end{tabular}
  \caption{Each algorithm considered for the local level model}
  \label{LLMalgorithms}
\end{table}

\subsection{Simulation Setup}

In order to test these algorithms, we simulated a fake dataset from the local level model for various choices of $V$, $W$, and $T$. We created a grid over $V$--$W$ space with $(V,W)$ ranging from $(10^{-2},10^{-2})$ to $(10^2, 10^2)$ and we simulated a dataset for all possible combinations of $V$ and $W$ with each of $T=10, 100, 1000$. Then for each dataset, we fit the local level model using each algorithm in Table \ref{LLMalgorithms}. We used the same priors for each model: $\theta_0\sim N(0,10^7)$, $V\sim IG(5, 4\tilde{V})$, and $W\sim IG(5, 4\tilde{W})$, mutually independent where $(\tilde{V},\tilde{W})$ are the true values of $V$ and $W$ used to simulate the time series. This ensures all priors and thus the posterior are proper and results in independent inverse gamma priors on $V$ and $W$ with prior means equal to the respective true values and moderately large prior variances. We chose these priors for $(V,W)$ for each sampler to sidestep issues of posterior propriety or noninformativeness for variances since they are largely orthogonal to our goals.

For each model and each sampler, we $n=3000$ draws and threw away the first $500$ as burn in. The chains were started at the true values used to simulated the time series, so we can examine the behavior of the chains to determine how well they mix but not how quickly they convergence. Define the effective sample proportion (ESP) for a scalar component of the chain as the effective sample size (ESS, REFERENCE HERE) of the component divided by the actual sample size, i.e. $ESP=ESS/n$. An $ESP=1$ indicates that the Markov chain is behaving as if it obtains iid draws from the posterior. It's possible to obtain $ESP>1$ if the draws are negatively correlated and this happens occasionally with some of our samplers, but we round this down to $ESP=1$ in order to simplify our plots.


\subsection{Base Results}

<<plotsetup, echo=FALSE, message=FALSE>>=
library(ggplot2)
library(scales)
library(plyr)
library(xtable)
library(reshape2)
library(gridExtra)
load("mixing/samout.RData")
load("mixing/postcors.RData")
sams <- c("error", "state", "dist")
alts <- c("sdalt", "sealt", "dealt", "trialt")
ints <- c("sdint", "seint", "deint", "triint")
kerns <- c("sdkern", "sekern", "dekern", "trikern")
samout$V.ES[samout$sampler %in% kerns] <- samout$V.ES[samout$sampler %in% kerns]*2
samout$W.ES[samout$sampler %in% kerns] <- samout$W.ES[samout$sampler %in% kerns]*2
samout$V.ES[samout$sampler == "trikern"] <- samout$V.ES[samout$sampler == "trikern"]*(3/2)
samout$W.ES[samout$sampler == "trikern"] <- samout$W.ES[samout$sampler == "trikern"]*(3/2)
samout$type <- "Base" #$
samout$type[samout$sampler %in% alts] <- "Alt" 
samout$type[samout$sampler %in% ints] <- "GIS" 
samout$type[samout$sampler %in% kerns] <- "RKern" 
samout$samplers <- "Base" #$
samout$samplers[substr(samout$sampler, 1, 2)=="sd"] <- "State-Dist" 
samout$samplers[substr(samout$sampler, 1, 2)=="se"] <- "State-Error" 
samout$samplers[substr(samout$sampler, 1, 2)=="de"] <- "Dist-Error" 
samout$samplers[substr(samout$sampler, 1, 3)=="tri"] <- "Triple" 
samout$samplers[samout$sampler=="error"] <- "Error"
samout$samplers[samout$sampler=="dist"] <- "Dist"
samout$samplers[samout$sampler=="state"] <- "State"
meltedsam <- melt(samout, id=c("type", "samplers", "sampler", "V.T", "W.T", 
                            "T.T"))
Vs <- unique(meltedsam$V.T)[1:9] #$
Ws <- Vs
breaks <- Vs[seq(1,9,2)]
label_both_parsed <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), function(x) parse(text = x))
}
label_both_parsed_split <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), 
        function(x) parse(text = strsplit(x, "\\.")[[1]][1]))
}
label_parsed_split <- function(variable, value){
  llply(as.character(value), function(x) parse(text = strsplit(x, "\\.")[[1]][1]))
}
meltedcors <- melt(postcors, id=c("V.T", "W.T", "T.T"))
## opts_chunk$set(fig.width=7, fig.height=4, out.width='1\\textwidth', 
##               fig.pos='!ht') #$
plotfun <- function(meltedsam, vars, sams, T){
  castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable + samplers ~ ., 
                     subset=.(variable %in% vars  & T.T==T & sampler %in% sams ))
  colnames(castedsam)[6] <- "value"
  out <- ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
         geom_tile() +
         scale_fill_gradient("ESP", low=muted("red"), high="white",
           guide=guide_colorbar(barheight=10),
           limits=c(0,1), na.value="white") +
         facet_grid(variable~samplers, scales="free", labeller=label_both_parsed_split) +
         scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
         ggtitle("ESP for V and W in the base samplers, T=10") +
         theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
  return(out)
}

@ 

Figure \ref{baseESplot} contains plots of ESP for $V$ and $W$ in each chain of each base sampler for each of $T=10$, $T=100$, and $T=1000$. We'll focus on $T=10$ first. The state sampler has a low ESP for $V$ and a high ESP for $W$ when the signal-to-noise ratio, $W/V$, is larger than one. When the signal-to-noise ratio is smaller than one, on the other hand, the state sampler has a low ESP for $W$ and a high ESP for $V$. In the usual case where the signal to noise ratio isn't too different from one, the state sampler has a modest to low ESP for both $V$ and $W$. Note that the particular values of $V$ and $W$ don't seem to matter at all --- just their relative values, i.e. the signal-to-noise ratio $W/V$. Moving up any diagonal on the plots for $V$ and $W$ in the state sampler, $W/V$ is constant and the ESS appears roughly constant. The basic lesson here is that the state sampler has mixing issues for whichever of $V$ or $W$ is smaller. 

Figure \ref{baseESplot} tells a different story for the scaled disturbance sampler. When the signal-to-noise ratio is less than one, ESPs for both $V$ and $W$ are nearly 1, i.e. the effective sample size is nearly the actual sample size of the chain. When the signal-to-noise ratio is greater than one, however, ESP for both $V$ and $W$ becomes small, especially for $V$. Once again the absolute values of $V$ and $W$ don't matter for this behavior --- just the relative values. The scaled error sampler has essentially the opposite properties. When $W/V$ is large, it has a near 1 ESP for both $V$ and $W$. On the other hand, when $W/V$ is small is has a low ESP for both $V$ and $W$, especially for $V$. The lesson here seems to be that the scaled disturbances ($\gamma_{0:T}$) are the preferred data augmentation for low signal-to-noise ratios and the scaled errors ($\psi_{0:T}$) are the preferred data augmentation for high signal-to-noise ratios, while the states ($\theta_{0:T}$) are preferred for signal-to-noise ratios near 1.

The plots for $T=100$ and $T=1000$ in Figure \ref{baseESplot} tell basically the same story, with a twist. Increasing the length of the time series seems to exacerbate all problems without changing the basic conclusions. As $T$ increases, $W/V$ has to be smaller and smaller for the scaled disturbance sampler to have decent mixing, and similarly $W/V$ has to be larger and larger for the scaled error sampler to have decent mixing. Interestingly, the scaled error sampler appears to mix well for both $V$ and $W$ over a larger region of the space $W/V<1$ than the scaled disturbance sampler does over $W/V>1$. The state sampler is stuck between a rock and a hard place, so to speak, since as $T$ increases, good mixing for $V$ requires $W/V$ to be smaller and smaller, but good mixing for $W$ requires $W/V$ to be larger and larger. 

<<baseESplot, fig.cap=cap, echo=FALSE, fig.height=4.5, fig.width=6, out.width=".48\\textwidth">>=
cap <- "Effective sample proportion in the posterior sampler for a time series of lengths $T=10$, $T=100$, and $T=1000$, for $V$ and $W$, and for the state, scaled disturbance, and scaled error samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Also note that for plotting purposes, effective sample proportions larger than $1$ were rounded down to $1$"
vars <- c("V.ES", "W.ES")
p1 <- plotfun(meltedsam, vars, sams, 10)
p2 <- plotfun(meltedsam, vars, sams, 100)
p3 <- plotfun(meltedsam, vars, sams, 1000)
p1
p2
p3
@ 

Since GIS algorithms work best when one algorithm works well in one region of the parameter space while the other works well in the other region of the parameter space, this suggests that the dist-error GIS algorithm will have the best performance of the bunch, though perhaps worse than the triple GIS algorithm. For large $T$, however, this suggests that none of the interweaving algorithms will perform well for $W/V$ not too far from one, i.e. the typically use-case. 

It's also worth noting that both the scaled error and scaled disturbance samplers run into trouble with their adaptive rejection sampling step in precisely the same region of the parameter space where they have good mixing for both $V$ and $W$, though as $T$ increases, this only happens in the increasingly extreme ends of the parameter space. More precisely, when $W/V>1$, $p(W|V,\psi_{0:T},y_{1:T})$ will often fail to be log concave, and when $W/V<1$, $p(V|W,\gamma_{0:T},y_{1:T})$ will often fail to be log concave, but as $T$ increases the degree to which $W/V$ must differ from one (in the appropriate direction) in order for log concavity to often or even occasionally fail increases. Outside of these respective regions, log-concavity of the relevant density failing is an extremely unlikely occurence. As a result, the adapftive rejection sampling algorithm of \citet{gilks1992adaptive} won't work in general. Another option is to give up directly sampling from either conditonal density and use a metropolis step, perhaps for $(V,W)$ jointly, though we haven't tried that. In general, the sampling algorithm should be prepared to use something other than adaptive rejection sampling if necessary because it's possible that the chain enters a region of the parameter space where the relevant density is not log concave, no matter what the likely values of $V$ and $W$ are. {\it NOTE: ADD DETAILS ABOUT PRECISELY HOW LARGE OR SMALL $W/V$ HAS TO BE TO THIS PARAGRAPH}


<<altintESplot, fig.cap=cap, echo=FALSE, fig.height=4, fig.width=5.5, out.width=".49\\textwidth">>=
cap <- "Effective sample proportion in the posterior sampler for a time series of length $T=10$, $T=100$, and $T=1000$, for $V$ and $W$, and for the GIS and alternating samplers based on the state, scaled disturbance, and scaled error samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Also note that for plotting purposes, effective sample proportions larger than $1$ were rounded down to $1$"
vars <- c("V.ES")
T <- 10
castedsam <- dcast(meltedsam, 
                   formula=samplers + V.T + W.T + variable + type ~ ., 
                   subset=.(variable %in% vars  & T.T==T & 
                     sampler %in% c(alts, ints, kerns) ))
colnames(castedsam)[6] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
  geom_tile() +
  scale_fill_gradient("ESP", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(type~samplers, scales="free", labeller=label_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + 
  scale_y_log10("W = signal", breaks=breaks) +
  ggtitle(paste("ESP for V in the hybrid samplers, T=",T,sep="")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
vars <- c("W.ES")
castedsam <- dcast(meltedsam, 
                   formula=samplers + V.T + W.T + variable + type ~ ., 
                   subset=.(variable %in% vars  & T.T==T & 
                     sampler %in% c(alts, ints, kerns) ))
colnames(castedsam)[6] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + 
  geom_tile() +
  scale_fill_gradient("ESP", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(type~samplers, scales="free", labeller=label_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + 
  scale_y_log10("W = signal", breaks=breaks) +
  ggtitle(paste("ESP for W in the hybrid samplers, T=",T,sep="")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
T <- 100
castedsam <- dcast(meltedsam, 
                   formula=samplers + V.T + W.T + variable + type ~ ., 
                   subset=.(variable %in% vars  & T.T==T & 
                     sampler %in% c(alts, ints, kerns) ))
colnames(castedsam)[6] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
  geom_tile() +
  scale_fill_gradient("ESP", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(type~samplers, scales="free", labeller=label_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + 
  scale_y_log10("W = signal", breaks=breaks) +
  ggtitle(paste("ESP for V in the hybrid samplers, T=",T,sep="")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
vars <- c("W.ES")
castedsam <- dcast(meltedsam, 
                   formula=samplers + V.T + W.T + variable + type ~ ., 
                   subset=.(variable %in% vars  & T.T==T & 
                     sampler %in% c(alts, ints, kerns) ))
colnames(castedsam)[6] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + 
  geom_tile() +
  scale_fill_gradient("ESP", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(type~samplers, scales="free", labeller=label_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + 
  scale_y_log10("W = signal", breaks=breaks) +
  ggtitle(paste("ESP for W in the hybrid samplers, T=",T,sep="")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
T <- 1000
castedsam <- dcast(meltedsam, 
                   formula=samplers + V.T + W.T + variable + type ~ ., 
                   subset=.(variable %in% vars  & T.T==T & 
                     sampler %in% c(alts, ints, kerns) ))
colnames(castedsam)[6] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
  geom_tile() +
  scale_fill_gradient("ESP", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(type~samplers, scales="free", labeller=label_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + 
  scale_y_log10("W = signal", breaks=breaks) +
  ggtitle(paste("ESP for V in the hybrid samplers, T=",T,sep="")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
vars <- c("W.ES")
castedsam <- dcast(meltedsam, 
                   formula=samplers + V.T + W.T + variable + type ~ ., 
                   subset=.(variable %in% vars  & T.T==T & 
                     sampler %in% c(alts, ints, kerns) ))
colnames(castedsam)[6] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + 
  geom_tile() +
  scale_fill_gradient("ESP", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(type~samplers, scales="free", labeller=label_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + 
  scale_y_log10("W = signal", breaks=breaks) +
  ggtitle(paste("ESP for W in the hybrid samplers, T=",T,sep="")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))

@ 


