\documentclass{article}
\usepackage{graphicx, color, amssymb, amsmath, bm, rotating, graphics,
epsfig, multicol}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{caption}
\begin{document}

% \SweaveOpts{fig.path='figure/', fig.align='center', fig.show='hold'}

<<setup, include=FALSE, cache=FALSE>>=
options(replace.assign=TRUE,width=90)
@


\title{MCMC Examples for DLMs}


\author{Matt Simpson}

\maketitle

\section{Introduction}

In this note, I compare the performance of two samplers
for exploring the posterior distribution of a local level state space
model. Suppose we have a univariate time series $y_t$ for
$t=1,2,...,T$. The local level model says
\begin{align*}
  y_t &= \theta_t + v_t\\
  \theta_t & = \theta_{t-1} + w_t
\end{align*}
for $t=1,2,...,T$ with
\[
\begin{bmatrix} v_t \\ w_t \end{bmatrix} \stackrel{iid}{\sim}
N\left(\begin{bmatrix} 0\\0\end{bmatrix}, \begin{bmatrix} V & 0 \\ 0 &
    W \end{bmatrix}\right)
\]
and
\[
\theta_0\sim N(m_0, C_0)
\]

$V$ and $W$ are treated as unkown, and we put independent inverse gamma
priors on each, i.e. $V\sim IG(\alpha_1, \beta_1)$ and $W\sim
IG(\alpha_2, \beta_2)$. The goal, then, is to simulate from the posterior
distribution of $(V,W,\theta_{0:T} | y_{0:T})$.

\subsection{Algorithm 1: The State Sampler}

The standard algorithm for doing this, which I'm calling the ``state
sampler,'' relies on Forward Filtering Backward Sampling (FFBS). FFBS
is an algorithm for sampling from the latent states, $\theta_{0:T}$
conditional on any unknown parameters. The algorithm samples the
states condition on $(V,W)$, then samples $(V,W)$ conditional on the
states, hence the name. In detail, the algorithm is as follows.

\begin{enumerate}
  \item Simulate $\theta_{0:T}$ from $\pi(\theta_{0:T}|V,W,y_{1:T})$
    using FFBS
  \item Simulate $V$ from $\pi(V|\theta_{0:T},W,y_{1:T})$:
    \[
    V|\theta_{0:T},W,y_{1:T} \sim IG(a_V, b_V)
    \]
    where
    \begin{align*}
      a_V =& \alpha_1 + T/2\\
      b_V = & \beta_1 + \sum_{t=1}^T(y_t-\theta_t)^2/2
    \end{align*}
  \item Simulate $W$ from $\pi(W|\theta_{0:T},V,y_{1:T})$:
    \[
    W|\theta_{0:T},V,y_{1:T} \sim IG(a_W, b_W)
    \]
    where
    \begin{align*}
      a_W =& \alpha_2 + T/2\\
      b_W = & \beta_1 + \sum_{t=1}^T(\theta_t-\theta_{t-1})^2/2
    \end{align*}
\end{enumerate}

\subsection{Algorithm 2: The Scaled Disturbance Sampler}

A second algorithm worth considering  samples $V$ and $W$ conditional
on $\gamma_{0:T}$ instead of conditional on $\theta_{0:T}$,  with
$\gamma_t = (\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,...,T$ and
$\gamma_0=\theta_0$. Note that for $t>0$, $\gamma_t = w_t/\sqrt{W}$,
i.e. the $\gamma$'s are the disturbances from the system equation, but
scaled so that they have a $N(0,1)$ distribution a priori. The basic
idea here is to sample the scaled disturbances conditional on $(V,W)$,
then sample $V$ conditional on the scaled disturbances and $W$, then
$W$ conditional on the scaled disturbances and $V$. Sampling the
scaled disturbances can be accomplished using FFBS to sample the
states, then transform the states to the scaled disturbances using the
formulas above, or by sampling them directly using the disturbance
smoother. The full algorithm is as follows:

\begin{enumerate}
  \item Simulate $\theta_{0:T}$ from $\pi(\theta_{0:T}|V,W,y_{0:T})$
    using FFBS and form $\gamma_{0:T}$.
  \item Simulate $V$ from $\pi(V|\gamma_{0:T},W,y_{1:T})$:
    \[
    V|\gamma_{0:T},W,y_{1:T} \sim IG(a_V, b_V)
    \]
    where
    \begin{align*}
      a_V =& \alpha_1 + T/2\\
      b_V = & \beta_1 + \sum_{t=1}^T(y_t-\gamma_0 -
      \sqrt{W}\sum_{j=1}^t\gamma_j)^2/2
    \end{align*}
  \item Simulate $W$ from $\pi(W|\gamma_{0:T},V,y_{1:T})$:
    \[
    \mathcal{L} \equiv \log \pi(W|\gamma_{0:T},V,y_{1:T}) = -aW +
    b\sqrt{W} - (\alpha_2 + 1)\log W -\beta_2/W +C
    \]
    where $C$ is some constant we can ignore for sampling purposes,
    $a=\sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$ and
    $b=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$.
\end{enumerate}

Step 3 of this algorithm is of some interest since the distribution to
be simulated from isn't standard. One possibility (suggested by
Dr. Roy) is to use an adaptive rejection sampling algorithm that
depends on the density being log-concave. This doesn't always work
because the target density isn't log-concave in general (Dr. Roy made
a mistake confusing $W$ and $\sqrt{W}$ in his note that led him to
derive an incorrect conditional density for $W$ which was
log-concave. The correct density, here, is clearly not guaranteed to
be log-concave). When $W/V$, the signal-to-noise ratio, is above 1 the
density tends to be log-concave and adaptive rejection sampling works
well. When the signal-to-noise ratio is below 1, the density is often not
log concave. In this case a proposal density from the location-scale
family of $t$ distributions works, albeit with some additional
computational cost. In this case, setting the location parameter equal
to the target density's mode and the scale parameter equal to the
square root of the negative second derivative of the log of the target
density results in a family of proposal densities defined by the
degrees of freedom parameter, $df$. Then it's just a matter of
choosing $df$ to minimize the rejection rate.

\subsection{Algorithm 3: The Scaled Error Sampler}

A final samples $V$ and $W$ conditional
on $\psi_{0:T}$ instead of conditional on $\theta_{0:T}$,  with
$\psi_t = (y_t - \theta_{t})/\sqrt{V}$ for $t=1,2,...,T$ and
$\psi_0=\theta_0$. Note that for $t>0$, $\psi_t = v_t/\sqrt{V}$,
i.e. the $\psi$'s are the errors from the observation equation, but
scaled so that they have a $N(0,1)$ distribution a priori. The basic
idea here is to sample the scaled errors conditional on $(V,W)$,
then sample $W$ conditional on the scaled errors and $V$, then
$V$ conditional on the scaled errors and $W$. Sampling the
scaled errors, like the scaled disturbances, can be accomplished using
FFBS to sample the states, then transform the states to the scaled
errors using the formulas above, or by sampling them directly using
the disturbance smoother. The full algorithm is as follows:

\begin{enumerate}
  \item Simulate $\theta_{0:T}$ from $\pi(\theta_{0:T}|V,W,y_{0:T})$
    using FFBS and form $\psi_{0:T}$.
  \item Simulate $W$ from $\pi(W|\psi_{0:T},V,y_{1:T})$:
    \[
    W|\psi_{0:T},V,y_{1:T} \sim IG(a_W, b_W)
    \]
    where
    \begin{align*}
      a_W =& \alpha_2 + T/2\\
      b_W = & \beta_2 + \sum_{t=1}^T(y_t- y_{t-1} -
      \sqrt{V}(\psi_t-\psi_{t-1}))^2/2
    \end{align*}
    (defining $y_0=0$)
  \item Simulate $V$ from $\pi(V|\psi_{0:T},W,y_{1:T})$:
    \[
    \mathcal{L} \equiv \log \pi(V|\psi_{0:T},W,y_{1:T}) = -aV +
    b\sqrt{V} - (\alpha_1 + 1)\log V -\beta_1/V +C
    \]
    where $C$ is some constant we can ignore for sampling purposes,
    $a=\sum_{t=1}^T(L\psi_t)^2/2W$ and
    $b=\sum_{t=1}^T(L\psi_tLy_t)/W$ where
    \begin{align*}
      L\psi_t &= \begin{cases} \psi_t - \psi_{t-1} &\text{for } t=2,3,...,T\\
                                \psi_1 & \text{for } t=1\end{cases}\\
      Ly_t & =\begin{cases} y_t - y_{t-1} &\text{for } t=2,3,...,T\\
                                y_1 - \psi_0 & \text{for } t=1\end{cases}\\
    \end{align*}
\end{enumerate}

Step 3 of this algorithm is again of some interest since the
distribution to be simulated from isn't standard. Notice that the
target density has the same form as the target in algorithm 2, so we
can use the same methods to directly simulate from it.

\section{Results}

<<tablesetup, echo=FALSE>>=
library(xtable)
load("scors.Rdata")
load("dcors.Rdata")
load("ecors.Rdata")
cnam <- colnames(scors[[2]][[1]])
rnam <- rownames(scors[[2]][[1]])
algn <- paste(paste("l",paste(rep("r",length(rnam)), collapse=""), sep=""),"", sep="")
sclbx <- 0.79
@

In order to test these algorithms, I simulated a fake dataset from
the local level model for various choices of $V$, $W$, and $T$ ---
i.e. all possible combinations of $V=0.01,\ 0.1,\ 1,\ 10,\ 100,\
1000$, $W=0.01,\ 0.1,\ 1,\ 10,\ 100,\ 1000$, and $T=10,\ 100,\
1000$. Then for each dataset, I fit the local level model using each
sampler. For both each sampler, I used the same priors: $m_0=0$,
$C_0=10^7$, $\alpha_1=\alpha_2=5$, $\beta_1=(\alpha_1-1)V$ and
$\beta_2=(\alpha_2-1)W$ where $(V,W)$ were the true values of $V$ and
$W$ used to simulate the time series. This ensures all priors and thus
the posterior are proper and results in independent inverse gamma
priors on $V$ and $W$ with prior means equal to the respective true
values and moderately large prior variances. For each sampler, I
obtained a sample of size $n=2000$ and threw away the first 500
samples as burn in. In the scaled disturbance sampler, I used adaptive
rejection sampling to sample $W$ when log-concavity was satisfied, and
the rejection sampling scheme with $t$ proposal mention above in all
other situations. Similarly, in the scaled error sampler, I used
adaptive rejection sampling to sample $V$ when it's density was
log-concave and the $t$-proposal rejection sampling scheme in all
other situations. Once the sampling was complete, I computed the
autocorrelation in the chain for each parameter, including each of the
$\theta$'s. The following tables show what I found.

\subsection{Results for $(V,W)$}

Table \ref{tableVWT10} shows the autocorrelation for $V$ and $W$ for
all three samplers in all of the fitted models for $T=10$. We can see
that for the state sampler when the the signal-to-noise ratio ($W/V$)
is high, $V$ has very low autocorrelation in the posterior sampler
while $W$ has rather high autocorrelation. On the other hand, when the
signal to noise ratio is low, $V$ has high autocorrelation while $W$
has low autocorrelation in the posterior sampler. Neither of these
facts seem affected by the actual values of $W$ and $V$, just their
relative values. When the signal to noise ratio is constant (down any
diagonal in the table) the first order autocorrelation for both $V$
and $W$ looks about constant even while $W$ and $V$ are changing (by
the same factor).  Note that the autocorrelation is never all that bad
even when it becomes high in one of the chains --- never above $0.6$
or so.

The scaled disturbance sampler had significantly different
results. When the signal-to-noise ratio is high, autocorrelation is
low for both $V$ and $W$. However, whent he signal-to-noise ratio is
low, autocorrelation is high for both $V$ and $W$ --- albeit it gets
much worse for $W$ than for $V$, maxing out at $0.99$ and $0.6$ or
so respectively. Again for constant signal-to-noise ratio, the values
of $V$ and $W$ respectively don't seem to matter much for
autocorrelation. Only the signal-to-noise ratio matters.

Finally, the scaled error sampler is essentially the opposite of the
scaled disturbance sampler. When the signal-to-noise ratio is low,
autocorrelation is low for both $V$ and $W$ and when the ratio is
high, autocorrelation is high for both $V$ and $W$. The scaled error
sampler also seems to have bigger problems with $V$ than with $W$ ---
the autocorrelation gets as high as about 0.99 for $V$ but only about
0.65 for $W$.

Now we can look at Table \ref{tableVWT100} and see essentially the
same pattern when $T=100$. The state sampler has low autocorrelation for $W$ and
high autocorrelation for $V$ when the signal-to-noise ratio is low and
high autocorrelation for $W$ and low autocorrelation for $V$ when the
signal-to-noise ratio is high. Note, however, that this time the
autocorrelation problem becomes much worse when the signal-to-noise
ratio is significantly different from 1 - reaching to about $0.9$ in
the worst cases.

The scaled disturbance sampler is also repeating the
same pattern --- high signal-to-noise ratio begets low autocorrelation
in both $V$ and $W$, while low signal-to-noise ratio begets high
autocorrelation in both $V$ and $W$. However now the autocorrelation
for $V$ now gets as high as $0.94$ while the autocorrelation for $W$
still gets to about $0.99$, but it gets there for somewhat higher
signal-to-noise ratios than for the smaller sample size. The scaled
error sampler once again has the opposite pattern with high
autocorrelation for a high signal-to-noise ratio and low
autocorrelation for a low signal-to-noise ratio. Similar to the scaled
disturbance sampler, the scaled error sampler also has worse
autocorelation problems when the problems do arise. The
autocorrelation for $V$ gets as high as $0.99$ again while the
autocorrelation for $W$ gets up to $0.94$ is the worst case.

Finally in Table \ref{tableVWT1000} we see the same pattern repeated
again for $T=1000$. The trend also seems to continue with respect to
the length of the time series --- as $T$ increases, problems with
autocorrelation only become worse. The error and disturbance samplers
continue to be exact opposites --- though note that the scaled error
sampler appears to have a slightly larger range of the parameter space
under which it has acceptable autocorrelation than the scaled
disturbance sampler (compare the top right corners of the error
sampler tables with the bottom left corners of the disturbance sampler
tables).

To sum up the results for $V$ and $W$, the state sampler is good for
$V$ when the signal-to-noise ratio is high; good for $W$ when the
signal-to-noise ratio is low; and good for $(V,W)$ when the
signal-to-noise ratio is near 1. The scaled disturbance sampler is
good for $(V,W)$ or either individually when the signal-to-noise ratio
is high and the scaled error sampler is good for $(V,W)$ or either
individually when the signal-to-noise ratio is low. Increasing $T$,
the length of the time series, only exacerbates autocorrelation
problems and restricts further the range of the parameter space where
any of the samplers work well.

Figures \ref{stateplot}, \ref{distplot}, and \ref{errorplot} contain
plots of autocorrelation vs. $T$ (length of the time series) in order
to illustrate these things. Figure \ref{stateplot} contains the plots
for the state sampler, Figure \ref{distplot} contains the plots for
the scaled disturbance sampler, and Figure \ref{errorplot} contains
the plots for the scaled error sampler. The basic trend as $T$
increases becomes apparent --- as long as the autocorrelation isn't
tiny (near 0) or huge (near 1), increasing $T$ seems to increase the
autocorrelation in the sampler for both variables. Sometimes when the
autocorrelation starts out tiny, increasing $T$ still seems to
increase the autocorrelation. The patterns across true values of $V$
and $W$ are also apparent.

%V,W tables for T=10
\begin{table}[htb]
\begin{minipage}{.5\textwidth}
\centering
<<tableT10Vs, echo=FALSE, results='asis'>>=
T10Vs <- xtable(scors[[2]][[1]], digit=4, align=algn)
print(T10Vs,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{V in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10Ws, echo=FALSE, results='asis'>>=
T10Ws <- xtable(scors[[3]][[1]], digit=4, align=algn)
print(T10Ws,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{W in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10Vd, echo=FALSE, results='asis'>>=
T10Vd <- xtable(dcors[[2]][[1]], digit=4, align=algn)
print(T10Vd,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{V in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10Wd, echo=FALSE, results='asis'>>=
T10Wd <- xtable(dcors[[3]][[1]], digit=4, align=algn)
print(T10Wd,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{W in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10Ve, echo=FALSE, results='asis'>>=
T10Ve <- xtable(ecors[[2]][[1]], digit=4, align=algn)
print(T10Ve,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{V in Scaled Error Sampler}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10We, echo=FALSE, results='asis'>>=
T10We <- xtable(ecors[[3]][[1]], digit=4, align=algn)
print(T10We,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{W in Scaled Error Sampler}
\end{minipage}
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=10$ for $V$ and $W$ and for the state, scaled
  disturbance, and scaled error samplers. Row and column names
  indicate the true values of $W$ and $V$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  across any diagonal.}
\label{tableVWT10}
\end{table}

%V,W tables for T=100
\begin{table}[htb]
\begin{minipage}{.5\textwidth}
\centering
<<tableT100Vs, echo=FALSE, results='asis'>>=
T100Vs <- xtable(scors[[2]][[2]], digit=4, align=algn)
print(T100Vs,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{V in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100Ws, echo=FALSE, results='asis'>>=
T100Ws <- xtable(scors[[3]][[2]], digit=4, align=algn)
print(T100Ws,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{W in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100Vd, echo=FALSE, results='asis'>>=
T100Vd <- xtable(dcors[[2]][[2]], digit=4, align=algn)
print(T100Vd,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{V in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100Wd, echo=FALSE, results='asis'>>=
T100Wd <- xtable(dcors[[3]][[2]], digit=4, align=algn)
print(T100Wd,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{W in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100Ve, echo=FALSE, results='asis'>>=
T100Ve <- xtable(ecors[[2]][[2]], digit=4, align=algn)
print(T100Ve,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{V in Scaled Error Sampler}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100We, echo=FALSE, results='asis'>>=
T100We <- xtable(ecors[[3]][[2]], digit=4, align=algn)
print(T100We,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{W in Scaled Error Sampler}
\end{minipage}
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=100$ for $V$ and $W$ and for the state, scaled
  disturbance, and scaled error samplers. Row and column names
  indicate the true values of $W$ and $V$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  across any diagonal.}
\label{tableVWT100}
\end{table}

%V,W tables for T=1000
\begin{table}[htb]
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000Vs, echo=FALSE, results='asis'>>=
T1000Vs <- xtable(scors[[2]][[3]], digit=4, align=algn)
print(T1000Vs,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{V in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000Ws, echo=FALSE, results='asis'>>=
T1000Ws <- xtable(scors[[3]][[3]], digit=4, align=algn)
print(T1000Ws,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{W in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000Vd, echo=FALSE, results='asis'>>=
T1000Vd <- xtable(dcors[[2]][[3]], digit=4, align=algn)
print(T1000Vd,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{V in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000Wd, echo=FALSE, results='asis'>>=
T1000Wd <- xtable(dcors[[3]][[3]], digit=4, align=algn)
print(T1000Wd,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{W in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000Ve, echo=FALSE, results='asis'>>=
T1000Ve <- xtable(ecors[[2]][[3]], digit=4, align=algn)
print(T1000Ve,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{V in Scaled Error Sampler}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000We, echo=FALSE, results='asis'>>=
T1000We <- xtable(ecors[[3]][[3]], digit=4, align=algn)
print(T1000We,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{W in Scaled Error Sampler}
\end{minipage}
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=1000$ for $V$ and $W$ and for the state, scaled
  disturbance, and scaled error samplers. Row and column names
  indicate the true values of $W$ and $V$ respectively for the
  simulated data. Note that the signal-to-noise ratio is constant
  across any diagonal.}
\label{tableVWT1000}
\end{table}

<<corrplots, echo=FALSE>>=
library(ggplot2)
plotdat <- data.frame(corr=0, sampler=0, variable=0, T=0, V=0, W=0)
k <- 1
Ts <- c(10, 100, 1000)
Vs <- c(0.01, 0.1, 1, 10, 100, 1000)
Ws <- Vs
for(i in 1:6){
  for(j in 1:6){
    for(t in 1:3){
      plotdat[k,] <- c(scors[[2]][[t]][i,7-j], "state", "V", Ts[t], rnam[i], cnam[7-j])
      k <- k+1
      plotdat[k,] <- c(scors[[3]][[t]][i,7-j], "state", "W", Ts[t], rnam[i], cnam[7-j])
      k <- k+1
      plotdat[k,] <- c(dcors[[2]][[t]][i,7-j], "dist", "V", Ts[t], rnam[i], cnam[7-j])
      k <- k+1
      plotdat[k,] <- c(dcors[[3]][[t]][i,7-j], "dist", "W", Ts[t], rnam[i], cnam[7-j])
      k <- k+1
      plotdat[k,] <- c(ecors[[2]][[t]][i,7-j], "error", "V", Ts[t], rnam[i], cnam[7-j])
      k <- k+1
      plotdat[k,] <- c(ecors[[3]][[t]][i,7-j], "error", "W", Ts[t], rnam[i], cnam[7-j])
      k <- k+1
    }
  }
}
plotdat$corr <- as.numeric(plotdat$corr)
plotdat$T <- as.numeric(plotdat$T)
@

\begin{figure}[htb]
  \centering
<<stateplot, echo=FALSE>>=
m <- qplot(T, corr, data=plotdat[plotdat$sampler=="state",], facets=V~W, color=variable, geom="line", size=I(1), log="x", ylab="Autocorrelation", main="State Sampler")
m + scale_y_continuous( breaks=c(0, 0.5, 1)) +
    theme(axis.text.x = element_text(angle = 90, vjust=0.45))
@
\caption{Plot of autocorrelations vs. length of time series ($T$) for
  the State Sampler. Across a diagonal, the signal-to-noise ratio
  ($W/V$) is constant. Note that when the signal-to-noise ratio is
  high, $W$ has high autocorrelation and $V$ has low autocorrelation,
  and when the signal-to-noise ratio is low, $V$ has high
  autocorrelation and $W$ has low autocorrelation. When the
  signal-to-noise ratio is near 1, both $V$ and $W$ have moderate to
  low autocorrelation. Also note that increasing $T$ seems to increase
  autocorrelation for both $W$ and $V$.}
\label{stateplot}
\end{figure}

\begin{figure}[htb]
  \centering
<<distplot, echo=FALSE>>=
m <- qplot(T, corr, data=plotdat[plotdat$sampler=="dist",], facets=V~W, color=variable, geom="line", size=I(1), log="x", ylab="Autocorrelation", main="Scaled Disturbance Sampler")
m + scale_y_continuous( breaks=c(0, 0.5, 1)) +
    theme(axis.text.x = element_text(angle = 90, vjust=0.45))
@
\caption{Plot of autocorrelations vs. length of time series ($T$) for
  the Scaled Disturbance Sampler. Across a diagonal, the signal-to-noise ratio
  ($W/V$) is constant. Note that when the signal-to-noise ratio is
  high, both $V$ and $W$ have low autocorrelation and when the signal-to-noise
  ratio is low, both $V$ and $W$ have high autocorrelation. Also note
  that increasing $T$ seems to increase autocorrelation for both $W$
  and $V$.}
\label{distplot}
\end{figure}

\begin{figure}[htb]
  \centering
<<errorplot, echo=FALSE>>=
m <- qplot(T, corr, data=plotdat[plotdat$sampler=="error",], facets=V~W, color=variable, geom="line", size=I(1), log="x", ylab="Autocorrelation", main="Scaled Error Sampler")
m + scale_y_continuous( breaks=c(0, 0.5, 1)) +
    theme(axis.text.x = element_text(angle = 90, vjust=0.45))
@
\caption{Plot of autocorrelations vs. length of time series ($T$) for
  the Scaled Error Sampler. Across a diagonal, the signal-to-noise ratio
  ($W/V$) is constant. Note that when the signal-to-noise ratio is
  high, both $V$ and $W$ have high autocorrelation and when the signal-to-noise
  ratio is low, both $V$ and $W$ have low autocorrelation. Also note
  that increasing $T$ seems to increase autocorrelation for both $W$
  and $V$.}
\label{errorplot}
\end{figure}


\subsection{Results for the $\theta$'s}
<<maxcor, echo=FALSE>>=
mscor <- round(max(scors[[1]][[1]], scors[[1]][[2]], scors[[1]][[3]]),2)
mdcor <- round(max(dcors[[1]][[1]], dcors[[1]][[2]], dcors[[1]][[3]]),2)
mecor <- round(max(ecors[[1]][[1]], ecors[[1]][[2]], ecors[[1]][[3]]),2)
@

We can do the same thing for some of the $\theta$'s. Tables
\ref{tableTHT10}, \ref{tableTHT100}, and \ref{tableTHT1000} contain the
first order autocorrelation for $\theta_0$ and $\theta_{T}$, for
$T=10$, $T=100$, and $T=1000$ respectively. The key thing we learn
here is that autocorrelation in the chains for the $\theta$'s doesn't
appear to be a problem - it's always pretty low and usually is
essentially 0. I picked the first and last states of the time series
to illustrate this, but the result holds across all states in all time
series. The maximum autocorrelation for any of the states in any of
the fitted models for the the state sampler is \Sexpr{mscor}. Likewise
for the scaled disturbance sampler the maximum autocorrelation is
\Sexpr{mdcor} and the for scaled error sampler the maximum
autocorrelation is \Sexpr{mecor}. Autocorrelation for the states
doesn't seem to be a problem, at least for the local level model.

%Theta tables for T=10
\begin{table}[htb]
\begin{minipage}{.5\textwidth}
\centering
<<tableT10TH0s, echo=FALSE, results='asis'>>=
sTH0T10 <- scors[[1]][[1]][,,1]
rownames(sTH0T10) <- rnam
colnames(sTH0T10) <- cnam
TH0T10s <- xtable(sTH0T10, digit=4, align=algn)
print(TH0T10s,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_0$ in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10THTs, echo=FALSE, results='asis'>>=
sTHTT10 <- scors[[1]][[1]][,,10+1]
rownames(sTHTT10) <- rnam
colnames(sTHTT10) <- cnam
THTT10s <- xtable(sTHTT10, digit=4, align=algn)
print(THTT10s,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_T$ in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10TH0d, echo=FALSE, results='asis'>>=
dTH0T10 <- dcors[[1]][[1]][,,1]
rownames(dTH0T10) <- rnam
colnames(dTH0T10) <- cnam
TH0T10d <- xtable(dTH0T10, digit=4, align=algn)
print(TH0T10d,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_0$ in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10THTd, echo=FALSE, results='asis'>>=
dTHTT10 <- dcors[[1]][[1]][,,10+1]
rownames(dTHTT10) <- rnam
colnames(dTHTT10) <- cnam
THTT10d <- xtable(dTHTT10, digit=4, align=algn)
print(THTT10d,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_T$ in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10TH0e, echo=FALSE, results='asis'>>=
eTH0T10 <- ecors[[1]][[1]][,,1]
rownames(eTH0T10) <- rnam
colnames(dTH0T10) <- cnam
TH0T10e <- xtable(eTH0T10, digit=4, align=algn)
print(TH0T10e,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_0$ in Scaled Error Sampler}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT10THTe, echo=FALSE, results='asis'>>=
eTHTT10 <- ecors[[1]][[1]][,,10+1]
rownames(eTHTT10) <- rnam
colnames(eTHTT10) <- cnam
THTT10e <- xtable(eTHTT10, digit=4, align=algn)
print(THTT10e,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_T$ in Scaled Error Sampler}
\end{minipage}
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=10$ for $\theta_0$ and $\theta_T$ for the state, scaled
  disturbance, and scaled error samplers. Row and column names indicate
  the true values of $W$ and $V$ respectively for the simulated
  data. Note that the signal-to-noise ratio is constant across any diagonal.}
\label{tableTHT10}
\end{table}

%Theta tables for T=100
\begin{table}[htb]
\begin{minipage}{.5\textwidth}
\centering
<<tableT100TH0s, echo=FALSE, results='asis'>>=
sTH0T100 <- scors[[1]][[2]][,,1]
rownames(sTH0T100) <- rnam
colnames(sTH0T100) <- cnam
TH0T100s <- xtable(sTH0T100, digit=4, align=algn)
print(TH0T100s,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_0$ in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100THTs, echo=FALSE, results='asis'>>=
sTHTT100 <- scors[[1]][[2]][,,100+1]
rownames(sTHTT100) <- rnam
colnames(sTHTT100) <- cnam
THTT100s <- xtable(sTHTT100, digit=4, align=algn)
print(THTT100s,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_T$ in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100TH0d, echo=FALSE, results='asis'>>=
dTH0T100 <- dcors[[1]][[2]][,,1]
rownames(dTH0T100) <- rnam
colnames(dTH0T100) <- cnam
TH0T100d <- xtable(dTH0T100, digit=4, align=algn)
print(TH0T100d,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_0$ in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100THTd, echo=FALSE, results='asis'>>=
dTHTT100 <- dcors[[1]][[2]][,,100+1]
rownames(dTHTT100) <- rnam
colnames(dTHTT100) <- cnam
THTT100d <- xtable(dTHTT100, digit=4, align=algn)
print(THTT100d,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_T$ in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100TH0e, echo=FALSE, results='asis'>>=
eTH0T100 <- ecors[[1]][[2]][,,1]
rownames(eTH0T100) <- rnam
colnames(dTH0T100) <- cnam
TH0T100e <- xtable(eTH0T100, digit=4, align=algn)
print(TH0T100e,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_0$ in Scaled Error Sampler}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT100THTe, echo=FALSE, results='asis'>>=
eTHTT100 <- ecors[[1]][[2]][,,100+1]
rownames(eTHTT100) <- rnam
colnames(eTHTT100) <- cnam
THTT100e <- xtable(eTHTT100, digit=4, align=algn)
print(THTT100e,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_T$ in Scaled Error Sampler}
\end{minipage}
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=100$ for $\theta_0$ and $\theta_T$ for the state, scaled
  disturbance, and scaled error samplers. Row and column names indicate
  the true values of $W$ and $V$ respectively for the simulated
  data. Note that the signal-to-noise ratio is constant across any diagonal.}
\label{tableTHT100}
\end{table}


%Theta tables for T=1000
\begin{table}[htb]
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000TH0s, echo=FALSE, results='asis'>>=
sTH0T1000 <- scors[[1]][[3]][,,1]
rownames(sTH0T1000) <- rnam
colnames(sTH0T1000) <- cnam
TH0T1000s <- xtable(sTH0T1000, digit=4, align=algn)
print(TH0T1000s,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_0$ in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000THTs, echo=FALSE, results='asis'>>=
sTHTT1000 <- scors[[1]][[3]][,,1000+1]
rownames(sTHTT1000) <- rnam
colnames(sTHTT1000) <- cnam
THTT1000s <- xtable(sTHTT1000, digit=4, align=algn)
print(THTT1000s,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_T$ in State Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000TH0d, echo=FALSE, results='asis'>>=
dTH0T1000 <- dcors[[1]][[3]][,,1]
rownames(dTH0T1000) <- rnam
colnames(dTH0T1000) <- cnam
TH0T1000d <- xtable(dTH0T1000, digit=4, align=algn)
print(TH0T1000d,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_0$ in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000THTd, echo=FALSE, results='asis'>>=
dTHTT1000 <- dcors[[1]][[3]][,,1000+1]
rownames(dTHTT1000) <- rnam
colnames(dTHTT1000) <- cnam
THTT1000d <- xtable(dTHTT1000, digit=4, align=algn)
print(THTT1000d,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_T$ in Scaled Disturbance Sampler}
\vspace{.19 in}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000TH0e, echo=FALSE, results='asis'>>=
eTH0T1000 <- ecors[[1]][[3]][,,1]
rownames(eTH0T1000) <- rnam
colnames(dTH0T1000) <- cnam
TH0T1000e <- xtable(eTH0T1000, digit=4, align=algn)
print(TH0T1000e,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_0$ in Scaled Error Sampler}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tableT1000THTe, echo=FALSE, results='asis'>>=
eTHTT1000 <- ecors[[1]][[3]][,,1000+1]
rownames(eTHTT1000) <- rnam
colnames(eTHTT1000) <- cnam
THTT1000e <- xtable(eTHTT1000, digit=4, align=algn)
print(THTT1000e,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{$\theta_T$ in Scaled Error Sampler}
\end{minipage}
\caption{Autocorrelation in posterior sampler for a time series of
  length $T=1000$ for $\theta_0$ and $\theta_T$ for the state, scaled
  disturbance, and scaled error samplers. Row and column names indicate
  the true values of $W$ and $V$ respectively for the simulated
  data. Note that the signal-to-noise ratio is constant across any diagonal.}
\label{tableTHT1000}
\end{table}


\subsection{Implications for Interweaving}

There are 3 different possible interweaving algorithms available ---
interweaving any 2 of the 3 algorithms above. Interweaving the error
and disturbance samplers is most promising in terms of convergence
since they have low autocorrelation in exactly the opposite
situations. However, as we'll see in the next section, the disturbance
sampler is rather slow and should probably be avoided if possible. It
may the case that each of the 3 available interweaving algorithms is
fastest to converge for different parameter values. In that case,
there may be an algorithm the somehow hybridizes the interweaving
algorithms in order to come up with something better. Another
possibility worth noting is some sort of 3-way interweaving algorithm.

It's worth noting that while the state sampler generalizes easily to
generic dynamic linear (gaussian) models, the the error sampler and
disturbance sampler do not necessarily. In order for the error sampler
to generalize, there has to be at least as many observation errors as
system disturbances, i.e. $dim(v_t)\geq dim(w_t)$. Similarly, in order
for the disturbance sampler to generalize, there has to be at least as
many system disturbances than observation errors, i.e. $dim(w_t)\geq
dim(v_t)$. This restricts the applicability of these two samplers and
interweaving algorithms using them somewhat, especially the error
sampler since in most use-cases $dim(w_t)\geq dim(v_t)$. However, both
the error sampler and the disturbance sampler can be extended to these
cases by appending a portion of the state vector to the scaled
errors/disturbances in order to make the transformation one-to-one (in
the same way that the initial state $\theta_0$ is appended for both
samplers) --- though this raises the question: {\it which} portion
should be appended?

\section{Computation Time}
<<computation, echo=FALSE>>=
load("stimes.Rdata")
load("dtimes.Rdata")
load("etimes.Rdata")
stimes <- stimes[[3]]
dtimes <- dtimes[[3]]
etimes <- etimes[[3]]
svtimes <- matrix(0,6,6)
colnames(svtimes) <- cnam
rownames(svtimes) <- rnam
swtimes <- svtimes
sstimes <- svtimes
dvtimes <- svtimes
dwtimes <- svtimes
dstimes <- svtimes
evtimes <- svtimes
ewtimes <- svtimes
estimes <- svtimes
darsprop <- svtimes
earsprop <- svtimes
for(i in 1:6){
  for(j in 1:6){
    stim <- apply(stimes[[i]][[j]], 2, sum)
    dtim <- apply(dtimes[[i]][[j]][,-4], 2, sum)
    etim <- apply(etimes[[i]][[j]][,-4], 2, sum)
    earsprop[i,j] <- mean(etimes[[i]][[j]][,4]=="ARS")
    darsprop[i,j] <- mean(dtimes[[i]][[j]][,4]=="ARS")
    sstimes[i,j] <- stim[1]
    svtimes[i,j] <- stim[2]
    swtimes[i,j] <- stim[3]
    dstimes[i,j] <- dtim[1]
    dvtimes[i,j] <- dtim[2]
    dwtimes[i,j] <- dtim[3]
    estimes[i,j] <- etim[1]
    evtimes[i,j] <- etim[2]
    ewtimes[i,j] <- etim[3]
  }
}
@

It's also worth comparing actual computation time for the three
algorithms above. We'll focus on two comparisons. First, it's worth
noting how costly it is to simulate the states via FFBS then transform
them to either the the erros or the disturbances. This will suggest
whether there are subtantial gains to be made by simulating the errors
or disturbances directly. Second, we'll consider the relative speeds
of the rejection sampling step in the error and disturbance samplers.

Table \ref{tablestatetime} contains the relative amount of time spent
sampling the states and transforming them appropriately in the scaled
disturbance and scaled error samplers, as a percentage of the amount
of time spent sampling the states in the state sampler for the
$T=1000$ simulations. The scaled
disturbance sampler tends to spend around $5-10\%$ more time sampling
the states while the scaled error sampler spends around $5-15\%$ more
time sampling the states than the state sampler.  Clearly, it appears the scaled
error sampler could benefit the more from directly sampling the errors
than the scaled disturbance sampler could from directly sampling the
disturbances, though it's probably worthwhile in both cases.

Table \ref{tablerejtime} contains the average amount of time in
thousandths of a second per
iteration spent on the rejection sampling step in scaled disturbance
and scaled error samplers in addition to proportion of iterations
using the adaptive rejection sampler, once again only for the $T=1000$
simulations. The disturbance sampler appears to spend about about
$0.7$ thousandths of a second on the rejection sampling step, except
as the signal-to-noise ratio get low. In this case it spends as much
as $38$ thousandths of a second on the rejection sampling step. Notice
that when this happens, the adaptive rejection sampling algorithm
isn't being used much --- this is almost assuredly the
cause. Similarly, the scaled error sampler spends about $0.7$
thousandths of a second on the rejection step, except when the
signal-to-noise ratio is high, in which case it spends as much as $48$
thousandths of a second on the rejection step. This, again, appears to
be because the sampler can't use adaptive rejection sampling in this case.

The big things to note here are 1) the scaled disturbance sampler is
slightly faster than the scaled error sampler in the state sampling
and transformation steps (as long as we're using FFBS to sample the
states, then transforming them appropriately). 2) The scaled
disturbance sampler and the scaled error sampler fail to be able to
use adaptive rejection sampling in opposite ends of the parameter
space, but the scaled disturbance sampler is able to use adaptive
rejection sampling in a wider range.


% state + transformation times
\begin{table}[htb]
\begin{minipage}{.5\textwidth}
\centering
<<tablesdtime, echo=FALSE, results='asis'>>=
dsreltimes <- (dstimes - sstimes)/sstimes*100
dsrt <- xtable(dsreltimes, digit=2, align=algn)
print(dsrt,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{Scaled Disturbance Sampler}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
<<tablesetime, echo=FALSE, results='asis'>>=
esreltimes <- (estimes - sstimes)/sstimes*100
esrt <- xtable(esreltimes, digit=2, align=algn)
print(esrt,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{Scaled Error Sampler}
\end{minipage}
\caption{Amount of time spent sampling the states and transforming
  them to the scaled disturbances or scaled errors, respectively, in
  algorithms 2 and 3 as a percentage of the amount of time spent
  sampling the states in the state sampler.}
\label{tablestatetime}
\end{table}

% rejection sampling times
\begin{table}[htb]
\begin{minipage}{.5\textwidth}
\centering
<<tablerejdtime, echo=FALSE, results='asis'>>=
drejtimes <- xtable(dwtimes/2, digit=2, align=algn)
print(drejtimes,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{Average rejection sampling times for Scaled Disturbance
  Sampler (for $W$)}
\end{minipage}
\vspace{.19 in}
\begin{minipage}{.5\textwidth}
<<tablerejetime, echo=FALSE, results='asis'>>=
erejtimes <- xtable(evtimes/2, digit=2, align=algn)
print(erejtimes,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{Average rejection sampling times for Scaled Error Sampler
  (for $V$)}
\end{minipage}
\vspace{.19 in}
\begin{minipage}{.5\textwidth}
<<tabledprop, echo=FALSE, results='asis'>>=
dprop <- xtable(darsprop, digit=3, align=algn)
print(dprop,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{Proportion of rejection samples using ARS in Scaled
  Disturbance Sampler (for $W$)}
\end{minipage}
\begin{minipage}{.5\textwidth}
<<tableeprop, echo=FALSE, results='asis'>>=
eprop <- xtable(earsprop, digit=3, align=algn)
print(eprop,
  floating=FALSE,
  hline.after=NULL,
  scalebox=sclbx,
  add.to.row=list(pos=list(-1,0, length(rnam)),
  command=c('\\toprule\n','\\midrule\n','\\bottomrule\n')))
@
\captionof*{table}{Proportion of rejection samples using ARS in Scaled
  Error Sampler (for $V$)}
\end{minipage}
\caption{Average amount of time in thousandths of a second per
  iteration spent on the rejection sampling step (for $W$ in the
  scaled disturbance sampler and for $V$ in the scaled error sampler)
  and the proportion of iterations spent using the adaptive rejection
  sampler for $T=1000$.}
\label{tablerejtime}
\end{table}

\end{document}
