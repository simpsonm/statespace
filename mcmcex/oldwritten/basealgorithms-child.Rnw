<<set-parent-LLMest, echo=FALSE, cache=FALSE>>=
set_parent('mcmcex.Rnw')
@


\section{Application: The Local Level Model}

{\it INSERT A PARAGRAPH MOTIVATING WHY WE LOOK AT THE LOCAL LEVEL MODEL}

The local level model (LLM) is a DLM with univariate data $y_t$ for $t=1,2,\cdots,T$ and a univariate latent state $\theta_t$ for $t=0,2,\cdots,T$ that satisfies
\begin{align}
  y_t |\theta_{0:T}& \stackrel{ind}{\sim} N(\theta_t,V) \label{llmobseq}\\
  \theta_t |\theta_{0:t-1}& \sim N(\theta_{t-1},W) \label{llmsyseq}
\end{align}
with $\theta_0\sim N(m_0,C_0)$. Here $\theta_t=E[y_t|\theta_{0:T}]$, i.e. the average value of $y_t$. The states are $\theta_{0:T}$, the scaled disturbances are $\gamma_{0:T}$ with $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$, and the scaled errors are $\psi_{0:T}$ with $\psi_0=\theta_0$ and $\psi_t=(y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$. The independent inverse Wishart priors on $V$ and $W$ in Section \ref{modelsec} cash out to independent inverse gamma priors for the local level model, i.e. $V\sim IG(\alpha_V,\beta_V)$ and $W\sim IG(\alpha_W,\beta_W)$. 

The joint density of $(V,W,\theta_{0:T},y_{1:T})$ is:
\begin{align*}
  p(&V_,W,\theta_{0:T},y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)} \exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \theta_{t})^2\right)\right]\nonumber\\
  &W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(\theta_t - \theta_{t-1})^2\right) \right] \exp\left[-\frac{1}{2C_0}(\theta_0 - m_0)^2\right]%\label{llmstatejoint}
\end{align*}
This immediately gives the state sampler:
\begin{alg}[State Sampler for LLM]\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{llmstatealg}
\item Draw $\theta_{0:T}$ from $p(\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T})$ using FFBS.
\item Draw $(V^{(k+1)},W^{(k+1)})$ from $p(V,W|\theta_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
In step 2, $V$ and $W$ are independent with $V\sim IG(a_V,b_V)$ and $W\sim IG(a_W, b_W)$ where $a_V = \alpha_V + T/2$, $b_V = \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2$, $a_W = \alpha_W + T/2$, and $b_W = \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2$.

The scaled disturbance sampler, the DA algorithm based on the scaled disturbances, is a bit more complicated. Following \eqref{dlmdistjoint}, we can write the joint posterior of $(V,W,\gamma_{0:T})$ as
\begin{align}
  p(&V,W,\gamma_{0:T}|y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)}\exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \gamma_0 - \sqrt{W}\textstyle\sum_{s=1}^t\gamma_s)^2\right)\right] \nonumber\\
  & \times W^{-(\alpha_W + 1)}\exp\left[-\frac{\beta_W}{W}\right] \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\gamma_t^2\right]\exp\left[-\frac{1}{2C_0}(\gamma_0-m_0)^2\right]\label{llmdistpost}
\end{align}
Now $V$ and $W$ are no longer conditionally independent given $\gamma_{0:T}$ and $y_{1:T}$. Instead of attempting the usual DA algorithm, we'll add an extra Gibbs step and draw $V$ and $W$ separately. This gives us the scaled disturbance sampler:
\begin{alg}[Scaled Disturbance Sampler for LLM]\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{llmdistalg}
\item Draw $\gamma_{0:T}$ from $p(\gamma_{0:T}|V^{(k)},W^{(k)},y_{1:T})$, possibly using FFBS to sample $\theta_{0:T}$ then transforming.
\item Draw $V^{(k+1)}$ from $p(V|W^{(k)},\gamma_{0:T},y_{1:T})$.
\item Draw $W^{(k+1)}$ from $p(W|V^{(k+1)},\gamma_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
In step 2, $V$ is drawn from the same inverse gamma distribution as in step 2 of Algorithm \ref{llmstatealg}. In step 3, the draw of $W$ is more complicated. The density can be written as
\begin{align*}
  p(W|V,\gamma_{0:T},y_{1:T}) \propto & W^{-\alpha_w - 1}\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \gamma_0 - \sqrt{W}\sum_{s=1}^t\gamma_s\right)^2\right].
\end{align*}
This density isn't any known form and is difficult to sample from. The log density can be written as
\begin{align*}
\log p(W|V,\gamma_{0:T},y_{1:T}) =& -aW + b\sqrt{W} - (\alpha_W + 1)\log W -\beta_W/W + C 
\end{align*}
where $C$ is some constant, $a=\sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$ and $b=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$. It can be shown that $b^2 > \frac{32}{9\beta_w}(\alpha_w+1)^3(1 - 2sgn(b)/3)$ implies that the density is log concave where
\begin{align*}
  sgn(b) & = \begin{cases} 1 &\text{if } b>0\\
    0 & \text{if } b=0\\
    -1 & \text{if } b<0.\\
    \end{cases}
\end{align*}
This condition is equivalent to $\partial^2\log p(W|.)/\partial W^2 < 0$ at the $W^*$ that maximizes $\partial^2\log p(W|.)/\partial W^2$ and hence guarantees the density is globally log-concave. It turns out that this tends to hold over a wide region of the parameter space --- so long as $V$ is smaller or isn't much larger than $W$. This allows for the use of adaptive rejection sampling in order to sample from this distribution in many cases, e.g. using \citet{gilks1992adaptive}. An alternative is to use a $t$ approximation to the conditional density as a proposal in a rejection sampler, but this is much more computationally expensive when necessary.

The scaled error sampler is similar to the scaled disturbance sampler. From \eqref{dlmerrorjoint} we can write $p(V,W,\psi_{0:T}|y_{1:T})$ as
\begin{align*}
    p(&V,W,\psi_{0:T},y_{1:T}) \propto W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(Ly_t + \sqrt{V}L\psi_t)^2\right)\right]\nonumber \\
 & V^{-(\alpha_V + 1)}\exp\left[-\frac{\beta_V}{V}\right]  \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\psi_t^2\right] \exp\left[-\frac{1}{2C_0}(\psi_0-m_0)^2 \right] 
\end{align*}
where we define $Ly_t=y_t-y_{t-1}$ for $t=2,3,...,T$ \& $Ly_1= y_1 - \psi_0$ and $L\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$ \& $L\psi_1=\psi_1-0$. Once again, $V$ and $W$ are no longer conditionally independent given $\psi_{0:T}$ and $y_{1:T}$. In fact, the density seems analgous to \eqref{llmdistpost} with $V$ and $W$ ``switching places'' so to speak. The scaled error sampler obtained from drawing $V$ and $W$ separately is:
\begin{alg}[Scaled Error Sampler for LLM]\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{llmerroralg}
\item Draw $\psi_{0:T}$ from $p(\psi_{0:T}|V^{(k)},W^{(k)},y_{1:T})$, possibly using FFBS to sample $\theta_{0:T}$ then transforming.
\item Draw $V^{(k+1)}$ from $p(V|W^{(k)},\psi_{0:T},y_{1:T})$.
\item Draw $W^{(k+1)}$ from $p(W|V^{(k+1)},\psi_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
In step 3, $W$ is drawn from the same inverse gamma distribution as in step 2 of Algorithm \ref{llmstatealg}. Drawing $V$ in step 2 is more complicated, but exactly analogous to drawing $W$ in Algorithm \ref{llmdistalg}. The log density of $V|W,\psi_{0:T},y_{1:T}$ can be written as
\begin{align*}
\log p(V|W,\psi_{0:T},y_{1:T}) =& -aV + b\sqrt{V} - (\alpha_V + 1)\log W -\beta_V/V + C 
\end{align*}
where again $C$ is some constant, but now $a=\sum_{t=1}^T(L\psi_t)^2/2W$ and $b=\sum_{t=1}^T(L\psi_tLy_t)/W$. So we can use the same methods to sample from this density -- adaptive rejection sampling, as in \citet{gilks1992adaptive}, will work as long as $b^2 > \frac{32}{9\beta_V}(\alpha_V+1)^3(1 - 2sgn(b)/3)$, and otherwise a $t$ proposal in a rejection sampler will work but will be substantially slower.

{\it OLD STUFF BELOW.}
