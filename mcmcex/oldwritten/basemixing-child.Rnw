<<set-parent-basemix, echo=FALSE, cache=FALSE>>=
set_parent('mcmcex.Rnw')
@

\section{Base Results}

<<plotsetup, echo=FALSE, message=FALSE>>=
library(ggplot2)
library(scales)
library(plyr)
library(xtable)
library(reshape2)
load("mixing/OldSims/samout.RData")
load("mixing/OldSims/postcors.RData")
sams <- c("error", "state", "dist")
alts <- c("sdalt", "sealt", "dealt", "trialt")
ints <- c("sdint", "seint", "deint", "triint")
samout$type <- "Base" #$
samout$type[samout$sampler %in% alts] <- "Alternating" 
samout$type[samout$sampler %in% ints] <- "Interweaving" 
samout$samplers <- "Base" #$
samout$samplers[substr(samout$sampler, 1, 2)=="sd"] <- "State-Dist" 
samout$samplers[substr(samout$sampler, 1, 2)=="se"] <- "State-Error" 
samout$samplers[substr(samout$sampler, 1, 2)=="de"] <- "Dist-Error" 
samout$samplers[substr(samout$sampler, 1, 3)=="tri"] <- "Triple" 
meltedsam <- melt(samout, id=c("type", "samplers", "sampler", "V.T", "W.T", 
                            "T.T"))
Vs <- unique(meltedsam$V.T) #$
Ws <- Vs
breaks <- Vs[seq(1,11,2)]
label_both_parsed <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), function(x) parse(text = x))
}
label_both_parsed_split <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), 
        function(x) parse(text = strsplit(x, "\\.")[[1]][1]))
}
label_parsed_split <- function(variable, value){
  llply(as.character(value), function(x) parse(text = strsplit(x, "\\.")[[1]][1]))
}
meltedcors <- melt(postcors, id=c("V.T", "W.T", "T.T"))
opts_chunk$set(fig.width=7, fig.height=4, out.width='1\\textwidth', 
               fig.pos='!ht') #$
@ 

In order to test these algorithms, I simulated a fake dataset from
the local level model for various choices of $V$, $W$, and $T$. We created a grid over $V$--$W$ space with $(V,W)$ ranging from $(10^{-2},10^{-2})$ to $(10^2, 10^2)$ and we simulated a dataset for all possible combinations of $V$ and $W$ with each of $T=10, 100, 1000$. Then for each dataset, we fit the local level model using each sampler. We used the same priors for each model: $\theta_0\sim N(0,10^7)$, $V\sim IG(5, 4\tilde{V})$, and $W\sim IG(5, 4\tilde{W})$, mutually independent where $(\tilde{V},\tilde{W})$ are the true values of $V$ and $W$ used to simulate the time series. This ensures all priors and thus the posterior are proper and results in independent inverse gamma priors on $V$ and $W$ with prior means equal to the respective true values and moderately large prior variances. We chose these priors for $(V,W)$ For each sampler to sidestep issues of posterior propriety or noninformativeness for variances since these issues are largely orthogonal to our goals.

For each model and each sampler, we $n=3000$ draws and threw away the first $500$ as burn in. The chains were started at the true values used to simulated the time series, so we can examine the behavior of the chains to determine how well they mix but not how quickly they convergence since we've essentially enforced convergence at from iteration one. Define the effective sample proportion (EFP) as the effective sample size (ESS, REFERENCE HERE) divided by the actual sample size, i.e. $ESP=ESS/n$. $ESP=1$ indicates that the markov chain is behaving as if it obtains iid draws from the posterior. Figure \ref{baseESplotT10} contains plots of ESP for $V$ and $W$ in each chain of each sampler. The state sampler has a low ESP for $V$ and a high ESP for $W$ when the signal-to-noise ratio, $W/V$, is larger than one. When the signal-to-noise ratio is smaller than one, on the other hand, the state sampler has a low ESP for $W$ and a high ESP for $V$. In the usual case where the signal to noise ratio isn't too different from one, the state sampler has a modest to low ESP for both $V$ and $W$. Note that the particular values of $V$ and $W$ don't seem to matter at all --- just their relative values, i.e. the signal-to-noise ratio $W/V$. Moving up any diagonal on the plots for $V$ and $W$ in the state sampler, $W/V$ is constant and the ESS appears roughly constant. The basic lesson here is that the state sampler has mixing issues for whichever of $V$ or $W$ is smaller.

Figure \ref{baseESplotT10} tells a different story for the scaled disturbance sampler. When the signal-to-noise ratio is less than one, ESPs for both $V$ and $W$ are nearly 1, i.e. the effective sample size is nearly the actual sample size of the chain. When the signal-to-noise ratio is greater than one, however, ESP for both $V$ and $W$ becomes small, especially for $V$. Once again the absolute values of $V$ and $W$ don't matter for this behavior --- just the relative values. The scaled error sampler has essentially the opposite properties. When $W/V$ is large, it has a near 1 ESP for both $V$ and $W$. On the other hand, when $W/V$ is small is has a low ESP for both $V$ and $W$, especially for $V$. The lesson here seems to be that the scaled disturbances ($\gamma_{0:T}$) are the preferred data augmentation for low signal-to-noise ratios and the scaled errors ($\psi_{0:T}$) are the preferred data augmentation for high signal-to-noise ratios, while the states ($\theta_{0:T}$) are preferred for signal-to-noise ratios near 1.

Figures \ref{baseESplotT100} and \ref{baseESplotT1000} contain the same plots for $T=100$ and $T=1000$ respectively. Increasing the length of the time series seems to exacerbate all problems, though the same basic conclusions hold. As $T$ increases, $W/V$ has to be smaller and smaller for the scaled disturbance sampler to have decent mixing, and similarly $W/V$ has to be larger and larger for the scaled error sampler to have decent mixing. Interestingly, the scaled error sampler appears to mix well for both $V$ and $W$ over a larger region of the space $W/V<1$ than the scaled disturbance sampler does over $W/V>1$. The state sampler is stuck between a rock and a hard place, so to speak, since as $T$ increases, good mixing for $V$ requires $W/V$ to be smaller and smaller, but good mixing for $W$ requires $W/V$ to be larger and larger. 

It's also worth noting that both the scaled error and scaled disturbance samplers run into trouble with their adaptive rejection sampling step in precisely the same region of the parameter space where they have good mixing for both $V$ and $W$, though as $T$ increases, this only happens in the increasingly extreme ends of the parameter space. More precisely, when $W/V>1$, $p(W|V,\psi_{0:T},y_{1:T})$ will often fail to be log concave, and when $W/V<1$, $p(V|W,\gamma_{0:T},y_{1:T})$ will often fail to be log concave, but as $T$ increases the degree to which $W/V$ must differ from one (in the appropriate direction) in order for log concavity to often or even occasionally fail increases. Outside of these respective regions, log-concavity of the relevant density failing is an extremely unlikely occurence. As a result, the adapftive rejection sampling algorithm of \citet{gilks1992adaptive} won't work in general. A more general rejection sampling algorithm based on a $t$ proposal can be used, but this is much more expensive. Another option is to give up directly sampling from either conditonal density and use a metropolis step, perhaps for $(V,W)$ jointly, though we haven't tried that. In general, the sampling algorithm should be prepared to use something other than adaptive rejection sampling if necessary because it's possible that the chain enters a region of the parameter space where the relevant density is not log concave, no matter what the likely values of $V$ and $W$ are. {\it NOTE: ADD DETAILS ABOUT PRECISELY HOW LARGE OR SMALL $W/V$ HAS TO BE TO THIS PARAGRAPH}


<<baseESplotT10, fig.cap=cap, echo=FALSE>>=
cap <- "Effective sample proportion in the posterior sampler for a time series of length $T=10$ for $V$ and $W$, and for the state, scaled disturbance, and scaled error samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Also note that for plotting purposes, effective sample proportions larger than $1$ were rounded down to $1$"
vars <- c("V.ES", "W.ES")
castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable ~ ., 
                   subset=.(variable %in% vars  & T.T==10 & sampler %in% sams ))
colnames(castedsam)[5] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
  geom_tile() +
  scale_fill_gradient("ESP", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(variable~sampler, scales="free", labeller=label_both_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
  ggtitle("Effective Sample Proportion for V and W in the base samplers, T=10") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@ 


<<baseESplotT100, fig.cap=cap, echo=FALSE>>=
cap <- "Effective sample proportion in the posterior sampler for a time series of length $T=100$ for $V$ and $W$, and for the state, scaled disturbance, and scaled error samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Also note that for plotting purposes, effective sample proportions larger than $1$ were rounded down to $1$"
vars <- c("V.ES", "W.ES")
castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable ~ ., 
                   subset=.(variable %in% vars  & T.T==100 & sampler %in% sams ))
colnames(castedsam)[5] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
  geom_tile() +
  scale_fill_gradient("ESP", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(variable~sampler, scales="free", labeller=label_both_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
  ggtitle("Effective Sample Proportion for V and W in the base samplers, T=100") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@ 


<<baseESplotT1000, fig.cap=cap, echo=FALSE>>=
cap <- "Effective sample proportion in the posterior sampler for a time series of length $T=1000$ for $V$ and $W$, and for the state, scaled disturbance, and scaled error samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Also note that for plotting purposes, effective sample proportions larger than $1$ were rounded down to $1$"
vars <- c("V.ES", "W.ES")
castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable ~ ., 
                   subset=.(variable %in% vars  & T.T==1000 & sampler %in% sams ))
colnames(castedsam)[5] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
  geom_tile() +
  scale_fill_gradient("ESP", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(variable~sampler, scales="free", labeller=label_both_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
  ggtitle("Effective Sample Proportion for V and W in the base samplers, T=1000") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@ 
