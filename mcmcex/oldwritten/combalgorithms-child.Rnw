<<set-parent-combalg, echo=FALSE, cache=FALSE>>=
set_parent('mcmcex.Rnw')
@


\section{Hybrid Algorithms: Alternating and Interweaving}

{\it NOTE: ADD IN REFERENCES TO THE RESULTS OF THE BASE ALGORITHMS AND TO THE FRUHWIRTH STOCHASTIC VOLATILITY PAPER THAT USES INTERWEAVING}

In their seminal paper, \citet{yu2011center} introduce the concept of an interweaving algorithm for MCMC sampling. Parts of this section follow parts of \citeauthor{yu2011center} closely. The interweaving idea is pretty simple --- suppose $Z$ denotes the parameter vector, i.e. $Z=(V,W)$ in our case, and $\theta$ denotes an augmented data vector while $y$ denotes the data vector. Let $\gamma$ denote an alternate augmented data vector, e.g. a transformation of $\theta$ that potentially depends on $Z$ and $y$. Furthermore suppose the joint distribution of $(Z, \theta, \gamma)$ is well defined (though possibly singular). Then an MCMC algorithm that interweaves between $\theta$ and $\gamma$ performs the following steps in a single iteration:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{inter}
\item Sample $\theta$ from $p(\theta|Z^{(k)},y)$
\item Sample $\gamma^{(k+1)}$ from $p(\gamma|\theta,y)$
\item Sample $Z^{(k+1)}$ from $p(Z|\gamma^{(k+1)},y)$
\end{enumerate}
\end{alg}
This is called a {\it global} interweaving strategy (GIS) since interweaving occurs globally across the entire parameter vector. It's possible to define a {\it componentwise} interweaving strategy (CIS) the interweaves within specific steps of a Gibbs sampler as well. Step two of the GIS algorithm is typically accomplished by sampling $Z|\theta,y$ and then $\gamma|\theta,Z,y$. The result of this algorithm is a markov chain with stationary distribution $p(Z|y)$ or even $p(Z,\gamma|y)$. In addition, if the full joint distribution of $(\theta, \gamma, Z| y)$ or perhaps just $p(Z,\theta|y)$ is desired, then an additional step to sample $\theta|\gamma,Z,y$ is needed. This results in the following:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{inter2}
\item Sample $\theta$ from $p(\theta|Z^{(k)},y)$
\item Sample $Z$ from $p(Z|\theta,y)$
\item Sample $\gamma^{(k+1)}$ from $p(\gamma|\theta,Z,y)$
\item Sample $Z^{(k+1)}$ from $p(Z|\gamma^{(k+1)},y)$
\item Sample $\theta^{(k+1)}$ from $p(\theta|\gamma^{(k+1)},Z^{(k+1)},y)$
\end{enumerate}
\end{alg}
Steps 2 and 3 replace step 2 from above in algorithm \eqref{inter} while step 5 is necessary to obtain a valid sample from $\theta$, though strictly speaking isn't necessary to sample from $(Z,\gamma|y)$. Often the transformation from $\theta$ to $\gamma$ is one-to-one conditional on $y$ and $Z$. This results in a singular joint distribution for $(\theta,\gamma,y)$ but doesn't cause any problems --- it actually makes things easier since steps 3 and 5 become transformations of previously sampled parameters and the data. \citeauthor{yu2011center} note that while often one-to-one transformations are typically simple and effective, in many cases a many-to-one transformation is needed to achieve satisfactory mixing --- this is where the generality of the interweaving idea shines.

The GIS algorithm is directly comparable to an {\it alternating algorithm} which helps motivate the name ``interweaving''. Given the same two data augmentations, $\theta$ and $\gamma$, and parameter vector $Z$, the alternating algorithm for sampling from $p(Z|y)$ is as follows
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{alt}
\item Sample $\theta$ from $p(\theta|Z^{(k)},y)$
\item Sample $Z$ from $p(Z|\theta,y)$
\item Sample $\gamma$ from $p(\gamma|Z,y)$
\item Sample $Z^{(k+1)}$ from $p(Z|\gamma,y)$
\end{enumerate}
\end{alg}
Compared to algorithm \eqref{inter2} there are two differences. The first difference is step 5 is missing in algorithm \eqref{alt} --- this is merely because we're only sampling from $p(Z|y)$ instead of $p(Z,\theta,\gamma|y)$ and so is immaterial. A similar step 5 would have to be added in order to sample from $p(Z,\theta,\gamma|y)$ jointly. The key difference is in step 3: instead of sampling from $p(\gamma|\theta,Z,y)$, the alternating algorithm samples from $p(\gamma|Z,y)$. In other words it alternates between two data augmentation algorithms in a single iteration. The interweaving algorithm, on the other hand, interweaves $\theta$ and $\gamma$ together by sampling $\gamma$ conditonal on $\theta$ in addition to everything else.

\citeauthor{yu2011center} show that the global interweavinig sampler has a geometric rate of convergence no worse than the worst of the two underlying algorithms. In models with ``nice'' priors in some sense, they also show that if the two augmentations are one-to-one transformations of each other (conditional on $\theta$ and $y$) and form a ``beauty and the beast'' pair, then the GIS algorithm is the same as the optimal PX-DA algorithm of \citet{meng1999seeking}, \citet{liu1999parameter}, \citet{van2001art} and \citet{hobert2008theoretical}. Specifically, the theorem requires that one of $\theta$ and $\gamma$ is a sufficient augmentation while the other is an ancillary augmentation. Mathematically, $\theta$ is a sufficient augmentation for $Z$, or a SA if $p(y|\theta, Z) = p(y|\theta)$, i.e. $\theta$ is a sufficient statistic for $Z$. Equivalently, $y\ind Z | \theta$. $\gamma$ is an ancillary augmentation for $Z$, or an AA if $p(\theta|Z)=p(\theta)$, i.e. $\theta$ is an ancillary statistic for $Z$, or equivalently $\theta\ind Z$. \citeauthor{yu2011center}'s SA and AA are the same as \citet{papaspiliopoulos2007general}'s centered and noncentered parameterizations respectively (CP and NCP). \citeauthor{yu2011center} call a GIS algorithm based on an AA-SA pair an ancillary sufficient interweaving strategy, or an ASIS algorithm.


\subsection{The near futile search for an ASIS algorithm for the local level model}

Ideally, then, we'd like to contruct an SA-AA pair of data augmentations. Now since \eqref{stateobseq} contains $V$ and \eqref{statesyseq} contains $W$, we see immediately that $\theta_{0:T}$ (i.e. the usual data augmentation) is neither a SA nor an AA for $(V,W)$. However from \eqref{distsyseq} and \eqref{errorsyseq} we see that both $\gamma_{0:T}$ and $\psi_{0:T}$, defined in \eqref{gammadef} and \eqref{psidef} respectively, are AA for $(V,W)$. With two options for an AA, we need only find a SA to construct an ASIS algorithm. This sounds simple but ends up being a difficult nut to crack. 

To see why, let $\phi_{0:T}(\theta_{0:T};y_{1:T},V,W):\Re^{T+1}\to\Re^{T+1}$ be a one-to-one and differentiable transformation of $\theta_{0:T}$. Let $\phi_t(\theta_{0:T};y_{1:T},V,W)$ denote the $t$'th element of the vector $\phi(\theta_{0:T};y_{1:T},V,W)$. Let $\phi^{-1}(\phi_{0:T};y_{1:T},V,W)=\theta(\phi_{0:T};y_{1:T},V,W)$ denote the inverse transformation and $\phi_t^{-1}(.;.)=\theta_t(.;.)$ its $t$'th element. To keep the notation from being cumbersome, we'll suppress the dependence of $\phi(.)$, $\phi^{-1}(.)$, $\phi_t(.)$, and $\phi_t^{-1}$ on $y_{1:T}$, $V$ and $W$. Furthermore, let $J$ denote the jacobian of this transformation and suppose that both $|J|$ and $\theta_0(\phi_{0:T})$ are free of $y_{1:T}$. Assume we have an arbitrary proper prior on $\theta_0$ with density $p(\theta_0|V,W)$ with respect to Lebesgue measure. Then we can write the joint distribution of $y_{1:T},\theta_{0:T}|V,W$ as
  \begin{align}
    p(y_{1:T},\theta_{0:T}|V,W) \propto &V^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T\left(\frac{y_t-\theta_t}{\sqrt{V}}\right)^2\right] W^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T\left(\frac{\theta_t-\theta_{t-1}}{\sqrt{V}}\right)^2\right] p(\theta_0|V,W)
    \intertext{so that}
    p(y_{1:T},\phi_{0:T}|V,W)  \propto &|J|V^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T\left(\frac{y_t-f^{-1}_t(\phi_{0:T})}{\sqrt{V}}\right)^2\right] \times \label{eq:exp1}\\
    &W^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T\left(\frac{f^{-1}_t(\phi_{0:T})-f^{-1}_{t-1}(\phi_{0:T})}{\sqrt{W}}\right)^2\right] p(f_0^{-1}(\phi_{0:T})|V,W)\label{eq:exp2}
  \end{align}
For $\phi_{0:T}$ to be SA for $(V,W)$ we need $p(y_{0:T}|\phi_{0:T},V,W)=p(y_{0:T}|\phi_{0:T})$. There are two relatively simple ways for this to occur:
\begin{enumerate}
\item The exponential in \ref{eq:exp1} is free of $(V,W)$ but still contains $y_{0:T}$ while the exponential in \ref{eq:exp2} is free of $y_{0:T}$. This means that
  \begin{align}
    &\frac{y_t-\theta_t(\phi_{0:T})}{\sqrt{V}}\label{case1a}
  \end{align}
  is free of $(V,W)$ but still contains $y_{0:T}$ for $t=1,2,...T$ and
  \begin{align}
    &\frac{\theta_t(\phi_{0:T})-\theta_{t-1}(\phi_{0:T})}{\sqrt{V}}\label{case1b}
  \end{align}
  is free of $y_{0:T}$.
\item The exponential in \ref{eq:exp2} is free of $(V,W)$ but contains $y_{0:T}$ while the exponential in \ref{eq:exp1} is free of $y_{0:T}$. This means that
  \begin{align}
    &\frac{\theta_t(\phi_{0:T})-\theta_{t-1}^{-1}(\phi_{0:T})}{\sqrt{W}}\label{case2a}
  \end{align}
  is free of $(V,W)$ but still contains $y_{0:T}$ for $t=1,2,...T$ and 
  \begin{align}
    &\frac{y_t-\theta_t(\phi_{0:T})}{\sqrt{V}}\label{case2b}
  \end{align}
  is free of $y_{0:T}$.
\end{enumerate}
Both cases are impossible. In case 1, \ref{case1a} implies that 
\begin{align}
  y_t - \theta_t(\phi_{0:T}) &= \sqrt{V}h_t(\phi_{0:T}, y_{1:T})
\end{align}
for $t=1,2,...,T$ where $h_t(.)$ is free of $(V,W)$. Rearranging and substituting into \ref{case1b} we get
\begin{align}
  \frac{\theta_t(\phi_{0:T}) - \theta_{t-1}(\phi_{0:T})}{\sqrt{W}}*=\frac{y_t - y_{t-1} - \sqrt{V}(h_t - h_{t-1})}{\sqrt{W}}\label{case1broke}
\end{align}
for $t=2,2,...,T$. Since $h_t$ and $h_{t-1}$ can't depend on $V$ or $W$, \ref{case1broke} will always depend on $y_t$ and $y_{t-1}$, contradicting the assupmtion. 

Now we turn to case 2. \ref{case2a} and \ref{case2b} imply that
\begin{align}
  y_t - \theta_t(\phi_{0:T}) &= g_t(\phi_{0:T},V,W)\\
  \theta_t(\phi_{0:T}) - \theta_{t-1}(\phi_{0:T}) = \sqrt{W}h_t(\phi_{0:T}, y_{1:T})
\end{align}
for $t=1,2,...,T$ where $g_t$ is free of $y_{0:T}$ and $h_t$ is free of $(V,W)$. Combining these two and recursively substituting yields
\begin{align}
  \theta_t(\phi_{0:T}) &= \sqrt{W}\sum_{s=1}^th_s + \theta_0(\phi_{0:T}) = y_t - g_t
  \intertext{and thus}
  y_t & = \sqrt{W}\sum_{s=1}^th_s + \theta_0(\phi_{0:T}) + g_t\\
  & = \sqrt{W}\kappa_t(y_{0:T}) + C
\end{align}
where $C$ is constant with respect to $y_{0:T}$ and $\kappa_t(.)$ is constant with respect to $(V,W)$ but not $y_{0:T}$, in particular $y_t$. Rearranging we have
\begin{align}
  \kappa_t =& \frac{y_t - C}{\sqrt{W}}
\end{align}
which contradicts that $\kappa_t$ is free of $W$.

This simple method of constructing a SA for $(V,W)$ doesn't work, but that doesn't rule out the possibility completely. In particular, a transformation in which $|J|$ or $\theta_t(\phi_{0:T})$ is not free of $y_{0:T}$ may be possible, and even if we impose these constraints, we haven't ruled out the possibility that
\begin{align}
  &\left(\frac{y_t - \theta_t(\phi_{0:T})}{\sqrt{V}}\right)^2 + \left(\frac{\theta_t(\phi_{0:T}) - \theta_{t-1}(\phi_{0:T})}{\sqrt{W}}\right)^2
\end{align}
can be additively separated into two functions, one of $y_{1:T}$ but not $(V,W)$ and the other of $(V,w)$ but not $y_{1:T}$. We can't currently prove that this is impossible, but it at least seems unlikely.

Another possibility is not to limit ourselves to one-to-one transformations. In fact, another option for the augmented data vector immediately presents itself: $(\theta_0,v_{1:T}, w_{1:T})$. This is a sort of SA for $(V,W)$, but $y_t$ is completely determined by this set of variables:
\begin{align}
  y_t &= \theta_0 + \sum_{s=1}^tw_s + v_t
\end{align}
Plugging that restriction back into $p(v_{1:T},w_{1:T}|V,W,\theta_0)$ will just yield $(\theta_0,v_{1:T})$ or $(\theta_0,w_{1:T})$ as the augmented data vector, which we've seen in sections \ref{sec:dist} and \ref{sec:error} will result in the state sampler.

While an ASIS algorithm for the local level model seems elusive at this point, there are still three GIS algorithms available --- interweaving between any two of the parameterizations $\theta_{0:T}$, $\gamma_{0:T}$ and $\psi_{0:T}$. Another possibility is a GIS algorithm that interweaves between all three parameterizations. \citet{yu2011center} don't talk about this possibility, but it yields a markov chain with stationary distribution $p(V,W,\psi_{0:T}|y_{1:T})$ assuming that $\psi_{0:T}$ is the last parameterization used in the algorithm. If, for example, $p(V,W,\theta_{0:T})$ is desired, then an additional step simulating $\theta_{0:T}|V,W,\psi_{0:T},y_{1:T}$ is necessary, though in our case this is actually just a back transformation form $\psi_{0:T}$ to $\theta_{0:T}$. While we don't have a SA-AA pair of data augmentations, the scaled disturbances ($\gamma_t$'s) and scale errors ($\psi_t$'s) seem to exhibit the ``beauty and the beast'' nature that makes such a pair desireable in the first place ({\it INSERT REFERENCE TO THE GRAPHS IN THE BASE RESULTS SECTION HERE. ALSO MAYBE MORE DETAIL})

The corresponding alternating algorithms are a good baseline for evaluating the interweaving algorithms, but there's another possibility --- a random kernel algorithm ({\it FIND REFERENCE TO THIS / NAME IN THE LITERATURE}). A random kernel algorithm randomly chooses one of the parameterizations to use at each iteration using a fixed probability distribution. So given any two or more of the parameterizations (or equivalently, base samplers), there's an alternating algorithm that alternates between the chosen parameterizations in every iteration, a random kernel algorithm that randomly selects one of the chosen parameterizations at each iteration by assigning each one an equal probability, and an interweaving algorithm that interweaves between each of the chosen parameterizations at each iteration. Table \ref{table:sams} contains each sampler considered for the local level model, including the base samplers.

\begin{table}[h]
  \centering
  \scalebox{.9}{
  \begin{tabular}{|l|cccc|}\hline
    Base: & State (S) & Scaled Disturbance (SD) & Scaled Error (SE) & \\
    Alternating: & S + SD & S + SE & SD + SE & S + SD + SE\\
    Interweaving: & S + SD & S + SE & SD + SE & S + SD + SE\\
    Random Kernel: & $\frac{1}{2}$S + $\frac{1}{2}$SD & $\frac{1}{2}$S + $\frac{1}{2}$SE & $\frac{1}{2}$SD + $\frac{1}{2}$SE & $\frac{1}{3}$S + $\frac{1}{3}$SD + $\frac{1}{3}$SE\\
    \hline
  \end{tabular}
  }
  \caption{All posterior samplers considered for the local level model. The fractions in the random kernel sampler indicate the probability that each base sampler is chosen in a given iteration. Note that there are two samplers omitted which are a sort of hybrid between alternating and interweaving: the sampler that alternates between S and SD, then interweaves between SD and SE, and the sampler which reverses which steps are interweaving and which are alternating.}
  \label{table:sams}
\end{table}

Note that with the alternating and interweaving samplers, there's an issue of choosing the order in which each parameterization is used. Presumably, $\theta_{0:T}$ is of some interest, but perhaps not $\gamma_{0:T}$ nor $\psi_{0:T}$. In this case, the default would be to use $\theta_{0:T}$ last so that an additional transformation step isn't needed in every iteration. For the purposes of comparison , we defaulted to the order $\theta\to\gamma\to\psi$ across all alternating and interweaving samplers. 
