\documentclass{article}
\usepackage{graphicx, color, amssymb, amsmath, bm, rotating, graphics,
epsfig, multicol, amsthm, bbm}
\usepackage{multicol}
\usepackage{textcomp}
\usepackage{fullpage}
\usepackage{booktabs}
\usepackage{caption}
\usepackage[authoryear]{natbib} %numbers instead of authoryear for [1] instead of [1980]
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newtheorem{alg}{Algorithm}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator{\Tr}{Tr}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
options(replace.assign=TRUE,width=90)
opts_knit$set(eval.after = "fig.cap")
opts_chunk$set(dev="pdf",
               fig.lp = "",
               fig.keep="high",
               fig.show="hold",
               fig.align="center",
               fig.pos="!ht")

@

\title{A note on priors}
\author{Matt Simpson}
\maketitle

\section{Introduction}
This is a quick note on priors for DLMs generally and for the LLM in particular, thanks to some comments from Sylvia Fr{\"u}hwirth-Schnatter at EFaB 2013 @ Duke. 

\section{Basic Idea and Motivation: Priors on $\pm \sqrt{\sigma^2}$ instead of $\sigma^2$}

Start with the local level model for simplicity. For $t=1,2,\cdots,T$:
\begin{align*}
  y_t & = \theta_t + v_t \\
  \theta_t & = \theta_{t-1} + w_t
\end{align*}
with
\begin{align*}
  \begin{bmatrix} v_{1:T} \\ w_{1:T} \end{bmatrix} &\sim N\left(\bm{0}_{2T\times 1}, \begin{bmatrix} V\bm{I}_{T} & \bm{0}_{T\times T} \\ \bm{0}_{T\times T} & W\bm{I}_{T} \end{bmatrix} \right)
\end{align*}
where $\bm{I}_T$ is the $T\times T$ identity matrix and $\bm{0}_{p\times q}$ is a $p\times q$ matrix of zeroes. 

To complete the model we need a prior for $(\theta_0, V, W)$. Sylivia's suggestion is to use what ammounts to a $G\left(\frac{1}{2}, \frac{1}{2Q}\right)$ prior for each of $V$ and $W$ with $Q$ chosen appropriately, but to do this by specifying normal priors on $\pm \sqrt{V}$ and $\pm \sqrt{W}$.

Let's ignore the dynamic setting for a moment and essentially impose that $W=0$ so that the model is just
\begin{align*}
  y_t \stackrel{iid}{\sim} & N(\theta, V).
\end{align*}
So now we need a prior for $(\theta, V)$.  We'll suppose $\theta$ is known to simplify even more, then suppose that $V$ has the $G(1/2, 1/(2Q))$ prior. This means that the prior density of $V$ is
\begin{align*}
  p(V) \propto & V^{-1/2}e^{-V/(2Q)}.\\
\end{align*}
Now let $\tau = \pm \sqrt{V}$ and suppose that a priori $\tau \sim N(0,Q)$. Then
\begin{align*}
  p(\tau) \propto & e^{-\tau^2/(2Q)}.\\
\end{align*}
If we transform from $\tau$ to $V$, $V=\tau^2$ and the jacobian is the same for both the positive and negative sides of $\tau$:
\begin{align*}
  |J| & = V^{-1/2}/2\\
\end{align*}
which gives 
\begin{align*}
  p(V) \propto & V^{-1/2} e^{-V/(2Q)}.\\
\end{align*}
So a $N(0,Q)$ prior on $\tau$ and a $G(1/2, 1/(2Q))$ prior on $V$ are equivalent. 

Given this prior we can write the posterior distribution of $\tau$ as
\begin{align*}
  p(\tau|y_{1:T}) \propto & \tau^{-T}e^{\sum_{t=1}^T(y_t - \theta)^2/(2\tau^2)}e^{-\tau^2/(2Q)}\\
  \propto & \tau^{-T}e^{-\frac{1}{2}\left(\sum_{t=1}^T(y_t-\theta)^2/\tau^2 + \frac{1}{Q}\tau^2\right)}
\end{align*}
or the posterior distribution of $V$ as
\begin{align*}
  p(V|y_{1:T}) \propto & V^{-(T+1)/2}e^{\sum_{t=1}^T(y_t - \theta)^2/(2V)}e^{-V/(2Q)}\\
  \propto & V^{-(T+1)/2}e^{-\frac{1}{2}\left(\sum_{t=1}^T(y_t-\theta)^2/V + \frac{1}{Q}V\right)}.
\end{align*}
The distribution of $V$ is an instance of the generalized inverse gaussian distribution, or GIG. A random variable $x>0$ has a $GIG(a,b,p)$ distribution if the density of $x$ is given by
\begin{align*}
  p(x) \propto & x^{p-1}e^{-(ax + b/x)/2}
\end{align*}
for $a\ge 0$, $b \ge 0$, and $p$ real valued. The gamma and inverse gamma distributions (among others) are special cases.

If we treat $y_t$ as a vector and thus $V$ as a matrix, let $CC'=V$ i.e. $C$ is the lower triangular cholesky decomposition of $V$. Then in a similar fashion you can put a normal prior on each of the components of $C$ (treating it as $\pm chol(V)$). Things get complicated here, but in a different setting this ends up being a pretty convenient prior for the variance of a hierachical distribution for regression parameters, and in the Gibbs sampler $C$ ends up being drawn from a normal distribution. We'll ignore that for now and go back to the LLM.

Suppose now that $V=\tau^2$ and $W=\sigma^2$ with $(\tau,\sigma)\sim N(0,Q)N(0,R)$, i.e. indpendent mean zero normal distributions with potentially different variances. We'll assume that $\theta_0$ is independent of $(\tau,\sigma)$ in the prior with $\theta_0\sim N(m_0,C_0)$. Now we can write the conditional posteriors of $\tau$ and $\sigma$ as
\begin{align*}
  p(\tau|\theta_{0:T},\cdots)\propto & \tau^{-T} e^{-\left(\sum_{t=1}^T(y_t-\theta_t)^2/\tau^2 + \frac{1}{Q}\tau^2\right)/2}\\
  p(\sigma|\theta_{0:T},\cdots)\propto & \sigma^{-T} e^{-\left(\sum_{t=1}^T(\theta_{t-1}-\theta_t)^2/\sigma^2 + \frac{1}{R}\sigma^2\right)/2}.
\end{align*}

Suppose now we reparameterize in terms of $\gamma_t=(\theta_t-\theta_{t-1})/\sqrt{W}=(\theta_t - \theta_{t-1})/\sigma$ for $t=1,\cdots,T$ and $\gamma_0=\theta_0$. The determinant of the jacobian of this transformation ends up being $W^{T/2}$. Then we have the conditional posteriors given $\gamma_{0:T}$ instead of $\theta_{0:T}$:
\begin{align*}
  p(\tau|\gamma_{0:T},\cdots)\propto & \tau^{-T} e^{-\left(\sum_{t=1}^T(y_t-\sigma\sum_{s=1}^t\gamma_s - \gamma_0)^2/\tau^2 + \frac{1}{Q}\tau^2\right)/2}\\
  p(\sigma|\gamma_{0:T},\cdots)\propto & e^{-\left(\sum_{t=1}^T(y_t-\sigma\sum_{s=1}^t\gamma_s - \gamma_0)^2/\tau^2 + \frac{1}{R}\sigma^2\right)/2}.
\end{align*}
So $\tau$ still has a GIG distribution, but notice that $\sigma$ now has a normal distribution with variance $\hat{\Sigma}=\left(\frac{1}{\tau^2}\sum_{t=1}^T \left( \sum_{s=1}^t\gamma_s\right)^2 + \frac{1}{R}\right)^{-1}$ and mean $\hat{\mu}=\hat{\Sigma}\frac{1}{\tau^2}\sum_{t=1}^T(y_t-\gamma_0)\sum_{s=1}^t\gamma_s$. This means no awful rejection sampling in order to draw $W|\gamma_{0:T},\cdots$ or $V|\psi_{0:T},\cdots$.

\section{Attempts to generalize}
Computationally cheap MCMC is a good reason to use this prior provided that it's flexible enough for the purpose of inference. However, note that the shape parameter of the implied gamma distribution on the variance is held fixed in this family of probability distributions. In other words, we're assuming that $V\sim G(\alpha, \beta)$ with $\beta=1/(2Q)$ and $\alpha$ fixed at $\alpha=1/2$. The question is, then, is there a prior on $\tau=\pm \sqrt{V}$ that implies the $G(\alpha,1/(2Q))$ prior on $V$? There is, and it looks like this:
\begin{align*}
  p(\tau)\propto &(\tau^2)^{\alpha - 1/2}\exp\left[-\beta\tau^2\right].
\end{align*}
With $\beta=1/(2Q)$. To see that this results in a $G(\alpha,1/(2Q))$ prior on $V=\tau^2$, consider the transformation. The jacobian is the same as before, giving
\begin{align*}
  p(V)\propto & V^{\alpha-1}\exp\left[-\beta V\right].
\end{align*}
I'm calling the distribution of $\tau$ the ``signed Nakagami distribution'' for reasons that will be clear shortly. The normalizing constant of the signed Nakagami distribution is
\begin{align*}
  \int_{-\infty}^\infty (\tau^2)^{\alpha - 1/2}\exp\left[-\beta\tau^2\right]d\tau & = \int_{0}^\infty y^{\alpha -1} \exp\left[-\beta V\right]V \\
  & = \Gamma(\alpha)/\beta^\alpha
\end{align*}
so
\begin{align*}
p(\tau) = & \frac{\beta^\alpha}{\Gamma(\alpha)}(\tau^2)^{\alpha-1/2}\exp\left[-\beta\tau^2\right].\\
&= \frac{1}{\Gamma(\alpha) (2Q)^\alpha}(\tau^2)^{\alpha-1/2}\exp\left[-\frac{1}{2Q}\tau^2\right].
\end{align*}
Then the CDF of the signed Nakagami distribution is:
\begin{align*}
  P(\tau\leq x) & = \frac{1 + sgn(x)F(x^2;\alpha,\beta)}{2}
\end{align*}
where $F(x;\alpha,\beta)$ is the CDF of the gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$. To see this, note that if $x<0$ then
\begin{align*}
  P(\tau \leq x) = &\int_{-\infty}^x \frac{\beta^\alpha}{\Gamma(\alpha) }(\tau^2)^{\alpha-1/2}\exp\left[-\beta\tau^2\right] d\tau \\
    &=-\frac{1}{2}\int_\infty^{x^2} \frac{\beta^\alpha}{\Gamma(\alpha) }(\tau^2)^{\alpha-1/2}\exp\left[-\beta \tau\right] d\tau\\
    &= \frac{1 - F(x^2;\alpha,\beta)}{2}.
\end{align*}
Now since the signed Nakagami distribution is clearly symmetric around 0, $P(\tau\leq 0)=1/2$ and thus if $x>0$
\begin{align*}
  P(\tau \leq x) = &\int_{-\infty}^x \frac{\beta^\alpha}{\Gamma(\alpha) }(\tau^2)^{\alpha-1/2}\exp\left[-\beta\tau^2\right] d\tau \\
  &= \frac{1}{2} + \int_{0}^x \frac{\beta^\alpha}{\Gamma(\alpha) }(\tau^2)^{\alpha-1/2}\exp\left[-\beta\tau^2\right] d\tau \\
    &=\frac{1}{2} + \frac{1}{2}\int_0^{x^2} \frac{\beta^\alpha}{\Gamma(\alpha) }(\tau^2)^{\alpha-1/2}\exp\left[-\beta \tau\right] d\tau\\
    &= \frac{1 + F(x^2;\alpha,\beta)}{2}.
\end{align*}

Now the Nakagami distribution has pdf
\begin{align*}
  p(x) = \frac{2 \beta^\alpha}{ \Gamma(\alpha)}x^{2\alpha -1}\exp\left[-\beta x^2\right]
\end{align*}
for $x>0$, and cdf $P(x\leq t)=F(t^2;\alpha,\beta)$ for $t>0$ where again $F$ is the cdf of the gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$. This is not the usual parameterization of the Nakagami distribution, but it's more convenient for our purposes. Now suppose we sign this distribution by flipping a fair coin. If it comes up heads, we'll call the drawn value of $x$ positive, while if it comes tails we'll call it negative. Let $w=1$ when the coin comes up heads and $w=-1$ when it comes up tails. Then if $t> 0$
\begin{align*}
  P(xw \leq t) =& P(x\leq t | w=1)P(w=1) + P(x\geq t|w=-1)P(w=-1) \\
  &= F(t^2;\alpha,\beta)\frac{1}{2} + \frac{1}{2}\\
  &=\frac{1+ F(t^2;\alpha,\beta)}{2}
\end{align*}
while if $t<0$
\begin{align*}
  P(xw \leq t) =& P(x\leq t | w=1)P(w=1) + P(x\geq t|w=-1)P(w=-1) \\
  &= 0 + [1-F(t^2;\alpha,\beta)]\frac{1}{2}
\end{align*}
and finally $P(xw\leq 0) = P(w=-1) = 1/2$. So by flipping a fair coin in order to choose the sign of a random variate drawn from the Nakagami distribution, we obtain a draw from the signed Nakagami distribution. This is useful since in order to draw from the $Nakagami(\alpha,\beta)$ distribution, we can simply draw from the $G(\alpha,\beta)$ distribution and take the positive square root, thus drawing from the signed Nakagami distribution is simple and cheap.

In the sources I've seen, the Nakagami distribution is only defined for $\alpha > 1/2$ and as $\alpha \to 1/2$ it converges to a half-normal distribution. However, the normalizing constant is finite for any $\alpha >0$ since the Nakagami distribution is simply the positive square root of the gamma distribution.

Suppose we have the basic local level model from above again: 
\begin{align*}
  y_t &= \theta_t + v_t\\
  \theta_t &= \theta_{t-1} + w_t
\end{align*}
with $v_t\stackrel{iid}{\sim} N(0,\tau^2)$ and $w_t \stackrel{iid}{\sim} N(0,\sigma^2)$. For priors we'll assume $p(\theta_0,\tau,\sigma)=p(\theta_0)p(\tau)p(\sigma)$ with $\theta_0\sim N(\theta_0,C_0)$ and signed Nakagami priors on $\tau$ and $\sigma$, i.e. $\tau\sim sNaka(a, 1/(2Q))$ and $\sigma \sim sNaka(b, 1/(2R))$. Then we can write the full conditional posteriors of $\tau$ and $\sigma$ as
\begin{align*}
  p(\tau|\theta_{0:T},\cdots) \propto & (\tau^2)^{a - (T+1)/2}\exp\left[-\frac{1}{2}\left(\frac{1}{Q}\tau^2 + \sum_{t=1}^T(y_t - \theta_t)^2 \frac{1}{\tau^2}\right)\right]\\
  p(\sigma|\theta_{0:T},\cdots) \propto & (\sigma^2)^{b - (T+1)/2}\exp\left[-\frac{1}{2}\left(\frac{1}{R}\sigma^2 + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2 \frac{1}{\sigma^2}\right)\right].
\end{align*}
This results in GIG distributions for both $\tau^2$ and $\sigma^2$ - again, this is something we can easily sample from, but consider conditioning on $\gamma_{0:T}$ instead of $\theta_{0:T}$ where again $\gamma_0=\theta_0$ and $\gamma_t = (\theta_t-\theta_{t-1})/\sigma$. This results in the conditional posteriors
\begin{align*}
  p(\tau|\gamma_{0:T},\cdots) & \propto  (\tau^2)^{a - (T+1)/2}\exp\left[-\frac{1}{2}\left(\frac{1}{Q}\tau^2 + \sum_{t=1}^T\left(y_t - \gamma_0 - \sigma\sum_{s=1}^t\gamma_s\right)^2\frac{1}{\tau^2} \right)\right]\\
  p(\sigma|\gamma_{0:T},\cdots) &\propto  (\sigma^2)^{b - 1/2}\exp\left[-\frac{1}{2}\left(\frac{1}{R}\sigma^2 + \sum_{t=1}^T\left(y_t-\gamma_0 - \sigma\sum_{s=1}^t\gamma_s\right)^2\frac{1}{\tau^2}\right)\right]\\
  \propto & (\sigma^2)^{b - 1/2}\exp\left[-\frac{1}{2}\left(\frac{1}{R} + \sum_{t=1}^T\left(\sum_{s=1}^t\frac{\gamma_s}{\tau}\right)^2\right)\left(\sigma - \left(\frac{1}{R} + \sum_{t=1}^T\left(\sum_{s=1}^t\frac{\gamma_s}{\tau}\right)^2\right)^{-1}\sum_{t=1}^T\frac{y_t-\gamma_0}{\tau}\sum_{s=1}^t\frac{\gamma_s}{\tau}\right)^2\right] 
\end{align*}

Again, $\tau$ has a GIG full conditional distribution. However, $\sigma$ has something a bit different --- it's related to the signed Nakagami distribution, but the density now has the form:
\begin{align*}
  p(x)\propto (x^2)^{\alpha - 1/2}\exp\left[-\frac{1}{2\beta}(x-\mu)^2\right].
\end{align*}
Using the known formulas for the absolute noncentral moments of the normal distribution, we can obtain the normalizing constant. If $x\sim N(\mu,\beta)$ and $p>0$ then 
\begin{align*}
  E[|x|^{p} ] = \frac{(2\beta)^{p/2}}{\sqrt{\pi}}\Gamma\left(\frac{1+p}{2}\right)M\left(-\frac{1}{2}p,\frac{1}{2},-\frac{1}{2}\frac{\mu^2}{\beta}\right)
\end{align*}
where $M(a,b,z)$ is the confluent hypergeometric function of the first kind, i.e.
\begin{align*}
  M(a,b,z) & = \sum_{n=0}^\infty \frac{a_{(n)}z^n}{b_{(n)}n!}
\end{align*}
where the notation $a_{(n)}$ means  $a_{(0)}=1$ and $a_{(n)} = a(a+1)(a+2)\cdots(a+n-1)$. Thus we can write the distribution in question as 
\begin{align*}
  p(x) =& \frac{1}{E[|z|^{2\alpha -1}]}\frac{1}{\sqrt{2\pi \beta}} (x^2)^{\alpha - 1/2}\exp\left[-\frac{1}{2\beta}(x-\mu)^2\right]\\
  =& \frac{1}{(2\beta)^{\alpha}\Gamma\left(\alpha\right)M\left(\frac{1}{2}-\alpha,\frac{1}{2},-\frac{1}{2}\frac{\mu^2}{\beta}\right)}(x^2)^{\alpha - 1/2}\exp\left[-\frac{1}{2\beta}(x-\mu)^2\right]\\
    =& \frac{1}{(2\beta)^{\alpha}e^{-\frac{1}{2}\frac{\mu^2}{\beta}}\Gamma\left(\alpha\right)M\left(\alpha,\frac{1}{2},\frac{1}{2}\frac{\mu^2}{\beta}\right)}(x^2)^{\alpha - 1/2}\exp\left[-\frac{1}{2\beta}(x-\mu)^2\right]\\
\end{align*}
where $z$ is a $N(\mu,\beta)$ random variable and the third line follows from Krummer's transformation: $M(a,b,z)=e^zM(b-a,b,-z)$. I'll call this the noncentral signed Nakagami distribution with noncentrality parameter $\mu$, or $nsNaka(\alpha, \beta, \mu)$ --- though note that this is not simply a location transformation of the original signed Nakagami distribution. Also note that while the signed Nakagami distribution only requires $\alpha>0$, the noncentral signed Nakagami distribution requires $\alpha\geq 1/2$ whenever $\mu\neq 0$. This is actually restricts our use of the signed Nakagami distribution as a prior to $\alpha\geq 1/2$ because if we put a $sNaka(\alpha,\beta)$ prior on $\sigma$, its full conditional posterior under the scaled disturbance parameterzation (the $\gamma_{0:T}$'s) is $nsNaka(\alpha,\delta,\mu)$ where $\mu>0$ and $\delta$ are functions of the data, the $\gamma$'s and $\tau$. So if we set $\alpha <1/2$ the full conditional isn't actually a probability distribution.

\section{Properties of the noncentral signed Nakagami distribution}
For reference, the density of the noncentral signed Nakagami distribution is
\begin{align}\label{dens1}
  p(x)  =& \frac{1}{(2\beta)^{\alpha}\Gamma\left(\alpha\right)M\left(\frac{1}{2}-\alpha,\frac{1}{2},-\frac{1}{2}\frac{\mu^2}{\beta}\right)}(x^2)^{\alpha - 1/2}\exp\left[-\frac{1}{2\beta}(x-\mu)^2\right].
\end{align}
with $\alpha\geq 1/2$, $\beta>0$ and $\mu\in\Re$ where $\Gamma(.)$ is the gamma function and $M(.,.,.)$ is the confluent hypergeometric function of the first kind, i.e.
\begin{align*}
M(a,b,z)=\sum_{n=0}^\infty\frac{\Gamma(a + n)\Gamma(b)z^n}{\Gamma(a)\Gamma(b+n)n!}  
\end{align*}. 
The main thing I would like to do is draw from this distribution cheaply. The cdf inversion method doesn't look too promising since ultimately I want to generalize this to multiple dimensions (for covariance matrices), though it might work fine in this case as long as the approximation routines for the confluent hypergeometric function work well. Rejection sampling is an option, but some care is involved because the distribution is often bimodal. If $\alpha=1/2$ clearly this is just a normal distribution so the mode is $\mu$. If $\alpha > 1/2$, however, consider the log density
\begin{align*}
\log(p(x)) & = \log(C) + (\alpha - 1/2)\log(x^2) - \frac{1}{2\beta}(x-\mu)^2.
\end{align*}
The FOC is
\begin{align*}
&  (\alpha - 1/2)\frac{2}{x} - \frac{1}{\beta}(x-\mu) = 0\\
&\implies \frac{1}{\beta}x^2 - \frac{\mu}{\beta}x + 1 - 2\alpha = 0
\end{align*}
so that
\begin{align*}
  x^{mode} & = \frac{\mu/2 \pm \sqrt{\mu^2/\beta^2 + 4(2\alpha - 1)/\beta}}{2/\beta}\\
    & = \mu/2 \pm \sqrt{\mu^2/4 + (2\alpha -1)\beta}.
\end{align*}
The SOSC requires 
\begin{align*}
  &-(2\alpha - 1)(x^{mode})^2- \frac{1}{\beta} < 0 \\
  &\iff (x^{mode})^2 > -\beta(2\alpha -1)
\end{align*}
but
\begin{align*}
  (x^{mode})^2 = & \mu^2/2 + (2\alpha -1)\beta \pm \mu\sqrt{\mu^2/4 + (2\alpha-1)\beta}.
\end{align*}
So $\mu/2 + \sqrt{\mu^2/4 + (2\alpha -1)\beta}$ is always a mode while $\mu/2 - \sqrt{\mu^2/4 + (2\alpha -1)\beta}$ is a mode if
\begin{align*}
  &\mu^2/2  + 2\beta(2\alpha -1) - \mu\sqrt{\mu^2/4 + (2\alpha-1)\beta} >0\\
  &\iff 2\left(\mu^2/4 + (2\alpha-1)\beta\right)>\mu\sqrt{\mu^2/4 + (2\alpha-1)\beta}\\
  &\iff \mu^2/4 + (2\alpha - 1)\beta > \mu^2/4.
\end{align*}
So if $\alpha>1/2$ then both solutions are maximums and thus there are two modes.

The first moment of this distribution is tricky to compute, but the second moment is pretty easy (up to approximating the confluent hypergeometric function). Let $C(\alpha,\beta,\mu)$ be the normalizing constant of the $nsNaka(\alpha,\beta,\mu)$ distribution, i.e. 
\begin{align*}
  \int_{-\infty}^{\infty}C(\alpha,\beta,\mu)(x^2)^{\alpha-1/2}\exp\left[-\frac{1}{2\beta}(x-\mu)^2\right]dx = 1.
\end{align*}
Then
\begin{align*}
  E[x^2] &= \int_{-\infty}^{\infty}C(\alpha,\beta,\mu)(x^2)^{\alpha}\exp\left[-\frac{1}{2\beta}(x-\mu)^2\right]\\
  &= \frac{C(\alpha,\beta,\mu)}{C(\alpha+1,\beta,\mu)}\\
  & = \frac{(2\beta)^{\alpha+1}\Gamma(\alpha+1)M\left(-\frac{1}{2}-\alpha,\frac{1}{2},-\frac{1}{2}\frac{\mu^2}{\beta}\right)}{(2\beta)^\alpha\Gamma(\alpha)M\left(\frac{1}{2}-\alpha,\frac{1}{2},-\frac{1}{2}\frac{\mu^2}{\beta}\right)}\\
  & = 2\alpha\beta\frac{M\left(\alpha+1,\frac{1}{2},\frac{1}{2}\frac{\mu^2}{\beta}\right)}{M\left(\alpha,\frac{1}{2},\frac{1}{2}\frac{\mu^2}{\beta}\right)}
\end{align*}

\section{Some notes on other priors}
I started with the $N(0,Q)$ prior on $\pm\sqrt{V}$ where $V$ is a variance parameter. This implies a $Gamma(1/2,1/(2Q))$ prior on $V$. I tried to generalize this by allowing the shape parameter of the implied Gamma distribution to be something other than 1/2. However, Another option is to pick a distribution for $\pm\sqrt{V}$ which is more general than the normal distribution, e.g. the non-standardized $t$ distribution, $t(m, Q, v)$ where $m$ is a location parameter, $Q$ is a scale parameter, and $v$ is the degrees of freedom. Using the fact that the $t$ distribution can be written as a scale mixuture of normals (if $x\sim t(m,Q,v)$ then $x|\omega \sim N(m,\omega)$ and $\omega\sim IG(v/2, vQ/2)$), we can get nice full conditional distributions for either $V$ or $\pm\sqrt{V}$ and either $W$ or $\pm\sqrt{W}$. This adds another Gibbs step to the algorithm but doesn't change much other than that, i.e. we still have GIG distributions for $V|\theta,...$ and $W|\theta,...$, but now they're also conditional on scale parameters $\omega_V$ and $\omega_W$ which now each have to be drawn in separate steps, but these will each be inverse gamma draws. The draw for $W|\gamma,...$ is still a normal draw, but again that's conditional on $\omega_W$ which also has to be drawn in a separate inverse gamma step. It would be interesting to generalize this even more - e.g. a scale mixture of signed Nakagami distributions, though this is equivalent to just putting a hyperprior on the scale parameter of the signed Nakagami distribution. In that sense, then, we're back to the problems of the previous section.

Another thing that we can easily do and should mention is allow for dependence in the prior - $p(V,W)\neq p(V)p(W)$. The only sampling algorithm that draws $(V,W)$ jointly is the state sampler (and interweaving algorithms that depend on it), so it shouldn't change much elsewhere -- provided that $p(V|W)$ and $p(W|V)$ are of sufficiently nice form.
\end{document}
