<<set-parent-basemix, echo=FALSE, cache=FALSE>>=
set_parent('mcmcex.Rnw')
@

\section{Base Results}

<<plotsetup, echo=FALSE, message=FALSE>>=
library(ggplot2)
library(scales)
library(plyr)
library(xtable)
library(reshape2)
load("mixing/OldSims/samout.RData")
load("mixing/OldSims/postcors.RData")

sams <- c("error", "state", "dist")
alts <- c("sdalt", "sealt", "dealt", "trialt")
ints <- c("sdint", "seint", "deint", "triint")
samout$type <- "Base" #$
samout$type[samout$sampler %in% alts] <- "Alternating" 
samout$type[samout$sampler %in% ints] <- "Interweaving" 
samout$samplers <- "Base" #$
samout$samplers[substr(samout$sampler, 1, 2)=="sd"] <- "State-Dist" 
samout$samplers[substr(samout$sampler, 1, 2)=="se"] <- "State-Error" 
samout$samplers[substr(samout$sampler, 1, 2)=="de"] <- "Dist-Error" 
samout$samplers[substr(samout$sampler, 1, 3)=="tri"] <- "Triple" 

meltedsam <- melt(samout, id=c("type", "samplers", "sampler", "V.T", "W.T", 
                            "T.T"))
Vs <- unique(meltedsam$V.T) #$
Ws <- Vs
breaks <- Vs[seq(1,11,2)]
label_both_parsed <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), function(x) parse(text = x))
}
label_both_parsed_split <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), 
        function(x) parse(text = strsplit(x, "\\.")[[1]][1]))
}
meltedcors <- melt(postcors, id=c("V.T", "W.T", "T.T"))
opts_chunk$set(fig.width=7, fig.height=4, out.width='1\\textwidth', 
               fig.pos='!ht')
@ 

In order to test these algorithms, I simulated a fake dataset from
the local level model for various choices of $V$, $W$, and $T$. We created a grid over $V$--$W$ space with 
\[
V,W=10^{-2}, 10^{-1.5}, 10^{-1}, 10^{-0.5}, 10^{0}, 10^{0.5}, 10^{1}, 10^{1.5}, 10^{2}, 10^{2.5}, 10^{3}
\]
and we simulated a dataset for all possible combinations of $V$ and $W$ with each of $T=10, 100, 1000$.
Then for each dataset, we fit the local level model using each
sampler. We used the same priors for each model: $\theta_0\sim N(0,10^7)$, $V\sim IG(5, 4*\tilde{V})$, and $W\sim IG(5, 4*\tilde{W})$, mutually independent where 
$(\tilde{V},\tilde{W})$ are the true values of $V$ and
$W$ used to simulate the time series. This ensures all priors and thus
the posterior are proper and results in independent inverse gamma
priors on $V$ and $W$ with prior means equal to the respective true
values and moderately large prior variances. We chose these priors for $(V,W)$ For each sampler to sidestep issues of posterior propriety or noninformativeness for variances since these issues are largely orthogonal to our goals.

For each model and each sampler, we $n=3000$ draws and threw away the first 500 as burn in. The chains were started at the true values used to simulated the time series, so we can examine the behavior of the chains to determine how well they mix but not how quickly they convergence since we've essentially enforced convergence at from iteration one. Define the effective sample proportion (EFP) as the effective sample size (ESS, REFERENCE HERE) divided by the actual sample size, i.e. $ESP=ESS/n$. $ESP=1$ indicates that the markov chain is behaving as if it obtains iid draws from the posterior. Figure \ref{baseESplotT10} contains plots of ESP for $V$ and $W$ in each chain of each sampler. The state sampler has a low ESP for $V$ and a high ESP for $W$ when the signal-to-noise ratio, $W/V$, is larger than one. When the signal-to-noise ratio is smaller than one, on the other hand, the state sampler has a low ESP for $W$ and a high ESP for $V$. In the usual case where the signal to noise ratio isn't too different from one, the state sampler has a modest to low ESP for both $V$ and $W$. Note that the particular values of $V$ and $W$ don't seem to matter at all --- just their relative values, i.e. the signal-to-noise ratio $W/V$. Moving up any diagonal on the plots for $V$ and $W$ in the state sampler, $W/V$ is constant and the ESS appears roughly constant.

Figure \ref{baseESplotT10} tells a different story for the scaled disturbance sampler. When the signal-to-noise ratio is less than one, ESPs for both $V$ and $W$ are nearly 1, i.e. the effective sample size is nearly the actual sample size of the chain. When the signal-to-noise ratio is greater than one, however, ESP for both $V$ and $W$ becomes small, especially for $V$. Once again the absolute values of $V$ and $W$ don't matter for this behavior --- just the relative values. The scaled disturbance sampler has essentially the opposite properties. When $W/V$ is large, it has a near 1 ESP for both $V$ and $W$.


<<baseESplotT10, fig.cap=cap, echo=FALSE>>=
cap <- "Effective sample proportion in the posterior sampler for a time series of length $T=10$ for $V$ and $W$, and for the state, scaled disturbance, and scaled error samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Also note that for plotting purposes, effective sample proportions larger than $1$ were rounded down to $1$"
vars <- c("V.ES", "W.ES")
castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable ~ ., 
                   subset=.(variable %in% vars  & T.T==10 & sampler %in% sams ))
colnames(castedsam)[5] <- "value"
ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
  geom_tile() +
  scale_fill_gradient("EffProp", low=muted("red"), high="white",
                         guide=guide_colorbar(barheight=10),
                         limits=c(0,1), na.value="white") +
  facet_grid(variable~sampler, scales="free", labeller=label_both_parsed_split) +
  scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
  ggtitle("Effective Sample Proportion for V and W in the base samplers, T=10") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
@ 
