<<set-parent-compalg, echo=FALSE, cache=FALSE>>=
set_parent('mcmcex.Rnw')
@

\section{Componentwise Interweaving}

{\it NOTE: THE ASIS AND COMPONENTWISE ASIS SAMPLERS FROM \citet{yu2011center} ALWAYS DO THE ANCILLARY AUGMENTATION FIRST IN EACH GIBBS COMPONENT, BUT I ALWAYS DO THE SA FIRST --- FIND OUT IF THIS MATTERS}

While it doesn't seem possible to find an SA-AA pair for $(V,W)$ in the local level model, note that if we hold $W$ constant, $\theta_{0:T}$ is a SA for $V$, or rather it's a SA for $V|W$. This opens up the possibility of interweaving within a Gibbs step, then interweaving again within the next Gibbs step, and so on if necessary. Each Gibbs step can even interweave between its own pair of parameterizations. This is called {\it componentwise} interweaving. For example, suppose we have two parameters, $(V,W)$, and four data augmentations, $\theta_j$, $j=1,...,4$. Where $\theta_1$ \& $\theta_2$ are distinct and $\theta_3$ \& $\theta_4$ are distinct, though e.g. it's possible that $\theta_1=\theta_3$. The joint distribution of $(\theta_1, \theta_2, \theta_3, \theta_4|y_{1:T},V,W)$ still needs to be well defined. Then the componentwise algorithm is as follows:
\begin{alg}\mbox{}\\[-\baselineskip]
  \begin{enumerate}\label{comp1}
  \item Draw $\theta_{1}$ from $p(\theta_{1}|V^{(k)}, W^{(k)}, y_{1:T})$
  \item Draw $V^{(k+.5)}$ from $p(V|\theta_1,W^{(k)}, y_{1:T})$
  \item Draw $\theta_{2}$ from $p(\theta_{2}|V^{(k+.5)}, W^{(k)}, y_{1:T})$
  \item Draw $V^{(k+1)}$ from $p(V|W^{(k)}, \theta_{2}, y_{1:T})$
  \item Draw $\theta_{3}$ from $p(\theta_{2}|V^{(k+1)}, W^{(k)}, y_{1:T})$
  \item Draw $W^{(k+.5)}$ from $p(W|\theta_3,V^{(k+1)}, y_{1:T})$
  \item Draw $\theta_{4}$ from $p(\theta_{3}|V^{(k+1)}, W^{(k+.5)}, y_{1:T})$
  \item Draw $W^{(k+1)}$ from $p(W|\theta_{4}, V^{(k+1)}, y_{1:T})$
  \end{enumerate}
\end{alg}
Steps 1-4 interweave for $V|W$ and steps $5-8$ interweave for $W|V$. Once again, an additional draw or transformation may be need to obtain the posterior distribution of the desired data augmentation vector.

This strategy is useful because it's a lot easier to find {\it conditional } SA's and AA's, in other words it's easier to find data augmentations that are SA or AA for a component of the parameter vector conditional on the rest of the parameter vector. One nice feature of this strategy is that it isn't necessary to interweave in every step of the Gibb's sampler --- if, for example, no SA can be found for $\phi_1$, a normal Gibbs step can be used there instead of an interweaving step. Note that steps 3 and 7 may simply be transformations if the transformation from one data augmentation to another is one-to-one. If this is the case, then the conditional distributions used in steps 2 and 4 must be different --- i.e. we must have
\begin{align}
p(V|W, \theta_{1}, y_{1:T}) &\neq p(V|W, \theta_{2}, y_{1:T})
\end{align}
Otherwise, the extra interweaving steps don't accomplish anything. There's a similar requirement for steps 6 and 8.

{\it FROM HERE, THINGS NEED TO BE TRANSLATED INTO THE CORRECT NOTATION AND PRUNED}

The standard data augmentation for the local level model, $\theta_{0:T}$, is a SA for $W|V$ and an AA for $V|W$. If we can find another data augmentation that is AA for $W|V$ and SA for $V|W$, the CIS algorithm is quite simple. Equation \ref{DA1} suggests one approach: somehow move $W$ to line 1 and $V$ to line 2 of the equation. It turns out that if we augment with $v_{0:T}$ where $v_0=\theta_0$ instead of with $\theta_{0:T}$, exactly this occurs, but the resulting algorithm is actually equivalent to the original algorithm (the state sampler). Instead, we would like to use a different secondary data augmentation for each of the two steps of our gibbs sampler. What we need is one augmentation that is AA for $W|V$ and another augmentation that is SA for $V|W$. It turns out that $\gamma_t=(\theta_t-\theta_{t-1})/\sqrt{W}$ (with $\gamma_0=\theta_0$) is AA for $W|V$ -- actually for $(V,W)$ jointly. To see this, note that
\[
p(y_{1:T},\gamma_{1:T}|V,W,\gamma_0) \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\gamma_t^2\right] V^{-T/2}\exp\left[-\frac{1}{2V}\sum_{t=1}^T(y_t - \gamma_0 - \sqrt{W}\sum_{s=1}^t\gamma_t)^2\right]
\]
so that we can write the model as, for $t=1,2,...,T$:
\begin{align}\label{DA2}
  y_t | \gamma_{0:T},V,W &\stackrel{ind}{\sim} N(\gamma_0 + \sqrt{W}\sum_{s=1}^t\gamma_s, V)\notag \\
  \gamma_t | V,W &\stackrel{iid}{\sim} N(0,1)
\end{align}

So we can see that the $\gamma_{0:T}$ augmentation is not a SA for either case, but it is an AA for $(V,W)$ jointly, and thus for each one conditional on the other. It turns out that the same result holds for the $\psi_t=(y_t-\theta_t)/\sqrt{V}$ augmentation --- it's also not a SA for either case and is an AA for $(V,W)$ jointly. In this case we have
\[
p(y_{1:T},\psi_{1:T}|V,W,\psi_0) \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\psi_t^2\right] W^{-T/2}\exp\left[-\frac{1}{2W}\sum_{t=1}^T(y_t - y_{t-1} - \sqrt{V}(\psi_t - \psi_{t-1})^2\right]
\]
Now we can write the model as, for $t=1,2,...,T$, defining $y_0=0$:
\begin{align}\label{DA3}
  y_t | \psi_{0:T}, y_{0:t-1}, V,W &\stackrel{ind}{\sim} N(y_{t-1} + \sqrt{V}(\psi_t - \psi_{t-1}), W)\notag \\
  \psi_t | V,W &\stackrel{iid}{\sim} N(0,1)
\end{align}

So all we need is a SA for $V|W$ or ideally for $(V,W)$. Before we get into that, we can still implement a partial CIS algorithm as follows:

\begin{enumerate}
  \item Draw $\theta_{0:T}$ from $p(\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T})$ using FFBS.
  \item Draw $V^{(k+1)}$ from $p(V|W^{(k)},\theta_{0:T},y_{1:T})$
  \item Draw $W^{(k+.5)}$ from $p(W|V^{(k+.5)},\theta_{0:T},y_{1:T})$
  \item Update $\gamma_{0:T}$ where $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t-\theta_{t-1})/\sqrt{W}$.
  \item Draw $W^{(k+1)}$ from $p(W|V^{(k+1)},\gamma_{0:T},y_{1:T})$
\end{enumerate}

Note that in step 4, we have to use the $\gamma$ data augmentation even though the $\psi$ augmentation is also ancillary for $W$. The issue is that the distribution of $W|V,\theta,y$ and the distribution of $W|V,\psi,y$ are the same --- the transformation from $\theta$ to $\psi$ doesn't impact the density of $W$. In the case of inverse gamma priors for $V$ and $W$, the conditional density is also inverse gamma and the parameters of these distribution don't depend on whether we conditioned on $\theta$ or $\psi$. So the interweaving steps (steps 4 and 5) don't actually change the usual Gibbs sampler (a.k.a. the state sampler).

Now in order to introduce a full CIS algorithm, let's introduce two new data agumentations: $\tilde{\gamma}_t=\gamma_t\sqrt{W}/\sqrt{V}$ for $t=1,..,T$ with $\tilde{\gamma}_0=\gamma_0=\theta_0$ and $\tilde{\psi}_t=\psi_t\sqrt{V}/\sqrt{W}$ for $t=1,..,T$ with $\tilde{psi}_0=\psi_0=\theta_0$. For $\tilde{\gamma}$ this results in
\[
p(\tilde{\gamma}_{1:T},y_{1:T}|V,W,\tilde{\gamma}_0)\propto (W/V)^{-T/2}\exp\left[-\frac{1}{2W/V}\sum_{t=1}^T\tilde{\gamma}_t^2\right] V^{-T/2}\exp\left[-\frac{1}{2V}\sum_{t=1}^T(y_t - \tilde{\gamma}_0 - \sqrt{V}\sum_{s=1}^{t-1}\tilde{\gamma}_s)^2\right]
\]
Then we can write the model in terms of $\tilde{\gamma}_{0:T}$ as
\begin{align}\label{DA4}
  y_t|\tilde{\gamma}_{0:T}, V, W &\stackrel{ind}{\sim} N(\tilde{\gamma}_0 + \sqrt{V}\sum_{s=1}^{t-1}\tilde{\gamma}_s, V)\notag\\
  \tilde{\gamma}_t|V,W &\stackrel{iid}{\sim} N(0, W/V)
\end{align}
We can immediately see that $\tilde{\gamma}$ is a SA for $W|V$, but not a SA or an AA for anything else.

If we do the same for $\tilde{\psi}$, we get
\[
p(\tilde{\psi}_{1:T},y_{1:T}|V,W,\tilde{\psi}_0) \propto (V/W)^{-T/2}\exp\left[-\frac{1}{2V/W}\sum_{t=1}^T\tilde{\psi}_t^2\right] W^{-T/2}\exp\left[-\frac{1}{2W}\sum_{t=1}^T(y_t-y_{t-1}-\sqrt{W}(\tilde{\psi}_t -\tilde{\psi}_{t-1})^2\right]
\]
so that we can write the model as, for $t=1,2,...T$:
\begin{align}\label{DA5}
  y_t|\tilde{\psi}_{0:T}, y_{0:t-1}, V, W & \sim N(y_{t-1} + \sqrt{W}(\tilde{\psi}_t - \tilde{\psi}_{t-1}), W)\notag\\
  \tilde{\psi}_t|V,W &\stackrel{iid}{\sim}N(0,V/W)
\end{align}
We see immediately that $\tilde{\psi}$ is a SA for $V|W$ and not a SA nor an AA for anything else.

So now $\tilde{\gamma}$ and $\gamma$ are a SA-AA pair for $W|V$ while $\tilde{\psi}$ and $\psi$ are a SA-AA pair for $V|W$. In particular, both pairs draw the relevant parameter from separate distributions. For $\tilde{\gamma}$ and $\gamma$, we have
\begin{align*}
  p(W|V, \gamma_{0:T}, y_{1:T}) &\propto \exp\left[-\frac{1}{2V}\sum_{t=1}^T(y_t - \gamma_0 - \sqrt{W}\sum_{s=1}^{t-1}\gamma_s)^2\right]p(W|V,\gamma_0)\\
  p(W|V, \tilde{\gamma}_{0:T}, y_{1:T}) &\propto W^{-T/2}\exp\left[-\frac{1}{2W}V\sum_{t=1}^T\tilde{\gamma}_t^2\right]p(W|V,\tilde{\gamma}_0)
\end{align*}
where $p(W|V,\tilde{\gamma}_0)=p(W|V,\gamma_0)=p(W|V,\theta_0)$ is the conditional prior for $W$ given $V$ and $\theta_0=\gamma_0=\tilde{\gamma}_0$. Since these two densities are different, we can interweave between them successfully.

Now for $\tilde{\psi}$ and $\psi$ we have
\begin{align*}
  p(V|W, \psi_{0:T}, y_{1:T}) &\propto \exp\left[-\frac{1}{2W}\sum_{t=1}^T(y_t - y_{t-1} -\sqrt{V}(\psi_t - \psi_{t-1}))^2\right]p(V|W,\psi_0)\\
  p(V|W, \tilde{\psi}_{0:T}, y_{1:T}) & \propto V^{-T/2}\exp\left[-\frac{1}{2V}W\sum_{t=1}^T\tilde{\psi}_t^2\right]p(V|W,\tilde{\psi}_0)
\end{align*}
where $p(V|W,\tilde{\psi}_0)=p(V|W,\psi_0)=p(V|W,\theta_0)$ is the conditional prior for $V$ given $W$ and $\theta_0=\psi_0=\tilde{\psi}_0$. Once again, since these two densities are different, we can interweave between them successfully.

The full componentwise interweaving algorithm is then:
\begin{enumerate}
  \item Draw $\tilde{\psi}_{0:T}$ from $p(\tilde{\psi}_{0:T}|V^{(k)},W^{(k)},y_{1:T})$.
  \item Draw $V^{(k+.5)}$ from $p(V|W^{(k)}, \tilde{\psi}_{0:T}, y_{1:T})$.
  \item Update $\psi_{0:T}$.
  \item Draw $V^{(k+1)}$ from $p(V|W^{(k)}, \psi_{0:T}, y_{1:T})$.
  \item Update $\tilde{\gamma}_{0:T}$.
  \item Draw $W^{(k+.5)}$ from $p(W|V^{(k+1)}, \tilde{\gamma}_{0:T}, y_{1:T})$.
  \item Update $\gamma_{0:T}$.
  \item Draw $W^{(k+1)}$ from $p(W|V^{(k+1)}, \gamma_{0:T}, y_{1:T})$.
\end{enumerate}

In this algorithm we've ignored the $\theta_{0:T}$ augmentation despite the fact that we could have used it in step 6 since $\theta_{0:T}$ is a SA for $W|V$. It turns out, though, drawing $W|V,\theta,y$ and drawing $W|V,\tilde{\gamma},y$ are equivlent. To see this note that
\begin{align*}
  p(W|V, \tilde{\gamma}_{0:T}, y_{1:T}) &\propto W^{-T/2}\exp\left[-\frac{1}{2W}V\sum_{t=1}^T\tilde{\gamma}_t^2\right]p(W|V,\tilde{\gamma}_0)\\
  p(W|V,\theta_{0:T},y_{1:T}) & \propto W^{-T/2}\exp\left[-\frac{1}{2W}\sum_{t=1}^T(\theta_t-\theta_{t-1})^2\right]p(W|V,\theta_0)
\end{align*}

Since $\sqrt{V}\tilde{\gamma}_t=\sqrt{W}\gamma_t=\theta_t-\theta_{t-1}$ and $\theta_0=\tilde{\gamma}_0$, these two densities are the same. So it doesn't matter whether we use $\theta$ or $\tilde{\gamma}$ to make this draw.

It turns out that this same sort equivalence holds between $\tilde{\psi}$ and $\theta$ for $V|W$, though $\theta$ isn't a SA for $V|$ while $\tilde{\psi}$ is. To see that drawing from $p(V|W,\tilde{\psi}_{0:T}, y_{1:T})$ is equivalent to drawing from $p(V|W,\theta_{0:T},y_{1:T})$:
\begin{align*}
    p(V|W, \tilde{\psi}_{0:T}, y_{1:T}) & \propto V^{-T/2}\exp\left[-\frac{1}{2V}W\sum_{t=1}^T\tilde{\psi}_t^2\right]p(V|W,\tilde{\psi}_0)\\
    p(V|W, \theta_{0:T}, y_{1:T}) & \propto V^{-T/2}\exp\left[-\frac{1}{2V}\sum_{t=1}^T(y_t - \theta_t)^2\right]p(V|W,\theta_0)
\end{align*}
Again since $\sqrt{W}\tilde{\psi}=\sqrt{V}\psi=y_t-\theta_t$ and $\theta_0=\tilde{\psi}_0$, these two densities are equivalent. So it turns out that even though $\theta_{0:T}$ isn't SA $V|W$, drawing from $p(V|W,\theta_{0:T},y_{1:T})$ is equivalent to drawing from $p(V|W,\tilde{\psi}_{0:T},y_{1:T})$ so that we can replace step 2 this draw. Thus an equivalent version of the full componentwise interweaving algorithm with the $\theta$'s replacing both the $\tilde{\psi}$'s and the $\tilde{\gamma}$'s is
\begin{enumerate}
  \item Draw $\theta_{0:T}$ from $p(\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T})$.
  \item Draw $V^{(k+.5)}$ from $p(V|W^{(k)}, \theta_{0:T}, y_{1:T})$.
  \item Update $\psi_{0:T}$.
  \item Draw $V^{(k+1)}$ from $p(V|W^{(k)}, \psi_{0:T}, y_{1:T})$.
  \item Update $\theta_{0:T}$.
  \item Draw $W^{(k+.5)}$ from $p(W|V^{(k+1)}, theta_{0:T}, y_{1:T})$.
  \item Update $\gamma_{0:T}$.
  \item Draw $W^{(k+1)}$ from $p(W|V^{(k+1)}, \gamma_{0:T}, y_{1:T})$.
\end{enumerate}
Now step 1 can be performed directly using FFBS, which is nice. Note that there are several variations of this algorithm where we change the order of drawing $V$ and $W$, or the order of the conditional densities we draw $V$ from and/or $W$ from. \citeauthor{yu2011center} note, however, that these sorts of variations tend to not make much of a difference in terms of convergence --- at least relative to the decision of whether or not to interweave.

