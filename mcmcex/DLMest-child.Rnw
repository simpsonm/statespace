<<set-parent-DLMest, echo=FALSE, cache=FALSE>>=
set_parent('mcmcex.Rnw')
@


\section{Estimating the Model via Data Augmentation: Parameterization Issues}\label{sec:DLMest}
The usual way to estimate the model is via data augmentation (DA) using forward filtering backward sampling (FFBS), as in \citet{fruhwirth1994data} and \citet{carter1994gibbs}. The basic idea is to implement a Gibbs sampler with two blocks. The generic DA algorithm with parameter $\phi$, augmented data $\theta$, and data $y$ obtains the $k+1$'st state of the Markov chain, $\phi^{(k+1)}$, from the $k$'th state, $\phi^{(k)}$ as follows:
\begin{alg}\mbox{}\\[-\baselineskip]
  \begin{enumerate}\label{DAalg}
  \item Draw $\theta$ from $p(\theta|\phi^{(k)},y)$
  \item Draw $\phi^{(k+1)}$ from $p(\phi|\theta,y)$
  \end{enumerate}
\end{alg}
The first block samples the states conditional on the data and model parameters while the second block samples the parameters conditonal on the states and the data. We're calling this algorithm the ``state sampler.'' The FFBS step consists of running the Kalman filter to obtain a draw from $\theta_T|V_{1:T},W_{1:T},y_{1:T}$, then moving backward to obtain draws from $\theta_{t}|V_{1:T},W_{1:T},y_{1:T},\theta_{t+1:T}$ for $t=T-1, T-2, \cdots, 0$. \citet{fruhwirth1994data}, \citet{carter1994gibbs}, and \citet{petris2009dynamic} contain the details of this process. For the subset of DLMs we are considering, the algorithm cashes out like this:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{statealg}
  \item Draw $\theta_{0:T}$ from $p(\theta_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$
    using FFBS
  \item Draw $(V_{1:T}^{(k+1)},W_{1:T}^{(k+1)})$ from $p(V_{1:T},W_{1:T}|\theta_{0:T},y_{1:T})$:\\~\\

    $V_1$, $V_2$, $\cdots$, $V_T$ and $W_1$, $W_2$, $\cdots$, $W_T$ are conditionally independent given $(\theta_{0:T},y_{1:T})$, with distributions
    \begin{align*}
      V_t|\theta_{0:T},y_{1:T} &\sim IW(\Psi_{t} + v_tv_t',\eta_{t} + 1 ) &   W_t|\theta_{0:T},y_{1:T} &\sim IW(\Omega_{t} + w_tw_t',\delta_{t} + 1)
    \end{align*}
    where $v_t = y_t - F_t\theta_t$ and $w_t = \theta_t - G_t\theta_{t-1}$.
\end{enumerate}
\end{alg}
We can immediately see why the ``standard priors'' are standard -- they are conditionally conjugate for each parameter in question so that the full conditional distributions are all easy to sample from. The main problem with this algorithm is that computation time increases quickly with the length of the time series because the Kalman filter essentially requires drawing from $\theta_t|V_{1:T},W_{1:T},\theta_{0:t},y_{0:T}$ for $t=0,1,\cdots,T$, so the FFBS step represents $2T$ multivariate draws. A second problem is that in some regions of the parameter space, the Markov chain mixes poorly for some of the parameters. For example, in the univariate local level model and similar models it's known that if the time constant variance of the latent states, $W$, is too small, mixing will be poor for $W$ \cite{fruhwirth2004efficient}.

One well known method of improving mixing and convergence in MCMC samplers is reparameterizing the model. \citet{papaspiliopoulos2007general} is a good summary. Most of the work in some way focuses on what are called centered and noncentered parameterizations. In our general notation where $\phi$ is the parameter, $\theta$ is the DA and $y$ is the data, the parameterization $(\phi,\theta)$ is a {\it centered parameterization} (CP) if $p(y|\theta,\phi)=p(y|\theta)$. The parameterization is a {\it noncentered parameterization} (NCP) if $p(\theta|\phi)=p(\theta)$. When $(\phi,\theta)$ is a CP, $\theta$ is called a {\it centered augmentation} (CA) for $\phi$ and when $(\phi,\theta)$ is a NCP, $\theta$ is called a {\it noncentered augmentation} (NCA) for $\phi$. A centered augmentation is sometimes called a {\it sufficient augmentation} (SA) and a noncentered augmentation is sometimes called an {\it ancillary augmentation} (AA), e.g. in \citet{yu2011center}. Like \citeauthor{yu2011center}, we prefer the latter terminology because it immediately suggests the intuiton that a sufficient augmentation is like a sufficient statistic while an ancillary augmentation is like an ancillary statistic. 

The key reasoning behind the emphasis on SAs and AAs is that typically when the DA algorithm based on the SA has nice mixing and convergence properties the DA algorithm based on the AA has poor mixing and convergence propeties and vice versa. In other words, the two algorithms form a ``beauty and the beast'' pair. This property suggests that there might be some way to combine the two DA algorithms or the two underlying parameterizations in order to construct a sampler which has ``good enough'' properties all the time.  Some work focuses on using partially noncentered parameterizations that are a sort of bridge between the CP and NCP, e.g. \citeauthor{papaspiliopoulos2007general} for general hierarchical models and \citet{fruhwirth2004efficient} in the context of a partiuclar DLM --- a dynamic univarite regression with a stationary AR(1) coefficient. But this doesn't quite accomplish what we want because it still picks a single parameterization to use that may depend on the region of the parameter space the posterior conentrates most of its mass. The interweaving concept of \citet{yu2011center} does precisely what we want, however. The idea is pretty simple: suppose that $\phi$ denotes the parameter vector, $\theta$ denotes one augmented data vector, $\gamma$ denotes another augmented data vector, and $y$ denotes the data. Then an MCMC algorithm that {\it interweaves} between $\theta$ and $\gamma$ performs the following steps in a single iteration to obtain the $k+1$'st draw, $\phi^{(k+1)}$, from the $k$'th draw, $\phi^{(k)}$:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{inter}
\item Draw $\theta$ from $p(\theta|\phi^{(k)},y)$
\item Draw $\gamma^{(k+1)}$ from $p(\gamma|\theta,y)$
\item Draw $\phi^{(k+1)}$ from $p(\phi|\gamma^{(k+1)},y)$.
\end{enumerate}
\end{alg}
Notice that an additional step is added to algorithm \ref{DAalg}, and the final step now draws $\phi$ conditional on $\gamma$ instead of $\theta$. This is the intuition behind the name ``interweaving''---the draw of the second augmented data vector is weaved in between the draws of $\theta$ and $\phi$. This particular method of interweaving is called a {\it global} interweaving strategy (GIS) since interweaving occurs globally across the entire parameter vector. It's possible to define a {\it componentwise} interweaving strategy (CIS) that interweaves within specific steps of a Gibbs sampler as well. Step two of the GIS algorithm is typically accomplished by sampling $\phi|\theta,y$ and then $\gamma|\theta,\phi,y$. In addition, $\gamma$ and $\theta$ are often, but not always, one-to-one transformations of each other conditional on $(\phi,y)$, i.e. $\gamma = M(\theta;\phi,y)$. Where $M(.;\phi,y)$ is a one-to-one function. In this case, the algorithm becomes:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{inter2}
\item Draw $\theta$ from $p(\theta|\phi^{(k)},y)$
\item Draw $\phi$ from $p(\phi|\theta,y)$
\item Draw $\gamma$ from $p(\gamma|\theta,\phi,y)$
\item Draw $\phi^{(k+1)}$ from $p(\phi|\gamma,y)$
\end{enumerate}
\end{alg}
When $\gamma$ is not a one-to-one transformation of $\theta$, step 4 is an update $\gamma=M(\theta;\phi,y)$. The GIS algorithm is directly comparable to an {\it alternating} algorithm. Given the same two DAs, $\theta$ and $\gamma$, and parameter vector $\phi$, the alternating algorithm for sampling from $p(\phi|y)$ is as follows:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{alt}
\item Draw $\theta$ from $p(\theta|\phi^{(k)},y)$
\item Draw $\phi$ from $p(\phi|\theta,y)$
\item Draw $\gamma$ from $p(\gamma|\phi,y)$
\item Draw $\phi^{(k+1)}$ from $p(\phi|\gamma,y)$
\end{enumerate}
\end{alg}
The key difference between this algorithm and algorithm \ref{inter2} is in step 3: instead of drawing from $p(\gamma|\theta,\phi,y)$, the alternating algorithm draws from $p(\gamma|\phi,y)$. In other words it alternates between two data augmentation algorithms in a single iteration. The interweaving algorithm, on the other hand, connects or ``weaves'' the two separate iterations together in step 3 by drawing $\gamma$ conditonal on $\theta$ in addition to $\phi$ and $y$.

\citeauthor{yu2011center} call a GIS approach where one of the DAs is a SA and the other is an AA an {\it ancillary sufficient interweaving strategy}, or an ASIS. They show that the GIS algorithm has a geometric rate of convergence no worse than the worst of the two underlying algorithms and in some cases better than the the corresponding alternating algorithm. In models with a ``nice'' prior on $\phi$ in some sense, they also show that the ASIS algorithm is the same as the optimal PX-DA algorithm of \citet{meng1999seeking}, \citet{liu1999parameter}, \citet{van2001art} and \citet{hobert2008theoretical}. Their results suggest that ASIS is a promising approach to improve the speed of MCMC in a variety of models no matter what region of the parameter space the posterior is concentrated. To gain some intuition about why this is so, recall that a typical problem with slow MCMC is that there is high autocorrelation in the Markov chain for $\phi$, $\{\phi^{(k)}\}_{k=1}^K$, leading to imprecise estimates of $\mathrm{E}[f(\phi)]$ for some function $f$. Our ultimate goal here is to reduce this dependence. In the usual DA algorithm, e.g. algorithm \ref{DAalg}, when $\phi$ and $\theta$ are highly dependent in the joint posterior the draws from $p(\theta|\phi,y)$ and then from $p(\phi|\theta,y)$ won't move the chain much, resulting in high autocorrelation in the chain. Interweaving helps break this autocorrelation in two ways. First, by inserting the extra step, e.g. steps 2 and 3 together in \ref{inter2}, the chain gets an additional chance to move in a single iteration thereby weaking the autocorrelation. Second, when one of $\theta$ and $\gamma$ is a ``beauty'' and the other is a ``beast'', as is often the case when they form a SA-AA pair, one of steps 2 and 4 in algorithm \ref{inter2} will significantly move the chain even if the other step will not. This intuition suggests that the key isn't so much that $\theta$ and $\gamma$ form a SA-AA pair as that they form a beauty and the beast pair. It just so happens that SA-AA pairs are often great at accomplishing this.

\subsection{The Scaled Disturbances}

The next step is to apply the ideas of interweaving to sampling from the posterior of the dynamic linear model. \citeauthor{papaspiliopoulos2007general} note that typically the usual parameterization results in a SA for the parameter $\phi$. All that's necessary for an ASIS algorithm, then, is to construct an AA for $\phi$. We immediately run into a problem because the standard DA for a DLM is the latent states $\theta_{0:T}$. From equations \eqref{dlmobseq} and \eqref{dlmsyseq} we see that $V_{1:T}$ is in the observation equation so that $\theta_{0:T}$ isn't a SA for $(V_{1:T},W_{1:T})$ while $W_{1:T}$ is in the system equation so that $\theta_{0:T}$ isn't an AA for $(V_{1:T},W_{1:T})$ either. In order to find a SA we need to somehow move $V_{1:T}$ from the observation equation \eqref{dlmobseq} to the system equation \eqref{dlmsyseq} while leaving $W_{1:T}$ in the system equation. Alternatively, we could find an AA by somehow moving $W_{1:T}$ from the system equation to the observation equation while leaving $V_{1:T}$ in the observation equation. A naive thing to try is to condition on the disturbances instead of the states and see if the disturbances for a SA or an AA for $(V_{1:T},W_{1:T})$. The disturbances $w_{0:T}$ are defined by $w_t = \theta_t - G_t\theta_{t-1}$ for $t=0,1,...,T$ and we define $\theta_{-1}=0$ so that $w_0=\theta_0$. However the DA algorithm based on the $w_t$'s is identical to the algorithm based on the $\theta_t$'s. This is because $w_{0:T}$ is a one-to-one function of $\theta_{0:T}$ that doesn't depend on $V_{1:T}$ or $W_{1:T}$, the conditional distributions $p(V_{1:T},W_{1:T}|\theta_{0:T},y_{1:T})$ and $p(V_{1:T},W_{1:T}|w_{0:T},y_{1:T})$ are identical.

\citeauthor{papaspiliopoulos2007general} suggest that in order to obtain an ancillary augmentation for a variance parameter, we must scale the sufficient agumentation by the square root of that parameter. Based on this intuition, note that if we hold $V_{1:T}$ constant then $\theta_{0:T}$ is a SA for $W_{1:T}$ from the observation and system equations, \eqref{dlmobseq} and \eqref{dlmsyseq}, i.e. we say $\theta_{0:T}$ is a SA for $W_{1:T}$ given $V_{1:T}$, or for $W_{1:T}|V_{1:T}$. Similarly $\theta_{0:T}$ is an AA for $V_{1:T}|W_{1:T}$. This suggests that if we scale $\theta_{t}$ by $W_{t}$ for all $t$ appropriately we'll have an ancillary augmentation for $V_{1:T}$ and $W_{1:T}$ jointly. The same intuition suggests scaling $w_{t}=\theta_{t}-G_t\theta_{t-1}$ by $W_{t}$ for all $t$ appropriately in order to find an ancillary augmentation for $(V_{1:T},W_{1:T})$. We'll work with the latter case though, again these two ideas are ultimately equivalent since the resulting DAs are one-to-one transformations of each other. 

To define the scaled disturbances in the general DLM, let $L_t$ denote the Cholesky decomposition of $W_t$, i.e. $L_t'L_t =W_t$, for $t=1,2,\cdots,T$. Then we'll define the scaled disturbances $\gamma_{0:T}$ by $\gamma_0=\theta_0$ and $\gamma_t = L_t^{-1}(\theta_t-G_t\theta_{t-1})$ for $t=1,2,\cdots,T$. We can confirm our intuition that the scaled disturbances are an AA for $V_{1:T}$ and $W_{1:T}$ jointly. The reverse transformation is defined recursively by $\theta_0=\gamma_0$ and $\theta_t=L_t\gamma_t + G_t\theta_{t-1}$ for $t=1,2,\cdots,T$. Then the Jacobian is block lower triangular with the identity matrix and the $L_t$'s along the diagonal blocks, so $|J| = \prod_{t=1}^T|L_t|=\prod_{t=1}^T|W_t|^{1/2}$. Then from \eqref{dlmjoint} we can write the full joint distribution of $(V_{1:T},W_{1:T},\gamma_{0:T},y_{1:T})$ as
 \begin{align}
  p(&V_{1:T},W_{1:T},\gamma_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\gamma_0-m_0)'C_0^{-1}(\gamma_0-m_0)\right] \nonumber\\
  &\times \prod_{t=1}^T |W_t|^{-(\delta_t + p + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Omega_tW_t^{-1}\right)\right] \exp\left[-\frac{1}{2}\gamma_t'\gamma_t\right] |V_t|^{-(\eta_t + k + 3)/2} \nonumber\\
  &\times \exp\left[-\frac{1}{2}\left(tr\left(\Psi_tV_t^{-1}\right) + \sum_{t=1}^T\left[y_t-F_t\theta_t(\gamma_{0:T},W_{1:T})\right]'V^{-1}\left[y_t-F_t\theta_t(\gamma_{0:T},W_{1:T})\right]\right)\right] \label{dlmdistjoint}
 \end{align}
where $\theta_t(\gamma_{0:T},W_{1:T})$ denotes the recursive back transformation defined by the scaled disturbances.

So ultimately under the scaled disturbance parameterization we can write the model as
\begin{align}
  y_t|\gamma_{0:T},V_{1:T},W_{1:T} & \stackrel{ind}{\sim} N\left(F_t\theta_t(\gamma_{0:T},W_{1:T}), V_t\right)\nonumber\\
  \gamma_t & \stackrel{iid}{\sim}N(0,I_p) \label{dlmdistmodel}
\end{align}
for $t=1,2,\cdots,T$ where $I_p$ is the $p\times p$ identity matrix. Neither $V_{1:T}$ nor $W_{1:T}$ are in the system equation, so the scaled disturbances are an AA for $(V_{1:T},W_{1:T})$. This parameterization is well known, e.g. \citet{fruhwirth2004efficient} use it in a dynamic regression model with stationary regression coefficient. 

The DA algorithm based on $\gamma_{0:T}$ is as follows:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{distalg}
  \item Draw $\gamma_{0:T}$ from $p(\gamma_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$
  \item Draw $(V_{1:T}^{(k+1)},W_{1:T}^{(k+1)})$ from $p(V_{1:T},W_{1:T}|\gamma_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
Step 1 can be accomplished directly with the disturbance smoother of \citet{koopman1993disturbance} or indirectly by using FFBS to draw the states and then transform them to the scaled disturbances. Step 2 ends up being complicated because the joint conditional posterior of $V$ and $W$ isn't a known density. We'll go through an example of this when both $y_t$ and $\theta_t$ are scalars later.

We previously mentioned that the intuition behind the scaled disturbances also suggests trying the scaled states, i.e. $\theta^s_{0:T}$ where $\theta^s_0=\theta_0$ and for $t=1,2,\cdots,T$, $\theta^s_t=L_t^{-1}\theta_t$u. Note that $\theta^s_{0:T}$ and $\gamma_{0:T}$ are completely determined by each other independently of $V_{1:T}$ and $W_{1:T}$, which suggests the conditional distribution of $(V_{1:T},W_{1:T})$ is unchanged. This intuition is dangerously close to running right into the Borel-Kolmogorov paradox, but in this case there is no issue since the determinant of the Jacobian will be the same whether we are transforming to the sclaed disturbances or the scaled states.

\subsection{The Scaled Errors}
The scaled disturbances immediately suggest another potential AA that seems like it should be analogous --- the scaled observation errors, or more succinctly the scaled errors. What we are referring to is $v_t=y_t - F_t\theta_t$ appropriately scaled by $V_t$ in the general DLM. Now let $K_t$ denote the Cholesky decomposition of $V_t$, that is $K_t'K_t=V_t$. Then we can define the scaled errors as $\psi_0 = \theta_0$ and $\psi_t = K_t^{-1}(y_t - F_t\theta_t)$ for $t=1,2,\cdots,T$. This is a bit strange since in general $dim(\psi_0)\neq dim(\psi_t)$ for $t=1,2,\cdots,T$. Ideally we might like an ``$F_0$'' so that we can set $\psi_0=F_0\theta_0$ in order for $\psi_0$ to have the same dimension as $\psi_1$. However, in general there is no $F_0$. In some DLMs $F_t$ is constant with respect to $t$ so that we could set $F_0=F$, but in dynamic regression for example, there is no natural ``$F_0$''.

This isn't where the difficulties end either. With this definition of $\psi_{0:T}$, it isn't straightforward to determine $p(\psi_{0:T}|V_{1:T},W_{1:T})$, i.e. to write down the model in terms of $\psi_{0:T}$ instead of $\theta_{0:T}$. When $F_t$ is $k\times k$ (so that $dim(y_t)=k=p=dim(\theta_t)$) and is invertible for $t=1,2,\cdots,T$, $\psi_{0:T}$ is a one-to-one transformation of $\theta_{0:T}$ and the problem is easier. Then $\theta_t = F_t^{-1}(y_t - K_t\psi_t)$ for $t=1,2,\cdots,T$ while $\theta_0=\psi_0$. The Jacobian of this transformation is block diagonal with a single copy of the identity matrix and the $F_t^{-1}K_t$'s along the diagonal, so $|J|=\prod_{t=1}^T|F_t|^{-1}|V_t|^{-1/2}$. Then from \eqref{dlmjoint} we can write the joint distribution of $(V_{1:T}, W_{1:T}, \psi_{0:T}, y_{1:T})$ as
\begin{align}
    p(&V_{1:T},W_{1:T},\psi_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\psi_0-m_0)'C_0^{-1}(\psi_0-m_0)\right] \nonumber\\
  &\times \prod_{t=1}^T   |V_t|^{-(\eta_t + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Psi_tV_t^{-1}\right)\right] \exp\left[-\frac{1}{2}\psi_t'\psi_t\right] \nonumber\\
   & \times |W_t|^{-(\delta_t + k + 3)/2}\exp\left[-\frac{1}{2}\left(tr\left(\Omega_tW_t^{-1}\right) + (y_t - \mu_t)'(F_tW_tF_t')^{-1}(y_t-\mu_t)\right)\right]\label{dlmerrorjoint}
\end{align}
where we define $\mu_t =K_t\psi_t + F_tG_tF_{t-1}(y_{t-1} - K_{t-1}\psi_{t-1})$, $y_0=0$, $K_0=I_k$, and $F_0=I_k$ where $I_k$ is the $k\times k$ identity matrix. The $|F_t|^{-1}$'s have been absorbed into the normalizing constant, but note that if the $F_t$'s depended on some unknown parameter then we couldn't do this. Now we can write the model in terms of the scaled error parameterization:
\begin{align*}
  y_t|V_{1:T},W_{1:T},\psi_{0:T},y_{1:t-1} &\sim N(\mu_t, F_t'W_tF_t)\\
  \psi_t & \stackrel{iid}{\sim} N(0,I_k)
\end{align*}
for $t=1,2,\cdots,T$. Now we see immediately that the scaled errors, $\psi_{0:T}$, are also an AA for $(V_{1:T},W_{1:T})$ since neither $V$ nor $W$ are in the system equation of this model, though note that both $V_{1:T}$ and $W_{1:T}$ are in the observation equation, so $\psi_{0:T}$ is not a SA for $(V_{1:T},W_{1:T})$ or for either one given the other.

The DA algorithm based on $\psi_{0:T}$ is:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{erroralg}
  \item Draw $\psi_{0:T}$ from $p(\psi_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$
  \item Draw $(V_{1:T}^{(k+1)},W_{1:T}^{(k+1)})$ from $p(V_{1:T},W_{1:T}|\psi_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
Once again step 1 can be accomplished directly with \citeauthor{koopman1993disturbance}'s disturbance smoother or indirectly using FFBS. Step 2 is also once again complicated since the joint conditional posterior of $V_{1:T}$ and $W_{1:T}$ isn't a known density. 

\subsection{Conditionally conjugate priors and the choice of DA}
After choosing the priors for $\theta_0$, $V_{1:T}$ and $W_{1:T}$ we motivated the choice by appealing to conditional conjugacy and thus computation. If this is our main concern for choosing a prior,  it's worth asking what the conditional conjugate priors are under the scaled disturbances and the scaled errors. We'll look closely at the scaled disturbances, but the scaled errors are analogous. Based on \eqref{dlmdistmodel} we can write the augmented data likelihood as
\begin{align*}
  p(y_{1:T}|\gamma_{0:T}, V_{1:T}, W_{1:T}) \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\gamma_t'\gamma_t\right]\prod_{t=1}^T|V_t|^{-1/2}\exp\left[-\frac{1}{2}\left(y_t - F_t\theta_t(\gamma_{0:T},W_{1:T})\right)'V_t^{-1}\left(y_t - F_t\theta_t(\gamma_{0:T},W_{1:T})\right)\right].
\end{align*}
Immediately we see that the conjugate prior for $V_t$ is inverse Wishart, so no change on that front. For $W_t$ on the other hand, it's unclear until we unpack $\theta_t(\gamma_{0:T},W_{1:T})$. Recall that in our definition of the scaled disturbances for $t=1,2,\cdots,T$, $\gamma_t = L_t^{-1}w_t = L_t^{-1}(\theta_t - G_t\theta_{t-1})$ where $L_t'L_t = W_t$ while $\gamma_0=\theta_0$. The reverse transformation is thus the recursion $\theta_t = L_t\gamma_t + G_t\theta_{t-1}$ for $t=1,2,\cdots,T$. This implies that for $t=0,1,\cdots,T$
\begin{align*}
  \theta_t = \sum_{s=0}^t\left(\prod_{r=s+1}^tG_r\right)L_s\gamma_s
\end{align*}
where we define $L_0=I_k$, the $k\times k$ identity matrix, and for $s+1>t$, $\prod_{r=s+1}^tG_r = I_k$. Now recall that $K_t'K_t = V_t$ and let $H_s^t = \prod_{r=s+1}^tG_r$. This allows us to write the full conditional distribution of $W_{1:T}$ as
\begin{align*}
  p(W_{1:T}&|\gamma_{0:T},\cdots)  \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(y_t - F_t\sum_{s=0}^tH_s^tL_s\gamma_s\right)'(K_t^{-1})'K_t^{-1}\left(y_t - F_t\sum_{s=0}^tH_s^tL_s\gamma_s\right)\right]p(W_{1:T})
\end{align*}
If the $W_t$'s are independent in the prior, then they are independent in their conditional posterior with density
\begin{align*}
  p(W_t & | \gamma_{0:T},\cdots) \propto \exp\left[-\frac{1}{2}\left((A_tL_t\gamma_t)'(A_tL_t\gamma_t) - 2B_tL_t\gamma_t\right)\right]p(W_t)
\end{align*}
where $A_t$ and $B_t$ are matrices of constants that are implicitly defined from the previous equation. 

This doesn't look like it has any conjugate form for $W_t$, but it looks a lot like a normal kernel for $L_t$, the Cholesky decomposition of $W_t$. Indeed, if we follow the approach of \cite{fruhwirth2008bayesian} and vectorize $L_t$ by stacking the nonzero elements of each column on top of each other, we can get a multivariate normal distribution. Specifically, let $L_t^*=Vec(L_t)$. Then the full conditional posterior distribution of $L_t^*$ is
\begin{align*}
  p(L_t^* & | \gamma_{0:T},\cdots) \propto \exp\left[-\frac{1}{2}\left((A^*_tL^*_t)'(A^*_tL^*_t) - 2B^*_tL^*_t\right)\right]p(L_t^*)\\
  & \propto \exp\left[-\frac{1}{2}(L_t^* - \mu_t^*)'(\Sigma_t^*)^{-1}(L_t^* - \mu_t^*)\right]p(L_t^*)
\end{align*}
where $A_t^*$ and $B_t^*$ are matrices of constants with respect to $L_t^*$, $\Sigma_t^* = ((A_t^*)'A_t^*)^{-1}$ and $\mu_t^* = (\Sigma_t^*)^{-1}B_t^*$. So the conjugate prior on $L_t^*$ is a multivariate normal distribution. This seems a strange since we expect the diagonal elements of $L_t$ to be positive since they are standard deviations, but this is no problem as long as we view this prior as a clever trick for defining a prior on $W_t=L_t'L_t = (-L_t)'(-L_t)$ so the sign doesn't matter. Strictly speaking here, we're subtly changed the definition of $L_t$ to $\pm Chol(W_t)$, the {\it signed} Cholesky decomposition of $W_t$, and thus subtly changed the defintion of the $\gamma_t$'s to take into account the sign of $L_t$. \cite{fruhwirth2008bayesian}, \cite{fruhwirth2011bayesian} and {\it CITE THE DYNAMIC PAPER SYLVIA IS WORKING ON WITH ANGELA} use this approach to choosing priors for the system (or hierarchical) variance when working with the scaled disturbances in dynamic and non-dynamic models. Typically they choose mean zero normal priors.

This is a bit strange though. We have two sets of covariance matrices, $W_{1:T}$ and $V_{1:T}$, and we want to put a different class of priors on each set. We can put the same sort of normal prior on $K_t^*$, the vectorized Cholesky decomposition of $V_t$. In the univariate case the conditional posterior of $V_t$ will come out to be a generalized inverse gaussian distribution which is a bit complicated but not awful to draw from. There's mild tension here as well --  depending on how we choose to write down the model we end up with a different class of prior distributions for at least $W_{1:T}$. Now the reason for this difference is ultimately computation --- it is known that sometimes using the scaled disturbances improves mixing in the Markov chain --- but ideally computational concerns should not have an effect on inference. It would be nice to unite these priors two priors under a common class without sacrificing their respective computaitonal advantages under the relevant data augmentations. 

{\it INSERT SOME EXPLANATION FOR WHY A NORMAL PRIOR ON THE NONZERO ELEMENTS OF THE CHOLESKY FACTORIZATION OF A COVARIANCE MATRIX YIELDS A WISHART PRIOR ON THE MATRIX ITSELF}

\section{Mixing Wisharts and inverse Wisharts}

The main goal of this section is to make precise and prove the statement ``an inverse Wishart mixture of Wisharts is the same as a Wishart mixture of inverse Wisharts'' as well as describe some of the properties of the resulting class of distributions. We will work with a slightly more general class of distribution in order to somewhat simplify the mathematics -- the (inverse) matrix gamma distribution. Let $X$ be a $p\times p$ nonnegative definite random matrix win a matrix gamma distribution given shape and scale parameters $\alpha > (p - 1)/2$ and $\beta >0$, and scale matrix parameter $\Sigma$, a $p\times p$ positive definite matrix, i.e. $X\sim MG_p(\alpha,\beta,\Sigma)$. Then the density of $X$ is
\begin{align*}
  p(X) = & \frac{|\Sigma|^{-\alpha}}{\beta^{p\alpha}\Gamma_p(\alpha)}|X|^{\alpha - (p+1)/2}\exp\left[\tr\left(-\frac{1}{\beta}\Sigma^{-1}X\right)\right]
\end{align*}
Where $|.|$ denotes the determinant operator, $\tr(.)$ the trace operator, and $\Gamma_p(.)$ is the multivariate gamma function.  $Y=X^{-1}$ has an inverse matrix gamma distribution, $Y\sim IMG_p(\alpha,\beta,\Psi)$ with $\Psi=\Sigma^{-1}$, and the density of Y is
\begin{align*}
  p(Y) = & \frac{|\Psi|^{\alpha}}{\beta^{p\alpha}\Gamma_p(\alpha)}|Y|^{-\alpha - (p+1)/2}\exp\left[\tr\left(-\frac{1}{\beta}\Psi Y^{-1}\right)\right].
\end{align*}
The (inverse) Wishart distribution is a special case with $\alpha = n/2$ and $\beta=0$, where $n$ is the degrees of freedom parameter of the (inverse) Wishart distribution. Now we can state the two main theorems of this section.

\begin{thm}\label{thm:MGIMG}
  Suppose $X|Y \sim MG_p(\alpha_1,\beta_1,Y)$ and $Y\sim IMG_p(\alpha_2,\alpha_2,\Sigma)$ Then the marginal distribution of $X$ is
  \begin{align*}
    p(X) =  \frac{\left|\frac{\beta_1}{\beta_2}\Sigma\right|^{\alpha_2}}{\B_p(\alpha_1,\alpha_2)} |X|^{\alpha_1 - (p+1)/2}  \left|X + \frac{\beta_1}{\beta_2}\Sigma\right|^{-(\alpha_1 + \alpha_2)}
  \end{align*}
\end{thm}

\begin{thm}\label{thm:IMGMG}
  Suppose $X|Y \sim IMG_p(\alpha_2,\beta_2,Y)$ and $Y\sim MG_p(\alpha_1,\beta_1,\Sigma)$ Then the marginal distribution of $X$ is
  \begin{align*}
    p(X) =  \frac{\left|\frac{\beta_1}{\beta_2}\Sigma\right|^{\alpha_2}}{\B_p(\alpha_1,\alpha_2)} |X|^{\alpha_1 - (p+1)/2}  \left|X + \frac{\beta_1}{\beta_2}\Sigma\right|^{-(\alpha_1 + \alpha_2)}
  \end{align*}
\end{thm}
where $\B_p(\alpha_1, \alpha_2)$ is the multivariate beta function with the property $\B_p(\alpha_1, \alpha_2) = \Gamma_p(\alpha_1)\Gamma_p(\alpha_2)/\Gamma_p(\alpha_1 + \alpha_2)$.

The proofs of these theorems are actually quite simple. We'll start with Theorem \ref{thm:MGIMG}. The joint distribution of $(X,Y)$ can be written as
\begin{align*}
  p(X,Y) = &  \frac{|\Sigma|^{\alpha_2}\Gamma_p(\alpha_1 + \alpha_2)}{\left(\beta_1^{\alpha_1} \beta_2^{\alpha_2}\right)^p\Gamma_p(\alpha_1)\Gamma_p(\alpha_2)}\left|\frac{1}{\beta_1}X + \frac{1}{\beta_2}\Sigma\right|^{-(\alpha_1 + \alpha_2)}|X|^{\alpha_1 - (p+1)/2} \\
  &\times \frac{\left|\frac{1}{\beta_1}X + \frac{1}{\beta_2}\Sigma\right|^{\alpha_1 + \alpha_2}}{\Gamma_p(\alpha_1 + \alpha_2)}|Y|^{-(\alpha_1 + \alpha_2) - (p+1)/2}\exp\left[\tr\left(-\left(\frac{1}{\beta_1}X + \frac{1}{\beta_2}\Sigma \right) Y^{-1}\right)\right].
\end{align*}
The second line is the density of an inverse matrix gamma distribution, so marginalizing out $Y$ and rearranging a bit we get
\begin{align*}
p(X) =  \frac{\left|\frac{\beta_1}{\beta_2}\Sigma\right|^{\alpha_1}}{\B_p(\alpha_1,\alpha_2)} |X|^{\alpha_1 - (p+1)/2}  \left|X + \frac{\beta_1}{\beta_2}\Sigma\right|^{-(\alpha_1 + \alpha_2)}
\end{align*}
which completes the proof.

To prove Theorem \ref{thm:IMGMG} we'll do something similar but ignore the normalizing constant. In this case the joint distribution of $X$ and $Y$ can be written as
\begin{align*}
  P(X,Y) \propto & |X|^{-\alpha_2 -(p+1)/2}|Y|^{\alpha_1 - (p+1)/2}\exp\left[\tr\left(-\left(\frac{1}{\beta_2}X^{-1} + \frac{1}{\beta_1}\Sigma^{-1}\right)Y\right)\right].
\end{align*}
Now we marginalize out $Y$ to get the desired result:
\begin{align*}
  p(X) \propto & |X|^{-\alpha_2 - (p+1)/2}\left|\frac{1}{\beta_2}X^{-1} + \frac{1}{\beta_1}\Sigma^{-1}\right|^{-(\alpha_1 + \alpha_2)} \\
  \propto & |X|^{\alpha_1 - (p+1)/2}\left|\frac{\beta_1}{\beta_2}\Sigma + X\right|^{-(\alpha_1 + \alpha_2)} \\
\end{align*}
where the last line comes from $|X\beta_1\Sigma|^{\alpha_1 + \alpha_2}|X\beta_1\Sigma|^{-(\alpha_1 + \alpha_2)}=1$.

When $\alpha_1 = 2n_1$, $\alpha_2=2n_2$ and $\beta_1=\beta_2$ (or both $beta$'s are absorbed into $\Sigma$), $X$ has the multivariate (or matrix-variate) F distribution $(X\sim F_p(n_1, n_2, \Sigma)$ with density
\begin{align*}
  p(X) \propto |X|^{\frac{n_1 - (p+1)}{2}}  \left|I_p + \Sigma^{-1}X\right|^{-(n_1 + n_2)/2}
\end{align*}
where $I_p$ is the $p\times p$ identity matrix. We'll focus on the multivariate F distribution.

\subsection{Properties of the multivariate F distribution}
It is commonly observed that the multivariate F distribution can be seen as a sort of generalization of the Wishart distribution. The following corollary illustrates this.
 
\begin{cor}\label{prop:multt}
Suppose for $i=1,2,\cdots,n_1$, $x_i$ is a $p-$dimensional random vector with 
\[
x_i \stackrel{iid}{\sim} T_p(n_2 - p + 1, 0, \Sigma/(n_2 - p + 1))
\]
where $T_p(\nu,\mu,\Omega)$ is the multivariate $T$ distribution with degrees of freedom $\nu$, $p\times 1$ location parameter $\mu$ and $p\times p$ scale matrix $\Omega$. Suppose $n_1,n_2>p-1$ and let $S=\sum_{i=1}^{n_1}x_ix_i'$. Then $S\sim F_p(n_1, n_2, \Sigma)$
\end{cor}

To see this, recall that if $x|Y \sim N_p(0,Y)$ and $Y\sim IW_p\left(\nu, \Sigma\right)$ then marginally $x \sim T_p(\nu - p + 1), 0, \Sigma/(\nu - p + 1)$. So we can write the distribution of the $x_i$'s as independent inverse Wishart mixtures of independent normals, i.e. $x_i|Y_1,Y_2,\cdots,Y_{n_1} \stackrel{ind}{\sim} N_p(0,Y_i)$ and $Y_i\stackrel{iid}{\sim} IW_p(n_2, \Sigma)$. But conditional on $Y$, we know $S\sim W_p(n_1, Y)$. So by Theorem \ref{thm:MGIMG} $S\sim F_p(n_1, n_2, \Sigma)$.

{\it IS THE CHOLESKY DECOMPOSITION HALF-T UNDER SOME CONDITIONS? LOOK INTO MATRIX DERIVATIVES TO FIND OUT}

{\it LOOK INTO THE MARGINAL DISTRIBUTIONS OF THE STANDARD DEVIATIONS AND THE CORRELATIONS}

{\it RELATE TO THE HUANG ET. AL. PAPER}

{\it LOOK AT WHAT HAPPENS WHEN EITHER $n_1$ OR $n_2$ GO TO $\infty$. WHEN $n_2\to\infty$ WE GET THE WISHART. SUSPECT WHEN $n_2\to\infty$ WE GET THE INVERSE WISHART}

\section{Interweaving in the DLM: Global and Componentwise}\label{sec:dlminter}
We now have three DAs for the generic DLM with known $F_t$'s and $G_t$'s. For simplicity we'll assume that $dim(y_t)=dim(\theta_t)$ and $F_t$ invertible for $t=1,2,\cdots,T$ so that the scaled errors are easy to work with. The three DAs are the states, $\theta_{0:T}$, the scaled disturbances $\gamma_{0:T}$, and the scaled errors $\psi_{0:T}$. This allows us to construct four separate GIS algorithms based on algorithm \ref{inter2}: three algorithms that interweave between any two of $\theta_{0:T}$, $\gamma_{0:T}$, and $\psi_{0:T}$, the state-dist, state-error, and dist-error interweaving algorithms, and one algorithm that interweaves between all three, the triple interweaving algorithm. Strictly speaking, the order in which we sample the DAs in the algorithm does matter, but \citeauthor{yu2011center} note that this tends not to make much difference. So while we actually have twelve separate GIS samplers (two of each GIS sampler depending on two DAs, and six GIS samplers depending on all three), effectively we only have four. For example, algorithm \ref{sdintalg} is the state-dist GIS algorithm:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{sdintalg}
\item Draw $\theta_{0:T}$ from $p(\theta_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$
\item Draw $(V^{(k+0.5)}_{1:T},W^{(k+0.5)}_{1:T})$ from $p(V_{1:T},W_{1:T}|\theta_{0:T},y_{1:T})$
\item Update $\gamma_{0:T}^{(k+1)}$ from $\gamma_0=\theta_0$ and $\gamma_t = L^{(k+0.5)}_t(\theta_t - G_t\theta_{t-1})$ for $t=1,2,\cdots,T$
\item Draw $(V_{1:T}^{(k+1)},W_{1:T}^{(k+1)})$ from $p(V_{1:T},W_{1:T}|\gamma_{0:T},y_{1:T})$
\end{enumerate}
\end{alg}
where again $L^{(k+0.5)}_t$ is the Cholesky decomposition of $(W^{(k+0.5)}_t)^{-1}$, i.e. $(L^{(k+0.5)}_t)'L^{(k+0.5)}_t=(W^{(k+0.5)}_t)^{-1}$. Steps 1 and 2 are the same as steps 1 and 2 in algorithm \ref{statealg}, and step 4 is the same as step 2 of algorithm \ref{distalg}. The triple interweaving algorithm is the same as algorithm \ref{sdintalg} except it adds two more steps at the end: an update of $\psi_{0:T}$ from $\gamma_{0:T}$ and the draw of $(V_{1:T},W_{1:T})$ in step 4, and then a draw from $(V_{1:T},W_{1:T}|\psi_{0:T},y_{1:T})$. In practice we may want to break up step 4 into two steps if it's easier to draw from the full conditionals of $V_{1:T}$ and $W_{1:T}$ rather than drawing them jointly. Algorithm \ref{sdintalg2} below is exactly this.
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{sdintalg2}
\item Draw $\theta_{0:T}$ from $p(\theta_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$
\item Draw $(V_{1:T},W_{1:T})$ from $p(V_{1:T},W_{1:T}|\theta_{0:T},y_{1:T})$
\item Update $\gamma_{0:T}^{(k+1)}$ from $\gamma_0=\theta_0$ and $\gamma_t = L_t^{-1}(\theta_t - G_t\theta_{t-1})$ for $t=1,2,\cdots,T$
\item Draw $V_{1:T}^{(k+1)}$ from $p(V_{1:T}|W_{1:T},\gamma_{0:T},y_{1:T})$
\item Draw $W_{1:T}^{(k+1)}$ from $p(W_{1:T}|V^{(k+1)}_{1:T},\gamma_{0:T},y_{1:T})$
\end{enumerate}
\end{alg}
Step 4 actually draws $V_{1:T}$ from the same density as in step 2, but only the last of the two draws is used for anything in the algorithm. As a result, we can either draw only $W_{1:T}$ in step 2 or step 4 can be ommitted. 

None of these GIS algorithms are ASIS algorithms --- none of the DAs are a SA for $(V_{1:T},W_{1:T})$. The states, $\theta_{0:T}$, are a SA for $W_{1:T}|V_{1:T}$ though, so this motivates a CIS algorithm. A partial CIS algorithm is immediate:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{pcisalg}
\item Draw $\theta_{0:T}$ from $p(\theta_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$
\item Draw $V_{1:T}^{(k+1)}$ from $p(V_{1:T}|W_{1:T}^{(k)},\theta_{0:T},y_{1:T})$
\item Draw $W^{(k+0.5)}_{1:T}$ from $p(W_{1:T}|V_{1:T}^{(k+1)},\theta_{0:T},y_{1:T})$
\item Update $\gamma_{1:T}^{(k+1)}$ from $\gamma_0=\theta_0$ and $\gamma_t = L^{(k+0.5)}_t(\theta_t - G_t\theta_{t-1})$ for $t=1,2,\cdots,T$
\item Draw $W_{1:T}^{(k+1)}$ from $p(W_{1:T}|V^{(k+1)},\gamma_{0:T},y_{1:T})$
\end{enumerate}
\end{alg}
This algorithm is actually the same as a version of the state-dist interweaving algorithm, specifically algorithm \ref{sdintalg2}. So we can construct a partial CIS algorithm, but it's actually the exact same algorithm as a GIS algorithm.

With a little more work, we can also construct a full CIS algorithm that also turns out to be the same as another GIS algorithm. Recall that $\gamma_t = L_t^{-1}(\theta_t - G_t\theta_{t-1})$  and $\psi_t = K_t^{-1}(y_t - \theta_t)$ for $t=1,2,\cdots,T$ where $L_t'L_t = W_t$ and $K_t'K_t = V_t$. Now define $\tilde{\gamma}_t=K_t^{-1}(\theta_t - G_t\theta_{t-1})$ and $\tilde{\psi}_t=L_t^{-1}(y_t - \theta_t)$ for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\tilde{\gamma}_0=\theta_0$. In other words, the ``tilde'' versions of the scaled disturbances and the scaled errors are scaled by the ``wrong'' Cholesky decomposition. Now we'll show that $\gamma_{0:T}$ and $\tilde{\gamma}_{0:T}$ are an AA-SA pair for $W_{1:T}|V_{1:T}$ while $\psi_{0:T}$ and $\tilde{psi}_{0:T}$ are an AA-SA pair for $V_{1:T}|W_{1:T}$. We've already shown that both $\psi_{0:T}$ and $\gamma_{0:T}$ are AAs for $(V_{1:T},W_{1:T})$, so we just need to show that $\tilde{gamma}_{0:T}$ is a SA for $W_{1:T}|V_{1:T}$ and that $\tilde{\psi}_{0:T}$ is a SA for $V_{1:T}|W_{1:T}$. 

First consider $\tilde{\gamma}_{0:T}$. If we define $L_0=K_0=I_k$ where $I_k$ is the $k\times k$ identity matrix, then $\tilde{\gamma}_t = K_t^{-1}L_t\gamma_t$ for $t=0,1,2,\cdots,T$. The reverse transformation is then $\gamma_t = L_t^{-1}K_t\tilde{\gamma}_t$. The Jacobian is then block diagonal with $L_t^{-1}K_t$'s along the diagonal. Thus $|J|=\prod_{t=0}^T|L_t|^{-1}|K_t|=\prod_{t=1}^T|W_t|^{-1/2}|V_t|^{1/2}$. Then from \eqref{dlmdistjoint} we can write the joint distribution of $(V_{1:T},W_{1:T},\tilde{\gamma}_{0:T},y_{1:T})$ as
 \begin{align}
  p(&V_{1:T},W_{1:T},\tilde{\gamma}_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\tilde{\gamma}_0-m_0)'C_0^{-1}(\tilde{\gamma}_0-m_0)\right] \prod_{t=1}^T   |V_t|^{-(\eta_t + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Psi_tV_t^{-1}\right)\right] \nonumber\\
  &\times  |W_t|^{-1/2}\exp\left[-\frac{1}{2}\left(y_t - F_t\theta_t(\tilde{\gamma}_{0:T})\right)'V_t^{-1}\left(y_t - F_t\theta_t(\tilde{\gamma}_{0:T})\right)\right]\nonumber\\
   & \times |W_t|^{-(\delta_t + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Omega_tW_t^{-1}\right)\right] \exp\left[-\frac{1}{2}\tilde{\gamma}_t'(K_t^{-1}W_t(K_t^{-1})')^{-1}\tilde{\gamma}_t\right]\label{dlmdisttildejoint}
 \end{align}
Then under $\tilde{\gamma}_{0:T}$ we can write the model as
\begin{align*}
  y_t|\tilde{\gamma}_{0:T},V_{1:T},W_{1:T} & \stackrel{ind}{\sim} N\left(F_t\theta_t(\tilde{\gamma}_{0:T}), V_t\right)\\
  \tilde{\gamma}_t & \stackrel{ind}{\sim}N(0,K^{-1}_tW_t(K^{-1}_t)')
\end{align*}
for $t=1,2,\cdots,T$. Since $K_t$ is the Cholesky decomposition of $V_t$, the observation equation doesn't contain $W_t$. So $\tilde{\gamma}_{0:T}$ is a SA for $W_{1:T}|V_{1:T}$ and thus $\gamma_{0:T}$ and $\tilde{\gamma}_{0:T}$ form an AA-SA pair for $W_{1:T}|V_{1:T}$. Note also that since $W_t$ and $K_t$ are both in the system equation, $\tilde{\gamma}_{0:T}$ is not an AA for $V_{1:T}$ nor for $W_{1:T}$. 

Now consider $\tilde{\psi}_t=L_t^{-1}K_t\psi_t$ for $t=0,1,2,\cdots,T$ where, again, $L_0=K_0=I_k$, the $k\times k$ identity matrix. Then $\psi_t = K_t^{-1}L_t\tilde{\psi}_t$ and the Jacobian is block diagonal with $K_t^{-1}L_t$'s along the diagonal. So $|J|=\prod_{t=1}^T|V_t|^{-1/2}|W_t|^{1/2}$ and from \eqref{dlmerrorjoint} we can write the joint distribution of $(V_{1:T}, W_{1:T}, \tilde{\psi}_{0:T}, y_{1:T})$ as
\begin{align}
    p(&V_{1:T},W_{1:T},\tilde{\psi}_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\tilde{\psi}_0-m_0)'C_0^{-1}(\tilde{\psi}_0-m_0)\right] \nonumber\\
  &\times \prod_{t=1}^T   |V_t|^{-(\eta_t + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Psi_tV_t^{-1}\right)\right] \exp\left[-\frac{1}{2}\tilde{\psi_t}'(L_t^{-1}V_t(L_t^{-1})')^{-1}\tilde{\psi}_t\right] \nonumber\\
   & \times |W_t|^{-(\delta_t + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Omega_tW_t^{-1}\right)\right] |V_t|^{-1/2}\exp\left[-\frac{1}{2}(y_t - \tilde{\mu}_t)'(F_tW_tF_t')^{-1}(y_t-\tilde{\mu}_t)\right]\label{dlmerrortildejoint}
\end{align}
where we define $\tilde{\mu}_t =L_t\psi_t + F_tG_tF_{t-1}(y_{t-1} - L_{t-1}\tilde{\psi}_{t-1})$ with, again, $y_0=0$ and $L_0=K_0=I_k$. In terms of $\tilde{\psi}_{0:T}$, the model is then:
\begin{align*}
  y_t|V_{1:T},W_{1:T},\tilde{\psi}_{0:T},y_{1:t-1} &\sim N(\tilde{\mu}_t, F_t'W_tF_t)\\
  \tilde{\psi}_t & \stackrel{ind}{\sim} N(0,L_t^{-1}V_t(L_t^{-1})')
\end{align*}
for $t=1,2,\cdots,T$. Since $\tilde{\mu}_t$ only depends on $W_t$ (through $L_t$) and not on $V_t$, $V_{1:T}$ is absent from the observation equation. Thus $\tilde{\psi}_{0:T}$ is a SA for $V_{1:T}|W_{1:T}$ and as a result $\psi_{0:T}$ and $\tilde{\psi}_{0:T}$ form an AA-SA pair for $V_{1:T}|W_{1:T}$. Again that both $W_t$ and $V_t$ are in the system equation so $\tilde{\psi}_{0:T}$ is not an AA for either $V_{1:T}$ or $W_{1:T}$. Now we can construct a full CIS algorithm:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{fullcis}
\item Draw $\tilde{\psi}_{0:T}$ from $p(\tilde{\psi}_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$.
  \item Draw $V^{(k+0.5)}_{1:T}$ from $p(V_{1:T}|W_{1:T}^{(k)},\tilde{\psi}_{0:T},y_{1:T})$
\item Update $\psi_{0:T}$ from $\psi_0=\tilde{\psi}_0$ and $\psi_t = (K^{-1}_t)^{(k+0.5)}(L_t)^{(k)}\tilde{\psi}_t$ for $t=1,2,\cdots,T$.
\item Draw $V^{(k+1)}_{1:T}$ from $p(V_{1:T}|W_{1:T}^{(k)},\psi_{0:T},y_{1:T})$.
\item Update $\tilde{\gamma}_{0:T}$ from $\psi_{0:T}$, $W_{1:T}^{(k)}$, and $V_{1:T}^{(k+1)}$.
\item Draw $W^{(k+0.5)}_{1:T}$ from $p(W_{1:T}|V_{1:T}^{(k+1)},\tilde{\gamma}_{0:T},y_{1:T})$
\item Update $\gamma_{0:T}$ from $\gamma_0=\tilde{\gamma}_0$ and $\gamma_t = (L^{-1}_t)^{(k+0.5)}(K^{(k+1)})_t\tilde{\gamma}_t$ for $t=1,2,\cdots,T$.
\item Draw $W^{(k+1)}_{1:T}$ from $p(W_{1:T}|V_{1:T}^{(k+1)},\gamma_{0:T},y_{1:T})$

\end{enumerate}
\end{alg}

Steps 1-4 constitute a Gibbs step for $V_{1:T}$ and steps 5-8 constitute a Gibbs step for $W_{1:T}$. Step 1 can be accomplished by using FFBS to draw the states and transforming appropriately, or by using the disturbance smoother of \citet{koopman1993disturbance} and again transforming appropriately. Note that $L_t^{(k)}$ is the Cholesky decomposition of $W_t^{(k)}$ and $K_t^{(k)}$ is the Cholesky decomposition of $V_t^{(k)}$. 

It turns out that $p(W_{1:T}|V_{1:T},\tilde{\gamma}_{0:T},y_{1:T})$ and $p(W_{1:T}|V_{1:T},\theta_{0:T},y_{1:T})$ are the same density, and $p(V_{1:T}|W_{1:T},\tilde{\psi}_{0:T},y_{1:T})$ and $p(V_{1:T}|W_{1:T}^{(k+1)},\theta_{0:T},y_{1:T})$ are also the same density. Since $\tilde{\gamma}_t=K^{-1}_t(\theta_t-G_t\theta_{t-1})$ is a one-to-one function of $\theta_{0:T}$ given $V_{1:T}$ with a diagonal Jacobian, the conditional distribution of $W_{1:T}$ does not depend on whether we condition on $\theta_{0:T}$ or $\tilde{\gamma}_{0:T}$. Similar reasoning applies to $W_{1:T}$ given either $\theta_{0:T}$ or $\tilde{\psi}_{0:T}$. The upshot is that step 1 of algorithm \ref{fullcis} can be replaced with a draw from $p(\theta_{0:T}|V_{1:T},W_{1:T},y_{1:T})$, and any time we condition on one of the ``tilde'' variables, we can condition on $\theta_{0:T}$ instead. 

Now we can rewrite the full CIS algorithm in terms of $\theta_{0:T}$ instead of the tilde variables. We'll also rearrange the order in which $\theta_{0:T}$ and $\psi_{0:T}$ are used in the Gibbs step for $V_{1:T}$. This rearranging does change the algorithm, but it's still a full CIS algorithm.
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{fullcis2}
\item Draw $\psi_{0:T}$ from $p(\psi_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$.
\item Draw $V^{(k+0.5)}_{1:T}$ from $p(V_{1:T}|W_{1:T}^{(k)},\psi_{0:T},y_{1:T})$.
\item Update $\theta_{0:T}$ from $\theta_0=\psi_0$ and $\theta_t = y_t - (K^{(k+0.5)}_t)\psi_t$ for $t=1,2,\cdots,T$.
\item Draw $V^{(k+1)}_{1:T}$ from $p(V_{1:T}|W_{1:T}^{(k)},\theta_{0:T},y_{1:T})$.
\item Draw $W^{(k+0.5)}_{1:T}$ from $p(W_{1:T}|V_{1:T}^{(k+1)},\theta_{0:T},y_{1:T})$.
\item Update $\gamma_{0:T}$ from $\gamma_0=\theta_0$ and $\gamma_t = (L_t^{-1})^{(k+0.5)}(\theta_t - G_t\theta_{t-1})$ for $t=1,2,\cdots,T$.
\item Draw $W^{(k+1)}_{1:T}$ from $p(W_{1:T}|V_{1:T}^{(k+1)},\gamma_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
There is no update step between the two Gibbs steps for $V_{1:T}$ and $W_{1:T}$, i.e. between steps 4 and 5, because the DA is alread in the proper form to draw $W_{1:T}^{(k+0.5)}$ in step 5. The only thing left to show now is that this is the same as a GIS algorithm. Consider the error--dist GIS algorithm that interweaves between the scaled errors and the scaled disturbances:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{errordist}
\item Draw $\psi_{0:T}$ from $p(\psi_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$.
\item Draw $(V^{(k+0.5)}_{1:T},W^{(k+0.5)}_{1:T})$ from $p(V_{1:T},W_{1:T}|\psi_{0:T},y_{1:T})$.
\item Update $\gamma_{0:T}$ from $\gamma_0=\psi_0$ and $\gamma_t = (L^{-1}_t)^{(k+0.5)}(\theta_t - G_t\theta_{t-1})$ for $t=1,2\cdots,T$ for $\theta_0=\psi_0$ and $\theta_t = y_t - (K^{(k+0.5)}_t)\psi_t$ for $t=1,2,\cdots,T$.
\item Draw $(V^{(k+1)}_{1:T},W^{(k+1)}_{1:T})$ from $p(V_{1:T},W_{1:T}|\gamma_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
This algorithm samples $V_{1:T}$ and $W_{1:T}$ jointly in steps 2 and 4. If we instead sample them from each of their full conditionals, we get another variant of this algorithm:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{errordist2}
\item Draw $\psi_{0:T}$ from $p(\psi_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$.
\item Draw $V^{(k+0.5)}_{1:T}$ from $p(V_{1:T}|W^{(k)}_{1:T},\psi_{0:T},y_{1:T})$.
\item Draw $W^{(k+0.5)}_{1:T}$ from $p(W_{1:T}|V^{(k + 0.5)}_{1:T},\psi_{0:T},y_{1:T})$.  
\item Update $\gamma_{0:T}$ from $\gamma_0=\psi_0$ and $\gamma_t = (L_t^{-1})^{(k+0.5)}(\theta_t - G_t\theta_{t-1})$ for $t=1,2\cdots,T$ with $\theta_0=\psi_0$ and $\theta_t = y_t - (K^{(k+0.5)}_t)\psi_t$ for $t=1,2,\cdots,T$.
\item Draw $V^{(k+1)}_{1:T}$ from $p(V_{1:T}|W^{(k+0.5)}_{1:T},\gamma_{0:T},y_{1:T})$.
\item Draw $W^{(k+1)}_{1:T}$ from $p(W_{1:T}|V^{(k+1)}_{1:T},\gamma_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
Step 4 can be broken up into two steps: a transformation from $\psi_{0:T}$ to $\theta_{0:T}$, and another transformation from $\theta_{0:T}$ to $\gamma_{0:T}$. This allows us to rewrite algorithm \ref{errordist2} as:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{errordist3}
\item Draw $\psi_{0:T}$ from $p(\psi_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$.
\item Draw $V^{(k+0.5)}_{1:T}$ from $p(V_{1:T}|W^{(k)}_{1:T},\psi_{0:T},y_{1:T})$.
\item Draw $W^{(k+0.5)}_{1:T}$ from $p(W_{1:T}|V^{(k + 0.5)}_{1:T},\psi_{0:T},y_{1:T})$.  
\item Update $\theta_{0:T}$ from $\theta_0=\psi_0$ and $\theta_t = y_t - (K^{(k+0.5)}_t)\psi_t$ for $t=1,2,\cdots,T$.
\item Update $\gamma_{0:T}$ from $\gamma_0=\theta_0$ and $\gamma_t = (L^{-1}_t)^{(k+0.5)}(\theta_t - G_t\theta_{t-1})$ for $t=1,2\cdots,T$.
\item Draw $V^{(k+1)}_{1:T}$ from $p(V_{1:T}|W^{(k+0.5)}_{1:T},\gamma_{0:T},y_{1:T})$.
\item Draw $W^{(k+1)}_{1:T}$ from $p(W_{1:T}|V^{(k+1)}_{1:T},\gamma_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
Now the draw of $W_{1:T}^{(k+0.5)}$ in step 3 could actually be drawn conditional on $\theta_{0:T}$ instead of $\psi_{0:T}$ since this does not change the conditional distribution of $W_{1:T}$, so the order of steps 3 and 4 doesn't matter. Similarly in step 6 $V_{1:T}$ could be drawn conditional on $\theta_{0:T}$ isntead of $\gamma_{0:T}$ without change the distribution from which it is drawn, so steps 6 and 5 can be interchanged. This allows us to rewrite algorithm \ref{errordist3} as:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{errordist4}
\item Draw $\psi_{0:T}$ from $p(\psi_{0:T}|V_{1:T}^{(k)},W_{1:T}^{(k)},y_{1:T})$.
\item Draw $V^{(k+0.5)}_{1:T}$ from $p(V_{1:T}|W^{(k)}_{1:T},\psi_{0:T},y_{1:T})$.
\item Update $\theta_{0:T}$ from $\theta_0=\psi_0$ and $\theta_t = y_t - (K^{(k+0.5)}_t)\psi_t$ for $t=1,2,\cdots,T$.
\item Draw $V^{(k+1)}_{1:T}$ from $p(V_{1:T}|\theta_{0:T},y_{1:T})$.
\item Draw $W^{(k+0.5)}_{1:T}$ from $p(W_{1:T}|\theta_{0:T},y_{1:T})$.  
\item Update $\gamma_{0:T}$ from $\gamma_0=\theta_0$ and $\gamma_t = (L^{-1}_t)^{(k+0.5)}(\theta_t - G_t\theta_{t-1})$ for $t=1,2\cdots,T$.
\item Draw $W^{(k+1)}_{1:T}$ from $p(W_{1:T}|V^{(k+1)}_{1:T},\gamma_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
which is identical to algorithm \ref{fullcis2}. So one version of the full CIS algorithm based on $\psi_{0:T}$, $\tilde{\psi}_{0:T}$, $\gamma_{0:T}$, and $\tilde{\gamma}_{0:T}$ is identical to a GIS algorithm. As long as we believe \citeauthor{yu2011center} and don't think the order in which we use each of the DAs in a CIS or GIS algorithm matters much, there doesn't appear to be any benefit to using CIS for DLMs. 
