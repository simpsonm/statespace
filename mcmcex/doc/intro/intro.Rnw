<<set-parent-intro, echo=FALSE, cache=FALSE>>=
set_parent('../mcmcex.Rnw')
@ 

\section{Introduction}\label{sec:Intro}
Ever since the seminal article by \citet{tanner1987calculation}, data augmentation has become a common strategy for constructing MCMC algorithms in order to approximate probability distributions. Suppose $p(\phi|y)$ is the target density, in this case the posterior distribution of some parameter $\phi$ given data $y$. We use $p(.)$ to denote the probability density of the enclosed random variables. Then the data augmentation (DA) algorithm adds an augmented data vector $\theta$ with joint distribution $p(\phi,\theta|y)$ such that $\int_{\Theta}p(\phi,\theta|y)d\theta = p(\phi|u)$. Then the DA algorithm is a Gibbs sampler which constructs a Markov chain for $(\phi,\theta)$ that obtains the $k+1$'st state of $(\phi,\theta)$ from the $k$'th state as follows (we implicitly condition on the data $y$ in all algorithms and only superscript the previous and new draws of the model parameters of interest):
\begin{alg}Data Augmentation.\label{alg:DAintro}
  \begin{align*}
  [\theta|\phi^{(k)}] \to [\phi^{(k+1)}|\theta]
\end{align*}
\end{alg}
Here $[\theta|\phi^{(k)}]$ means a draw of $\theta$ from $p(\theta|\phi^{(k)},y)$ and $[\phi^{(k+1)}|\theta]$ means a draw from $p(\phi^{(k+1)}|\theta,y)$ --- implicitly we condition on the data unless otherwise noted. The augmented data vector, $\theta$, need not be interesting in any scientific sense --- it can be viewed purely as a computational construct. But for cases where the natural DA is intrinsically interesting, the DA algorithm does incidentally obtain joint draws from $p(\phi,\theta|y)$ but we will view $\theta$ as a nuisance parameter.

The EM algorithm of \citet{dempster1977maximum} and its variants are closely analogous to DA algorithms --- the DA algorithm can be viewed as a stochastic version of the EM algorithm. In fact there is a long history of using methods typically used to speed up one algorithm to speed up the other; \citet{van2010cross} shows how much overlap in the two literatures exists. The main advantage of DA and EM algorithms is their ease of implementation, but much of this work is necessary because DA and EM algorithms can often be prohibitively slow. Most of this work has focused on multilevel models --- e.g. \citet{van2001art} and \citet{papaspiliopoulos2007general}, but relatively little attention has been paid to time series models despite strong similarities between some time series models and the hierarchical models typically studied. We seek to improve DA schemes in a particular class of models --- linear, Gaussian statespace models, a.k.a. dynamic linear models (DLMs).

One particularly recent advance in the DA literature, from \citet{yu2011center}, is the notion of interweaving two separate DAs together. Suppose that we have a second agumented data vector $\gamma$ with a full joint distribution $p(\phi,\theta,\gamma|y)$ such that $\int_{\Theta}\int_{\Gamma}p(\phi,\theta,\gamma|y)d\gamma d\theta=p(\phi|y)$. Then a GIS or general interweaving strategy obtains $\phi^{(k+1)}$ from $\phi^{(k)}$ as follows:
\begin{alg}GIS.\label{alg:GISintro}
  \begin{align*}
    [\theta|\phi^{(k)}] \to [\gamma|\theta] \to [\phi^{(k+1)}|\gamma].
  \end{align*}
\end{alg}
\citeauthor{yu2011center} show that when $\theta$ is a sufficient augmentation (SA, a.k.a. centered augmentation)  and $\gamma$ is an ancillary augmentation (AA, a.k.a. non-centered augmentation), i.e. $p(\gamma|\phi)=p(\gamma)$, then under some weak conditions this ``ancillary-sufficient'' interweaving strategy, or ASIS, is equivalent to the the optimal PX-DA algorithm of, e.g. \citet{meng1999seeking}, \citet{liu1999parameter}, \citet{van2001art} and \citet{hobert2008theoretical}. Our main purpose is to apply this idea to a particular class of DLMs.

The generic DLM can be defined as follows:
\begin{align}
y_t &= F_t\theta_t + v_t && v_t \stackrel{ind}{\sim} N_k(0,V_t) \label{dlmtdobseq}\\
\theta_t &= G_t\theta_{t-1} + w_t && w_t \stackrel{ind}{\sim} N_p(0,W_t) \label{dlmtdsyseq}
\end{align}
for $t=1,2,\cdots,T$, and $v_{1:T}$, $w_{1:T}$ independent. Equation \eqref{dlmtdobseq} is called the {\it observation equation} and equation \eqref{dlmtdsyseq} is called the {\it system equation}. Similarly, $v_{1:T}$ are called the {\it observation errors}, $V_{1:T}$ are called the {\it observation variances}, $w_{1:T}$ are called the {\it system disturbances} and $W_{1:T}$ are called the {\it system variances}. The observed data is $y_{1:T}$ while $\theta_{0:T}$ is called the latent states, and is the usual DA for this model. For each $t=1,2,\cdots,T$, $F_t$ is a $k\times p$ matrix and $G_t$ is a $p\times p$ matrix. Let $\phi$ denote the vector of unknown parameters in the model. Then possibly $F_{1:T}$, $G_{1:T}$, $V_{1:T}$, and $W_{1:T}$ are all functions of $\phi$. 

The subclass of DLMs we will focus on forces the variances to be time independent and treats $F_{1:T}$ and $G_{1:T}$ as known. Typically additional model structure is used to learn about $V_{1:T}$ and $W_{1:T}$ if time dependence is enforced anyway -- e.g. a stochastic volatility prior which would require a statespace model describing the $V_{1:T}$'s and $W_{1:T}$'s as data. However, many of our results may be useful in more complicated time-varying variance models. So we set $V_t=V$ and $W_t=W$ for $t=1,2,\cdots,T$. Forcing $F_{1:T}$ and $G_{1:T}$ to be constant on the one hand is not a big constraint since relaxing it will simply add one or more Gibbs steps to the algorithms we explore so long as no parameter that enters any $F_t$ or $G_t$ also enters $V$ or $W$. On the other hand, the behavior of these algorithms will depend on the precise structure of the unknown components of $F_{1:T}$ and $G_{1:T}$ and in one of the data augmentations that we discuss there is a bit more housekeeping associated with $F_{1:T}$ depending on an unknown parameter (Section \ref{sec:scalederrors}). In any case, when $\phi=(V,W)$ is our unkown parameter vector and we can write the model as
\begin{align}
  y_t|\theta_{0:T} \stackrel{ind}{\sim} & N(F_t\theta_t,V) \label{dlmobseq}\\
  \theta_t|\theta_{0:t-1}  \sim & N(G_t\theta_{t-1},W) \label{dlmsyseq}.
\end{align}
To complete the model specification in a Bayesian context, we need priors on $\theta_0$, $V$, and $W$. We'll use the standard approach and assume that they're mutually independent a priori and that $\theta_0 \sim N(m_0, C_0)$, $V \sim IW(\Lambda_V, \lambda_V)$ and $W \sim IW(\Lambda_W, \lambda_W)$ where $m_0$, $C_0$, $\Lambda_V$, $\lambda_V$, $\Lambda_W$, and $\lambda_W$ are known hyperparameters and $IW(\Lambda, \lambda)$ denotes the inverse Wishart distribution with degrees of freedom $\lambda$ and positive definite scale matrix $\Lambda$.

We show that in this particular class of DLMs, the usual DA vector, $\theta_{0:T}$, is neither a SA nor an AA. Furthermore, we show that given some reasonable assumptions about the any SA, generically $\theta$, drawing from $p(\theta|\phi,y)$ is just as difficult as drawing from the target posterior distribution $p(\phi|y)$, which defeats the purpose of looking for a SA to begin with. We do, however, find two separate AAs --- the scaled disturbances, $\gamma_{0:T}$, defined by $\gamma_0=\theta=0$ and $\gamma_t = L_W^{-1}w_t$ for $t=1,2,\cdots, T$ where $L_W$ is the Cholesky decomposition of $W$, and the scaled errors, $\psi_{0:T}$, defined by $\psi_0=\theta_0$ and $\psi_t=L_V^{-1}v_t$ for $t=1,2,\cdots T$ where $L_V$ is the Cholesky decomposition of $V$. The former has been used in both the multilevel models and time series literature and is essentially the standard non-centered augmentation, but the latter is novel. 

Furthermore, we employ the componentwise interweaving strategy (CIS) of \citeauthor{yu2011center}. A CIS algorithm for $\phi=(\phi_1, \phi_2)$ essentially employs interweaving for each block of $\phi$ spearately, e.g.
\begin{alg}CIS.\label{alg:CIS}\\
  \begin{center}
    \begin{tabular}{llllll}
      $[\theta_1|\phi_1^{(k)},\phi_2^{(k)}]$ & $\to$  & $[\gamma_1|\phi_2^{(k)},\theta_1]$ & $\to$ & $[\phi_1^{(k+1)}|\phi_2^{(k)},\gamma_1]$ &$\to$ \\
      $[\theta_2|\phi_1^{(k+1)},\phi_2^{(k)},\gamma_1]$ &$\to$ & $[\gamma_2|\phi_1^{(k+1)},\theta_2]$ & $\to$ & $[\phi_2^{(k+1)}|\phi_1^{(k+1)},\gamma_2]$ &
    \end{tabular}
  \end{center}
\end{alg}
where $\theta_i$ and $\gamma_i$ are distinct data augmentations for $i=1$ or $i=2$, but potentially $\gamma_1=\theta_i$ for $i=1$ or $i=2$ or $\gamma_2=\theta_i$ for $i=1,2$. The first line draws $\phi_1$ conditional on $\phi_2$ using interweaving in a Gibbs step, while the second line does the same for $\phi_2$ condtional on $\phi_1$. The algorithm can easily be extended to greater than two blocks within $\phi$. The advantage with CIS is that it is often easier to find an AA--SA pair of DAs for $\phi_1$ conditional on $\phi_2$ and another pair for $\phi_2$ conditional on $\phi_1$ than for $\phi=(\phi_1,\phi_2)$ jointly. We construct a CIS algorithm for the subclass of DLMs we consider, updating $V$ and $W$ in separate blocks, based on the ``wrongly scaled'' versions of the scaled errors and scaled disturbances, i.e. we create an AA-SA pair for $W$ using $\gamma_{0:T}$ and $\tilde{\gamma}_{0:T}$ where $\tilde{\gamma}_0=\theta_0$ and $\tilde{\gamma}_t=L_V^{-1}w_t$ for $t=1,2,\cdots,T$ and similarly for $V$ using $\psi_{0:T}$ and $\tilde{\psi}_{0:T}$, analogously defined. Further, we that $\tilde{\gamma}_{0:T}$ and $\tilde{\psi}_{0:T}$ can both be replaced with $\theta_{0:T}$ without changing the algorithm, despite the fact that $\theta_{0:T}$ is a SA for $W$ conditional on $V$ but not for $V$ conditional on $W$. Even further, we show that this algorithm is the same as a GIS algorithm that interweaves between $\gamma_{0:T}$ and $\psi_{0:T}$, except with the steps arranged in a different order. 

In the context of a particular DLM, the local level model with univariate $y_t$, univariate $\theta_t$, and $F_t=G_t=1$, we conduct a simulation study in order to explore the properties of the various MCMC algorithms derived for the general case. In the process we find that we have to give up drawing $V$ and $W$ jointly when conditioning on any DA other than the states, and in doing so we draw from two classes of densities closely related to the generalized inverse Gaussian distribution (see e.g. \citet{jorgensen1982statistical}). These densities take the form
\begin{align*}
  p(x)&\propto x^{-\alpha - 1}\exp\left[-ax + bx^{1/2} - cx^{-1}\right]\\
  \intertext{and}
  p(x)&\propto x^{-\alpha - 1}\exp\left[-ax + bx^{-1/2} - cx^{-1}\right]
\end{align*}
Both densities contain the generalized inverse Gaussian as a special case when $b=0$ and in our problem both are often log-concave. 

In our simulations, we find that the true signal-to-noise ratio, $R=W/V$, determines the behavior of all of the standard DA algorithms based on the various DAs we construct, and the behavior of the GIS algorithms can be traced to the behavior fo the base DA algorithms for the DAs used in the GIS algorithm. In particular we find that the scaled disturbance DA algorithm works best for $R<1$ while the scaled error DA algorithm works best for $R>1$, while GIS algorithm that interweaves between the scaled disturbances and the scaled errors works well as long as $R$ is not too close to one, though for larger sample sizes all algorithms have problems in increasingly large regions of the parameter space.

The rest of the paper is organized as follows. In Section \ref{sec:DLMest} we discuss data augmentation methods, introduce several new data augmentations for this class of DLMs and show that it is unlikely that a useful sufficient augmentation (centered augmentation) exists when we consider all model parameters as unknown. Section \ref{sec:DLMinter} contains several interweaving algorithms based on the various data augmentations we discuss in Section \ref{sec:DLMest} along with some results about when various CIS and GIS algorithms are equivalent. In Section \ref{sec:LLMest} we work out an example with the local level model and report the results of fitting the model to different simulated datasets using several of the MCMC samplers we construct. Finally Section \ref{sec:Discussion} discusses our results and concludes.
