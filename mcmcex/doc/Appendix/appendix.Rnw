<<set-parent-Appendix, echo=FALSE, cache=FALSE>>=
set_parent('../mcmcex.Rnw')
@

\section{Appendix}\label{sec:append}

\subsection{Efficiently drawing from $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$}
Both of these two densities are of the form
\begin{align*}
\log p(x) = lp(x) & -ax + b\sqrt{x} - (\alpha + 1)\log x -\beta/x + C 
\end{align*}
for $x>0$ where $C$ is some constant, $\alpha$ and $\beta$ are the hyperparameters for $x$, and $a>0$ \& $b\in \Re$ are parameters that depend on the data, $y$, the relevant data augmentation ($\psi$ or $\gamma$), and the other variable ($W$ or $V$). This density is not a known form and is difficult to sample from. We provide two different rejection sampling strategies below that work well under different circumstances, and combine them into a single strategy.

\subsubsection{Adaptive rejection sampling}
One nice strategy is to use adaptive rejection sampling, e.g. \citet{gilks1992adaptive}. This requires $lp(x)$ to be concave, which is easy enough to check. The second derivative of $lp(x)$ is:
\begin{align*}
\frac{\partial^2 lp(x)}{\partial x^2} &= -\frac{1}{4}bx^{-3/2} +(\alpha + 1)x^{-2} -2 \beta x^{-3}.
\end{align*}
Then we have
\begin{align*}
  &\frac{\partial^2 lp(x)}{\partial x^2} < 0 \iff \\
  &-\frac{b}{4}x^{3/2} + (\alpha + 1)x - 2\beta < 0
\end{align*}
which would imply that $lp(x)$ is concave. We can maximize the left hand side of the last equation very easily. When $b\leq 0$ the max occurs at $x=\infty$ such that $LHS > 0$, but when $b > 0$:
\begin{align*}
  \frac{\partial LHS}{\partial x} &= -\frac{3}{8}bx^{1/2} + \alpha + 1 = 0\\
  \implies & x^{max} = \frac{(\alpha + 1)^2}{b^2}\frac{64}{9}.
\end{align*}
Then we have
\begin{align*}
  LHS \leq LHS|_{x=x^{max}} = \frac{(\alpha + 1)^3}{b^2}\frac{64}{27} - 2\beta
\end{align*}
so that
\begin{align*}
  LHS|_{x=x^{max}} < 0 &\iff  \frac{(\alpha + 1)^3}{b^2}\frac{64}{27} < 2\beta\\
    &\iff b > \left(\frac{(\alpha + 1)^3}{\beta}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}.
\end{align*}
This last condition is necessary and sufficient for $lp(x)$ to be globally (for $x>0$) concave since $b < 0$ forces $LHS > 0$ for some $x$. When the condtion is satisfied, we can use adaptive rejection sampling --- which is already implemented in the \verb0R0 package \verb0ars0. We input the initial evalutions of $lp(x)$ at the mode $x^{mode}$ and at $2x^{mode}$ and $0.5x^{mode}$ in order to get the algorithm going.

\subsubsection{Rejection sampling on the log scale}
When $b \leq \left(\frac{(\alpha + 1)^3}{\beta}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}$, which happens often --- especially for small $T$ --- we need to rely on a different method to sample from $p(x)$. A naive approach would be to construct a normal or $t$ approximation to $p(x)$ and use that as a proposal in a rejection sampler. It turns out that this is often very inefficient, but for $y=log(x)$ the approach works well. Note that
\begin{align*}
  p_y(y) = p_x(e^y)e^y
\end{align*}
so that we can write the log density of $y$ as (dropping the subscripts):
\begin{align*}
  lp(y) = -ae^y + be^{y/2} - \alpha y - \beta e^{-y}.
\end{align*}
The mode of this density $y^{mode}$ can be easily found numerically, and the second derivative is:
\begin{align*}
  \frac{\partial^2 lp(y)}{\partial y^2} = -ae^y + \frac{b}{4}e^{y/2} - \beta e^{-y}.
\end{align*}
The $t$ approximation then uses the proposal distribution 
\begin{align*}
  t_{v}\left(y^{mode}, \left[\left.\frac{\partial^2 lp(y)}{\partial y^2}\right|_{y=y^{mode}}\right]^{-1}\right).
\end{align*}
In practice choosing degrees of freedom $v=1$ works very well over the region of the parameter space where adaptive rejection sampling cannot be used. We can easily use this method when adaptive rejection sampling does not work, then transform $y$ back to $x$. It remains to check that the tails of $t$ distribution dominate the tails of our target distribution. Let $lq(y)$ denote the log density of the proposoal distribution. Then we need
\begin{align*}
  lp(y) - lq(y) \leq M\\
  \intertext{for some constant M, i.e.}
  -ae^y + be^{y/2} - \alpha y - \beta e^{-y} -\left(\frac{v+1}{2}\right)\log\left[1 + \frac{1}{v}\left(\frac{y-\mu}{\sigma}\right)\right]\leq M
\end{align*}
where $a>0$, $\alpha>0$, $\beta>0$, $v>0$, $\sigma>0$, and $b,\mu\in \Re$. We can rewrite the LHS as
\begin{align*}
    e^{y/2}(b-ae^{y/2}) - \alpha y - \beta e^{-y} -\left(\frac{v+1}{2}\right)\log\left[1 + \frac{1}{v}\left(\frac{y-\mu}{\sigma}\right)\right].
\end{align*}
So as $y\to\infty$ this quantity goes to $-\infty$ since the first term will eventually become negative no matter the value of $b$, and all other terms are always negative. Similarly as $y\to\-\infty$ this quantity goes to $-\infty$. Now pick any interval $(y_1,y_2)$ such that outside of the interval, $LHS<\epsilon$. Since treated as a function of $y$ the LHS is clearly continuous, it attains a maximum on this interval, and thus is bounded.

\subsubsection{Intelligently choosing a rejection sampler}
In practice, adaptive rejection sampling is relatively efficient for $p_x(x)$ but inefficient for $p_y(y)$ --- so much so that rejection sampling with the $t$ approximation for $p_y(y)$ is more efficient. To minimize computation time, it is best to use adaptive rejection sampling for $p_x(x)$ when the concavity condition is satisfied. When it is not, the $t$ approximation works well.

\subsection{Efficiently drawing from $p(W|V,\tilde{\gamma},y)$ and $p(V|W,\tilde{\psi},y)$ in the LLM}

Both the density of $\log(W)|V,\tilde{\gamma}_{0:T},y_{1:T}$ and the density of $\log(V)|W,\tilde{\psi}_{0:T},y_{1:T}$ have the following form:
\begin{align*}
  p(y)\propto \exp\left[-\alpha y - ae^{-y} + be^{-y/2} - ce^y\right].
\end{align*}
where $\alpha>0$, $a>0$, $c>0$, and $b\in \Re$. The log density is:
\begin{align*}
  lp(y) = -\alpha y - ae^{-y} + be^{-y/2} - ce^y + C
\end{align*}
where $C$ is some constant. We only provide one strategy for rejection sampling from this density: the $t$ approximation. Similar reasoning to above shows that we can use a $t$ distribution as a proposal in a rejection sampler. Now we choose the location parameter by maximizing $lp(y)$ in $y$ numerically to find the mode, $y^{mode}$. Next the second derivative of $lp(y)$ is given by
\begin{align*}
  \frac{\partial^2 lp(y)}{\partial y^2} = -ae^{-y} + \frac{b}{4}e^{-y/2}-ce^y.
\end{align*}
We then set the scale parameter to be
\begin{align*}
  -\left[\left.\frac{\partial^2 lp(y)}{\partial y^2}\right|_{y=y^{mode}}\right]^{-1}
\end{align*}
as in the normal approximation, and the degrees of freedom parameter to $v=1$. This rejection sampler is tolerably efficient for our purposes, but it is not fast.


