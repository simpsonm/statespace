<<set-parent-mod, echo=FALSE, cache=FALSE>>=
set_parent('../mcmcex.Rnw')
@ 

\section{Model}\label{sec:DLMmodel}

The general dynamic linear model (DLM) is a linear, gaussian, state space model and can be written as
\begin{align}
y_t &= F_t\theta_t + v_t && v_t \stackrel{ind}{\sim} N_k(0,V_t) \label{dlmtdobseq}\\
\theta_t &= G_t\theta_{t-1} + w_t && w_t \stackrel{ind}{\sim} N_p(0,W_t) \label{dlmtdsyseq}
\end{align}
for $t=1,2,\cdots,T$, and $v_{1:T}$, $w_{1:T}$ independent. Equation \eqref{dlmtdobseq} is called the {\it observation equation} and equation \eqref{dlmtdsyseq} is called the {\it system equation}. Similarly, $v_{1:T}$ are called the observation errors, $V_{1:T}$ are called the observation variances, $w_{1:T}$ are called the system disturbances and $W_{1:T}$ are called the system variances. The observed data is $y_{1:T}$ while $\theta_{0:T}$ are called the latent states. For each $t=1,2,\cdots,T$, $F_t$ is a $k\times p$ matrix and $G_t$ is a $p\times p$ matrix. Let $\phi$ denote the vector of unknown parameters in the model. Then possibly $F_{1:T}$, $G_{1:T}$, $V_{1:T}$, and $W_{1:T}$ are all functions of $\phi$. 

We will focus our attention on a simple version of the DLM. Typically additional model structure is used to learn about $V_{1:T}$ and $W_{1:T}$ if time dependence is enforced -- e.g. a stochastic volatility prior which would require a statespace model describing the $V_{1:T}$'s and $W_{1:T}$'s as data. Because of this additional complexity, we focus on the time-constant variances model, though many of our results may be useful in more complicated time-varying variance models. So we set $V_t=V$ and $W_t=W$ for $t=1,2,\cdots,T$. We will also suppose that $F_t$ and $G_t$ are known matrices for $t=1,2,\cdots,T$, though this constraint is immaterial since relaxing it will simply add one or more Gibbs steps to the algorithms we explore so long as no parameter that enters any $F_t$ or $G_t$ also enters $V$ or $W$. However, in one of the data augmentations that we discuss, the scaled error data augmentation, there is a bit more housekeeping associated with $F_{1:T}$ depending on an unknown parameter (Section \ref{sec:scalederrors}).

When $\phi=(V,W)$ is our unkown parameter vector and we can write the model as
\begin{align}
  y_t|\theta_{0:T} \stackrel{ind}{\sim} & N(F_t\theta_t,V) \label{dlmobseq}\\
  \theta_t|\theta_{0:t-1}  \sim & N(G_t\theta_{t-1},W) \label{dlmsyseq}
\end{align}
To complete the model specification in a Bayesian context, we need priors on $\theta_0$, $V$, and $W$. We'll use the standard approach and assume that they're mutually independent a priori and that $\theta_0 \sim N(m_0, C_0)$, $V \sim IW(\Lambda_V, \lambda_V)$ and $W \sim IW(\Lambda_W, \lambda_W)$ where $m_0$, $C_0$, $\Lambda_V$, $\lambda_V$, $\Lambda_W$, and $\lambda_W$ are known hyperparameters and $IW(\Lambda, \lambda)$ denotes the inverse Wishart distribution with degrees of freedom $\lambda$ and positive definite scale matrix $\Lambda$. This allows us to write the full joint distribution of $(V,W,\theta_{0:T},y_{1:T})$ as
\begin{align}
  p(&V,W,\theta_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\theta_0-m_0)'C_0^{-1}(\theta_0-m_0)\right] \nonumber\\
  &\times   |V|^{-(\lambda_V + k + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - F_t\theta_t)'V^{-1}(y_t - F_t\theta_t)\right] \nonumber\\
   & \times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t-G_t\theta_{t-1})'W^{-1}(\theta_t-G_t\theta_{t-1})\right]\label{dlmjoint}
 \end{align}
where $p=dim(\theta_t)$, $k=dim(y_t)$, and $\tr(.)$ is the matrix trace operator.




