<<set-parent-LLMest, echo=FALSE, cache=FALSE>>=
set_parent('../mcmcex.Rnw')
@

\section{Application: The Local Level Model}\label{sec:LLMest}

In order to illustrate how these algorithms work, we will focus on the local level model for simplicity though there are still some difficulties. The local level model (LLM) is a DLM with univariate data $y_t$ for $t=1,2,\cdots,T$ and a univariate latent state $\theta_t$ for $t=0,2,\cdots,T$ that satisfies
\begin{align}
  y_t |\theta_{0:T}& \stackrel{ind}{\sim} N(\theta_t,V) \label{llmobseq}\\
  \theta_t |\theta_{0:t-1}& \sim N(\theta_{t-1},W) \label{llmsyseq}
\end{align}
with $\theta_0\sim N(m_0,C_0)$. Here $\theta_t=E[y_t|\theta_{0:T}]$. The states are $\theta_{0:T}$, the scaled disturbances are $\gamma_{0:T}$ with $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$, and the scaled errors are $\psi_{0:T}$ with $\psi_0=\theta_0$ and $\psi_t=(y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$. The independent inverse Wishart priors on $V$ and $W$ in Section \ref{sec:Intro} cash out to independent inverse gamma priors for the local level model, viz. $V\sim IG(\alpha_V,\beta_V)$ and $W\sim IG(\alpha_W,\beta_W)$. 

\subsection{Base Samplers}\label{sec:LLMbase}

The joint density of $(V,W,\theta_{0:T},y_{1:T})$ is:
\begin{align}
  p(&V,W,\theta_{0:T},y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)} \exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \theta_{t})^2\right)\right]\nonumber\\
  &W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(\theta_t - \theta_{t-1})^2\right) \right] \exp\left[-\frac{1}{2C_0}(\theta_0 - m_0)^2\right]\label{llmstatejoint}
\end{align}
This immediately gives the state sampler:
\begin{alg}State Sampler for LLM.\label{alg:LLMstate}\\
  \begin{center}
    \begin{tabular}{lll}
      $[\theta_{0:T}|V^{(k)},W^{(k)}]$& $\to$& $[V^{(k+1)},W^{(k+1)}|\theta_{0:T}]$
    \end{tabular}
  \end{center}
\end{alg}
In step 2, $V$ and $W$ are independent with $V\sim IG(a_V,b_V)$ and $W\sim IG(a_W, b_W)$ where $a_V = \alpha_V + T/2$, $b_V = \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2$, $a_W = \alpha_W + T/2$, and $b_W = \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2$.

The scaled disturbance sampler, i.e. the DA algorithm based on the scaled disturbances, is a bit more complicated. In this context $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$, and thus $\theta_t = \gamma_0 + \sqrt{W}\sum_{s=1}^t\gamma_s$ for $t=1,2,\cdots,T$. Following \eqref{dlmdistjoint}, we can write the joint posterior of $(V,W,\gamma_{0:T})$ as
\begin{align}
  p(&V,W,\gamma_{0:T}|y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)}\exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \gamma_0 - \sqrt{W}\textstyle\sum_{s=1}^t\gamma_s)^2\right)\right] \nonumber\\
  & \times W^{-(\alpha_W + 1)}\exp\left[-\frac{\beta_W}{W}\right] \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\gamma_t^2\right]\exp\left[-\frac{1}{2C_0}(\gamma_0-m_0)^2\right]\label{llmdistpost}
\end{align}
Now $V$ and $W$ are no longer conditionally independent given $\gamma_{0:T}$ and $y_{1:T}$. Instead of attempting the usual DA algorithm, we will add an extra Gibbs step and draw $V$ and $W$ separately primarily for ease of computation. This gives us the scaled disturbance sampler:
\begin{alg}Scaled Disturbance Sampler for LLM.\label{alg:LLMdist}\\
  \begin{center}
    \begin{tabular}{lllll}
      $[\gamma_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V^{(k+1)}|W^{(k)},\gamma_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$
    \end{tabular}
  \end{center}
\end{alg}
In step 2, $V$ is drawn from the same inverse gamma distribution as in step 2 of algorithm \ref{alg:LLMstate}. In step 3, the draw of $W$ is more complicated. The density can be written as
\begin{align*}
  p(W|V,\gamma_{0:T},y_{1:T}) \propto & W^{-\alpha_W - 1}\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \gamma_0 - \sqrt{W}\sum_{s=1}^t\gamma_s\right)^2\right]\exp\left[-\frac{\beta_W}{W}\right].
\end{align*}
This density is not any known form and is difficult to sample from, though its functional form is similar to the generalized inverse gaussian distribution. The density can be written as
\begin{align*}
p(W|V,\gamma_{0:T},y_{1:T}) \propto& W^{-\alpha_W - 1}\exp\left[-aW + b\sqrt{W} -\frac{\beta_W}{W}\right]. 
\end{align*}
where $a=\sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$ and $b=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$. It can be shown that $b > \left(\frac{(\alpha + 1)^3}{\beta}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}$ implies that the density is log concave --- see the appendix for details. It turns out that this tends to hold over a wide region of the parameter space --- so long as $V$ is smaller or is not much larger than $W$. This allows for the use of adaptive rejection sampling in order to sample from this distribution in many cases, e.g. using \citet{gilks1992adaptive}. An alternative is to use a $t$ approximation to the conditional density as a proposal in a rejection sampler. This is much more computationally expensive when necessary, but it works ok on the log scale.

The scaled error sampler is similar to the scaled disturbance sampler and this is easy to see in the local level model. Here $\psi_0=\theta_0$ and $\psi_t = (y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$ so that $\theta_t = y_t - \sqrt{V}\psi_t$ for $t=1,2,\cdots,T$. From \eqref{dlmerrorjoint} we can write $p(V,W,\psi_{0:T}|y_{1:T})$ as
\begin{align}
    p(&V,W,\psi_{0:T},y_{1:T}) \propto W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(Ly_t - \sqrt{V}L\psi_t)^2\right)\right]\nonumber \\
 & V^{-(\alpha_V + 1)}\exp\left[-\frac{\beta_V}{V}\right]  \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\psi_t^2\right] \exp\left[-\frac{1}{2C_0}(\psi_0-m_0)^2 \right] \label{llmerrorjoint}
\end{align}
where we define $Ly_t=y_t-y_{t-1}$ for $t=2,3,\cdots,T$ \& $Ly_1=y_1 - \psi_0$ and $L\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$ \& $L\psi_1=\psi_1-0$. Once again, $V$ and $W$ are no longer conditionally independent given $\psi_{0:T}$ and $y_{1:T}$. In fact, the density is analgous to \eqref{llmdistpost} with $V$ and $W$ switching places. The scaled error sampler obtained from drawing $V$ and $W$ separately is:
\begin{alg}Scaled Error Sampler for LLM.\label{alg:LLMerror}\\
  \begin{center}
    \begin{tabular}{lllll}
    $[\psi_{0:T}|V^{(k)},W^{(k)}]$&$\to$&$[V^{(k+1)}|W^{(k)},\psi_{0:T}]$&$\to$&$[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$
    \end{tabular}
  \end{center}
\end{alg}
In step 3, $W$ is drawn from the same inverse gamma distribution as in step 2 of algorithm \ref{alg:LLMstate}. Drawing $V$ in step 2 is exactly analogous to drawing $W$ in algorithm \ref{alg:LLMdist}. The density of $V|W,\psi_{0:T},y_{1:T}$ can be written as
\begin{align*}
\log p(V|W,\psi_{0:T},y_{1:T}) \propto V^{-\alpha_V - 1}\exp\left[ -aV + b\sqrt{V} -]frac{\beta_V}{V}\right] 
\end{align*}
where $a=\sum_{t=1}^T(L\psi_t)^2/2W$ and $b=\sum_{t=1}^T(L\psi_tLy_t)/W$, i.e. the form of the density is the same as that of $W|V,\gamma_{0:T},y_{1:T}$. 

We can also construct the DA algorithms based on the ``wrongly scaled'' disturbances or errors. The wrongly scaled disturbances are defined by $\tilde{\gamma}_t = \gamma_t\frac{\sqrt{W}}{\sqrt{V}}$ for $t=1,2,\cdots,T$ and $\tilde{\gamma}_0=\gamma_0$ while the wrongly scaled errors are defined by $\tilde{\psi}_t = \psi_t\frac{\sqrt{V}}{\sqrt{W}}$ for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\psi_0$. For $\tilde{\gamma}_{0:T}$ we have
\begin{align}
  p(V,W&|\tilde{\gamma}_{0:T},y_{1:T}) \propto W^{-\alpha_W - T/2 - 1}\exp\left[-\frac{1}{2W/V}\sum_{t=1}^T\tilde{\gamma}_t^2\right]\exp\left[-\frac{\beta_W}{W}\right]\nonumber\\
  &\times V^{-\alpha_V-1}\exp\left[-\frac{\beta_V}{V}\right]\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \tilde{\gamma}_0 - \sqrt{V}\sum_{s=1}^t\tilde{\gamma}_s\right)^2\right].\label{llmwdistpost}
\end{align}
Thus the conditional posterior of $W$ given $V$ and $\tilde{\gamma}_{0:T}$ is the same as if we had conditioned on $\theta_{0:T}$ instead of $\tilde{\gamma}_{0:T}$. In other words
\begin{align*}
  p(W|V,\tilde{\gamma}_{0:T},y_{1:T}) \propto W^{-(\alpha_W + T/2) - 1}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}V\sum_{t=1}^T\tilde{\gamma}_t^2\right)\right]
\end{align*}
so that $V|W,\tilde{\gamma}_{0:T},y_{1:T}\sim IG(a_W, b_W)$ where $a_W = \alpha_W + T/2$ and 
\begin{align*}
  b_W = \beta_W + \frac{1}{2}V\sum_{t=1}^T\tilde{\gamma}_t^2 = \beta_W + \frac{1}{2}\sum_{t=1}^T(\theta_t - \theta_{t-1})^2.
\end{align*}
The conditional posterior of $V$ is more complicated. We have
\begin{align*}
  p(V|W,\tilde{\gamma}_{0:T},y_{1:T}) &\propto \exp\left[-\frac{1}{2W/V}\sum_{t=1}^T\tilde{\gamma}_t^2\right] V^{-\alpha_V-1}\exp\left[-\frac{\beta_V}{V}\right]\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \tilde{\gamma}_0 - \sqrt{V}\sum_{s=1}^t\tilde{\gamma}_s\right)^2\right]\\
  &\propto V^{-\alpha_V - 1}\exp\left[-\frac{a}{V} + \frac{b}{\sqrt{V}} - cV\right]
\end{align*}
where 
\begin{align*}
  a & = \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\gamma}_0)^2 > 0\\
  b & = \sum_{t=1}^T(y_t - \tilde{\gamma}_0)\sum_{s=1}^t\tilde{\gamma}_s\\
  c & = \frac{1}{2W}\sum_{t=1}^T\tilde{\gamma}_t^2 > 0.
\end{align*}
We will return to this density momentarily.

For the wrongly scaled errors, we have
\begin{align}
  p(V,W&|\tilde{\psi}_{0:T},y_{1:T}) \propto V^{-\alpha_V - T/2 -1}\exp\left[-\frac{1}{2V/W}\sum_{t=1}^T\tilde{\psi}_t^2\right]\exp\left[-\frac{\beta_V}{V}\right]\nonumber\\
  &\times W^{-\alpha_W-1}\exp\left[-\frac{1}{2W}\sum_{t=1}^T\left(\tilde{Ly}_t - \sqrt{W}(\tilde{L\psi}_t)\right)\right]\label{llmwerrorpost}
\end{align}
where we define $L\tilde{y}_t = y_t - y_{t-1}$ for $t=1,2,\cdots,T$ and $L\tilde{y}_1=y_1 - \tilde{\psi}_0$, and $L\tilde{\psi}_t=\tilde{\psi}_t-\tilde{\psi}_{t-1}$ for $t=1,2,\cdots,T$ with $L\tilde{\psi}_1=\tilde{\psi}_1$. Then the conditional posterior of $V$ is the same as if we had conditioned on $\theta_{0:T}$ instead of $\tilde{\psi}_{0:T}$, i.e.
\begin{align*}
  p(V|W,\tilde{\psi}_{0:T},y_{1:T}) & \propto V^{-(\alpha_V - T/2)-1}\exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}W\sum_{t=1}^T\tilde{\psi}_t^2\right)\right]
\end{align*}
so that $V|W,\tilde{\psi}_{0:T},y_{1:T}\sim IG(a_V, b_V)$ where $a_V = \alpha_V + T/2$ and
\begin{align*}
  b_V = \beta_V + \frac{1}{2}W\sum_{t=1}^T\tilde{\psi}_t^2 = \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \theta_t)^2.
\end{align*}
The conditional posterior of $W$ is more complicated but similar to that of $V$ when we conditioned on $\tilde{\gamma}_{0:T}$. We have
\begin{align*}
  p(W|V,\tilde{\psi}_{0:T},y_{1:T}) &\propto \exp\left[-\frac{1}{2V/W}\sum_{t=1}^T\tilde{\psi}_t^2\right] W^{-\alpha_W-1}\exp\left[-\frac{1}{2W}\sum_{t=1}^T\left(L\tilde{y} - \sqrt{W}L\tilde{\psi}\right)\right]\\
  &\propto W^{-\alpha_W - 1}\exp\left[-\frac{a}{W} + \frac{b}{\sqrt{W}} - cW\right]
\end{align*}
where now
\begin{align*}
  a & = \beta_W + \frac{1}{2}\sum_{t=1}^TL\tilde{y}_t^2 > 0\\
  b & = \sum_{t=1}^TL\tilde{y}_tL\tilde{\psi}_t\\
  c & = \frac{1}{2V}\sum_{t=1}^T\tilde{\psi}_t^2 > 0.
\end{align*}

So in the case of both wrongly scaled DAs we need to sample from a density of the form
\begin{align*}
  p(x) \propto x^{-\alpha -1}\exp\left[-\frac{a}{x} + \frac{b}{\sqrt{x}} - cx\right].
\end{align*}
The density of $y=\log(x)$ is 
\begin{align*}
  p(y) \propto \exp\left[-\alpha y - ae^{-y} + be^{-y/2} - ce^y\right].
\end{align*}
This density is easy to sample from fairly efficiently with rejection sampler using a $t$ approximation as a proposal. The details of this sampler are in the appendix.

\subsection{Hybrid Samplers: Interweaving and Alternating}
Section \ref{sec:DLMinter} contains the details for the interweaving algorithms in the general DLM. In the local level model the only difference is that we only sample $V$ and $W$ jointly when we condition on the states. We will consider all four GIS samplers based on any two or three of the base samplers and one CIS sampler. In the GIS samplers, the order of the parameterizations will always be the states $(\theta_{0:T})$, then the scaled disturbances $(\gamma_{0:T})$, then the scaled errors $(\psi_{0:T})$. All of the GIS algorithms and the Full CIS algorithm are below in Table \ref{GISalgorithms}. Note the distributional forms for each of these steps (in some cases a transformation) are in Section \ref{sec:LLMbase}. We omit the partial CIS algorithm, though note that in practice it behaves essentially the same as the State-Dist algorithm.
\begin{table}[!h]
  \centering
\begin{enumerate}
  \item State-Dist GIS algorithm:\\
    \begin{center}
      \begin{tabular}{lllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V,W|\theta_{0:T}]$& $\to$&\\ 
        $[\gamma_{0:T}|V,W,\theta_{0:T}]$& $\to$& $[V^{(k+1)}|W,\gamma_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$
        \end{tabular}
    \end{center}
  \item State-Error GIS algorithm:\\
    \begin{center}
      \begin{tabular}{lllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$&$[V,W|\theta_{0:T}]$& $\to$&\\ 
        $[\psi_{0:T}|V,W,\theta_{0:T}]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$
      \end{tabular}
    \end{center}
  \item Dist-Error GIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllll}
        $[\gamma_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V|W^{(k)}, \gamma_{0:T,}y_{1:T}]$& $\to$& $[W|V, \gamma_{0:T}]$& $\to$\\ 
        $[\psi_{0:T}|V,W,\gamma_{0:T}]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$&
      \end{tabular}
    \end{center}
  \item Triple GIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V,W|\theta_{0:T}]$& $\to$&& \\
        $[\gamma_{0:T}|V,W,\theta_{0:T}]$& $\to$& $[V|W,\gamma_{0:T}]$& $\to$& $[W|V,\gamma_{0:T}]$ & $\to$\\
        $[\psi_{0:T}|V,W,\gamma_{0:T}]$& $\to $&$[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$&
      \end{tabular}
    \end{center}
  \item Full CIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$& $\to$& $[V|W^{(k)},\theta_{0:T}]$& $\to$& $[\psi_{0:T}|V,W,\theta_{0:T}]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$\\ 
        $[\theta_{0:T}|V^{(k+1)},W]$& $\to$& $[W|V^{(k+1)},\theta_{0:T}]$& $\to$& $[\gamma_{0:T}|V^{(k+1)},W]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$&
    \end{tabular}
\end{center}
\end{enumerate}
\caption{GIS and CIS algorithms for the local level model}
\label{GISalgorithms}
\end{table}

Interweaving algorithms are conceptually very similar to alternating algorithms. For every GIS algorithm, there is a corresponding alternating algorithm  where each $[DA_2|V,W,DA_1]$ step is replaced by a $[DA_2|V,W]$ step (here $DA_i$ is a data augmentation for $i=1,2$.). Table \ref{altalgorithms} contains each alternating algorithm. Note that there are two possible ``hybrid triple'' algorithms that we do not consider here where the move from $\theta_{0:T}$ to $\gamma_{0:T}$ interweaves and while the move from $\gamma_{0:T}$ to $\psi_{0:T}$ alternates and vice versa.
\begin{table}[!h]
  \centering
\begin{enumerate}
  \item State-Dist alternating algorithm:\\
    \begin{center}
      \begin{tabular}{lllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V,W|\theta_{0:T}]$& $\to$& \\
        $[\gamma_{0:T}|V,W]$& $\to$& $[V^{(k+1)}|W,\gamma_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$
      \end{tabular}
    \end{center}
  \item State-Error alternating GIS algorithm:\\
    \begin{center}
      \begin{tabular}{lllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V,W|\theta_{0:T}]$& $\to$& \\
        $[\psi_{0:T}|V,W]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$
      \end{tabular}
    \end{center}
  \item Dist-Error alternating GIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllll}
        $[\gamma_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V|W, \gamma_{0:T}]$& $\to$& $[W|V, \gamma_{0:T}]$& $\to$\\
        $[\psi_{0:T}|V,W]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$&
      \end{tabular}
    \end{center}
  \item Triple alternating GIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$&$ [V,W|\theta_{0:T}]$&$ \to$&&\\
        $[\gamma_{0:T}|V,W]$&$ \to$&$ [V|W,\gamma_{0:T}]$&$ \to$&$ [W|V,\gamma_{0:T}]$& $\to$\\
        $[\psi_{0:T}|V,W]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$&
      \end{tabular}
    \end{center}
\end{enumerate}
\caption{Alternating algorithms for the local level model}
\label{altalgorithms}
\end{table}

Table \ref{LLMalgorithms} lists each algorithm we considered for the local level model. The basic idea here is that the alternating algorithms should serve as a sort of baseline to compare the corresponding interweaving algorithms against. The GIS version of an algorithm at least be similar to the alternating version since both take advantage of the fact that when at least one of the underlying DA algorithms works well, the hybrid algorithm should also work well. When the two DAs are (nearly) independent in the posterior, it is plausible that the GIS version of the algorithm would improve upon the results of the alternating version. Since we do not have such a pair of DAs, it seems unlikely that there will be much of a difference between the GIS and alternating versions of a given algorithm. We can make the comparisons fairly precise by considering the effective sample size (ESS) of the Markov chain -- given the same sample size for two algorithms, we can compare ESS to see which one has better mixing.

\begin{table}[!ht]
  \centering
  \begin{tabular}{|l||l|l|l|l|}\hline
    Base & State & (wrongly) Scaled Disturbance & (wrongly) Scaled Error & \\\hline
    GIS & State-Dist & State-Error & Dist-Error & Triple \\
    Alt & State-Dist & State-Error & Dist-Error & Triple \\
    CIS & \multicolumn{3}{l|}{State-Error for $V|W$; State-Dist for $W|V$} & \\
    \hline
  \end{tabular}
  \caption{Each algorithm considered for the local level model}
  \label{LLMalgorithms}
\end{table}

\subsection{Simulation Setup}

In order to test these algorithms, we simulated a fake dataset from the local level model for various choices of $V$, $W$, and $T$. We created a grid over $V$--$W$ space with $(V,W)$ ranging from $(10^{-2},10^{-2})$ to $(10^2, 10^2)$ and we simulated a dataset for all possible combinations of $V$ and $W$ with each of $T=10, 100, 1000$. Then for each dataset, we fit the local level model using each algorithm in Table \ref{LLMalgorithms}. We used the same rule for constructing priors for each model: $\theta_0\sim N(0,10^7)$, $V\sim IG(5, 4V^*)$, and $W\sim IG(5, 4W^*)$, mutually independent where $(V^*,W^*)$ are the true values of $V$ and $W$ used to simulate the time series. So both the prior and likelihood and thus the posterior roughly agree about the likely values of $V$ and $W$. 

For each dataset and each sampler we obtained $n=3000$ draws and threw away the first $500$ as burn in. The chains were started at the true values used to simulated the time series, so we can examine the behavior of the chains to determine how well they mix but not how quickly they converge. Define the effective sample proportion (ESP) for a scalar component of the chain as the effective number of independent draws, i.e. effective sample size (ESS) (see e.g. \citet{gelman2003bayesian}) of the component divided by the actual sample size, i.e. $ESP=ESS/n$. An $ESP=1$ indicates that the Markov chain is behaving as if it obtains iid draws from the posterior. It is possible to obtain $ESP>1$ if the draws are negatively correlated and occasionally for some of our samplers our estimates of $ESP$ are greater than one, but we round this down to one for plotting purposes.

\subsection{Base Results}

<<plotsetup, echo=FALSE, message=FALSE>>=
library(ggplot2)
library(scales)
library(plyr)
library(xtable)
library(reshape2)
library(gridExtra)
load("../../mixing/samout.RData")
samout2 <- samout
load("../../das/OldDAs/samout.RData")
samout$stime <- 0
samout <- rbind(samout2, samout)
rm(samout2)
base <- c("error", "state", "dist")
alts <- c("sdalt", "sealt", "dealt", "trialt")
ints <- c("sdint", "seint", "deint", "triint")
kerns <- c("sdkern", "sekern", "dekern", "trikern")
cis <- c("fullcis", "partialcis")
wrongs <- c("errorda", "distda")
samout$V.ES[samout$sampler %in% kerns] <- samout$V.ES[samout$sampler %in% kerns]*2
samout$W.ES[samout$sampler %in% kerns] <- samout$W.ES[samout$sampler %in% kerns]*2
samout$V.ES[samout$sampler == "trikern"] <- samout$V.ES[samout$sampler == "trikern"]*(3/2)
samout$W.ES[samout$sampler == "trikern"] <- samout$W.ES[samout$sampler == "trikern"]*(3/2)
samout$type <- "Base" #$
samout$type[samout$sampler %in% alts] <- "Alt" 
samout$type[samout$sampler %in% ints] <- "GIS" 
samout$type[samout$sampler %in% kerns] <- "RKern" 
samout$type[samout$sampler %in% cis] <- "CIS" 
samout$type[samout$sampler %in% wrongs] <- "W-Base" 
samout$samplers <- "Base"
samout$samplers[substr(samout$sampler, 1, 2)=="sd"] <- "State-Dist" 
samout$samplers[substr(samout$sampler, 1, 2)=="se"] <- "State-Error" 
samout$samplers[substr(samout$sampler, 1, 2)=="de"] <- "Dist-Error" 
samout$samplers[substr(samout$sampler, 1, 3)=="tri"] <- "Triple" 
samout$samplers[samout$sampler=="fullcis"] <- "FullCIS"
samout$samplers[samout$sampler=="partialcis"] <- "PartialCIS"
samout$samplers[samout$sampler=="error"] <- "Error"
samout$samplers[samout$sampler=="dist"] <- "Dist"
samout$samplers[samout$sampler=="state"] <- "State"
samout$samplers[samout$sampler=="errorda"] <- "W-Error"
samout$samplers[samout$sampler=="distda"] <- "W-Dist"
samlevels <- c("State", "Dist", "Error", "State-Dist", "State-Error", "Dist-Error", 
               "Triple", "FullCIS", "PartialCIS", "W-Dist", "W-Error")
samout$samplers <- factor(samout$samplers, levels=samlevels)
samout$V.time <- samout$time/samout$V.ES
samout$W.time <- samout$time/samout$W.ES
meltedsam <- melt(samout, id=c("type", "samplers", "sampler", "V.T", "W.T", 
                            "T.T"))
Vs <- unique(meltedsam$V.T)[1:9] #$
Ws <- Vs
breaks <- Vs[seq(1,9,2)]
label_both_parsed <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), function(x) parse(text = x))
}
label_both_parsed_split <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), 
        function(x) parse(text = strsplit(x, "\\.")[[1]][1]))
}
label_parsed_split <- function(variable, value){
  llply(as.character(value), function(x) parse(text = strsplit(x, "\\.")[[1]][1]))
}
plotfun <- function(meltedsam, vars, sams, T, title){
  castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable + samplers ~ ., 
                     subset=.(variable %in% vars  & T.T==T & sampler %in% sams &
                       V.T<=10^2 & W.T<=10^2))
  colnames(castedsam)[6] <- "value"
  out <- ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
         geom_tile() +
         scale_fill_gradient("ESP", low=muted("red"), high="white",
           guide=guide_colorbar(barheight=10),
           limits=c(0,1), na.value="white", trans="sqrt") +
         facet_grid(variable~samplers, scales="free", labeller=label_parsed_split) +
         scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
         ggtitle(paste(title, T, sep="")) +
         theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
  return(out)
}
load("../../cors/newpostcors.RData")
newpostcors <- newpostcors[newpostcors$V.T <= 10^2 & newpostcors$W.T <= 10^2,]
plotfuntime <- function(meltedsam, vars, sams, T, title, top){
  castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable + samplers ~ ., 
                     subset=.(variable %in% vars  & T.T==T & sampler %in% sams &
                       V.T<=10^2 & W.T<=10^2))
  colnames(castedsam)[6] <- "value"
  out <- ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=log(value*1000/60))) + #$
         geom_tile() +
         scale_fill_gradient("Log min", high=muted("red"), low="white",
           guide=guide_colorbar(barheight=10), na.value="red") +
         facet_grid(variable~samplers, scales="free", labeller=label_parsed_split) +
         scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
         ggtitle(paste(title, T, sep="")) +
         theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
  return(out)
  ##, limits=c(0,top)
}
plotfuncor <- function(newpostcors, var, title){
  dat <- newpostcors
  id <- which(colnames(newpostcors)==var)
  colnames(dat)[id] <- "value"
  out <- ggplot(data=dat, aes(x=V.T, y=W.T, fill=value)) +
      geom_tile() +
      scale_fill_gradient2("Corr", low=muted("blue"), high=muted("red"),
         limits=c(-1,1), mid="white") +
      facet_grid(.~T, scales="free", labeller=label_both) +
      scale_x_log10("V = noise", breaks=breaks) +
      scale_y_log10("W = signal", breaks=breaks) +
      ggtitle(title) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
  return(out)
}
@ 

Figure \ref{baseESplot} contains plots of ESP for $V$ and $W$ in each chain of each base sampler for $T=100$ and $T=1000$ --- we omit the $T=10$ plots for brevity. We will focus on $T=100$ first. The state sampler has a low ESP for $V$ and a high ESP for $W$ when the signal-to-noise ratio, $W/V$, is larger than one. When the signal-to-noise ratio is smaller than one, on the other hand, the state sampler has a low ESP for $W$ and a high ESP for $V$. In the typical case where the signal to noise ratio close to one, the state sampler has a modest to low ESP for both $V$ and $W$. Note that the particular values of $V$ and $W$ do not seem to matter at all --- just their relative values, i.e. the signal-to-noise ratio $R=W/V$. Moving up any diagonal on the plots for $V$ and $W$ in the state sampler, $W/V$ is constant and the ESS appears roughly constant. The basic lesson here is that the state sampler has mixing issues for whichever of $V$ or $W$ is smaller. 

Figure \ref{baseESplot} tells a different story for the scaled disturbance sampler. When the signal-to-noise ratio is less than one, ESPs for both $V$ and $W$ are nearly 1, i.e. the effective sample size is nearly the actual sample size of the chain. When the signal-to-noise ratio is greater than one, however, ESP for both $V$ and $W$ becomes small, especially for $V$. Once again the absolute values of $V$ and $W$ do not matter for this behavior --- just the relative values. The scaled error sampler has essentially the opposite properties. When $R$ is large, it has a near 1 ESP for both $V$ and $W$. On the other hand, when $R$ is small is has a low ESP for both $V$ and $W$, especially for $V$. The lesson here seems to be that the scaled disturbances are the preferred data augmentation for low signal-to-noise ratios and the scaled errors  are the preferred data augmentation for high signal-to-noise ratios, while the states are preferred for signal-to-noise ratios near 1. The wrongly scaled disturbances ($\tilde{\gamma}_{0:T}$) and wrongly scaled errors ($\tilde{\psi}_{0:T}$), on the other hand, look like worse versions of the state sampler. The pattern of mixing for $V$ and $W$ over the range of the parameter space is essentially the same as the state sampler, except the wrongly scaled disturbance sampler has worse mixing for $V$ than the state sampler almost everywhere and similarly the wrongly scaled error sampler has worse mixing for $W$ than the state sampler almost everywhere.

The plots for $T=1000$ in Figure \ref{baseESplot} tell basically the same story, with a twist. Increasing the length of the time series seems to exacerbate all problems without changing the basic conclusions. As $T$ increases, $R$ has to be smaller and smaller for the scaled disturbance sampler to have decent mixing, and similarly $R$ has to be larger and larger for the scaled error sampler to have decent mixing. Interestingly, the scaled error sampler appears to mix well for both $V$ and $W$ over a larger region of the space $R<1$ than the scaled disturbance sampler does over $R>1$. The state sampler is stuck between a rock and a hard place, so to speak, since as $T$ increases, good mixing for $V$ requires $R$ to be smaller and smaller, but good mixing for $W$ requires $R$ to be larger and larger. The wrongly scaled samplers are again pretty similar to the state sampler for larger $T$ except the wrongly scaled sampler tends to be worse everywhere for the variance that was used to scale --- i.e. once again the wrongly scaled disturbance sampler has worse mixing for $V$ than the state sampler while the wrongly scaled error smapler has worse mixing for $W$ than the state sampler. However, the wrongly scaled samplers do appear to have slightly better mixing than the state sampler for the variance that was {\it not} used to scale. In particular, the wrongly scaled error sampler appears to have slightly better mixing for $V$ than the state sampler over part of the parameter space. 

The behavior of the wrongly scaled data augmentation algorithms is consistent with what we showed in Section \ref{sec:DLMinter} --- that the Full CIS algorithm based on the scaled errors and disturbances and the wrongly scaled errors and disturbances is equivalent to the Full CIS algorithm that replaces the wrongly scaled DAs with the usual latent states. Since the behavior of the state sampler and the wrongly scaled disturbance sampler are the same for $W$, we might expect that when drawing $W$ it does not matter whether we use the states or the wrongly scaled disturbances in the Full CIS algorithm. Similarly since the behavior of the state sampler and the wrongly scaled error sampler are the same for $V$, we might expect that it does not matter which one we use when drawing $V$. In fact this is what we found when constructing the Full CIS algorithm. Even though $(\gamma_{0:T}, \tilde{\gamma}_{0:T})$ forms an AA-SA pair for $W|V$ and $(\psi_{0:T},\tilde{\psi}_{0:T})$ forms an AA-SA pair for $V|W$ while $\theta_{0:T}$ is not a SA for $V$, replacing $\tilde{\gamma}_{0:T}$ and $\tilde{\psi}_{0:T}$ with $\theta_{0:T}$ does not actually change the CIS algorithm.

We summarize some of the above results for convenience in Table \ref{tab:stnmix}.
\begin{table}
  \centering
  \begin{tabular}{|l|ccccc|}\hline
    Parameter & State & Dist & Error & W-Dist & W-Error \\\hline
    V & $R < 1$ & $R < 1$ & $R > 1$ & $R < 1$ & $R < 1$\\
    W & $R > 1$ & $R < 1$ & $R > 1$ & $R > 1$ & $R > 1$ \\\hline
  \end{tabular}
  \caption{Rule of thumb for when each base algorithm has a high effective sample size for each variable as a function of the true signal-to-noise ratio, $R=W/V$.}
  \label{tab:stnmix}
\end{table}
Most of the patterns of Figure \ref{baseESplot} and Table \ref{tab:stnmix} can be explained by Figure \ref{corplot}, which contains the estimated posterior correlations between various functions of parameters estimated using the simulations from the Triple-Alternating sampler. First we need to understand the correlations we are looking at. The state sampler consists of two steps --- a draw of $\theta_{0:T}$ given $V$ and $W$, and a draw of $(V,W)$ given $\theta_{0:T}$. From Section \ref{sec:LLMbase} we have that conditional on $\theta_{0:T}$, $V$ and $W$ are independent in the posterior and each has an inverse gamma distribution that depends on the states only through the second parameter:
\begin{align*}
  b_V &= \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2\\
  b_W &= \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2.
\end{align*}
So we can view $(b_V,b_W)$ as the data augmentation instead of $\theta_{0:T}$ and thus the state sampler is
\begin{align*}
  [b_V, b_W|V^{(k)},W^{(k)}] \to [V^{(k+1)},W^{(k+1)}|b_V,b_W].
\end{align*}
Thus the dependence between $(V,W)$ and $(b_V,b_W)$ in the posterior will determine how much the state sampler moves in a given iteration. 

For the scaled disturbance sampler, things are a bit more complicated. Now the data augmentation becomes
\begin{align*}
  b_V &= \beta_V + \sum_{t=1}^T(y_t - \gamma_0 -\sqrt{W}\sum_{s=1}^t\gamma_t^2)^2/2 = \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2\\
  a_\gamma & = \sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V\\
  b_\gamma &=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V.
\end{align*}
This again comes from Section \ref{sec:LLMbase}. The draw of $V|W,\gamma_{0:T}$ is the same inverse gamma draw as in the state sampler. The draw of $W|V,\gamma_{0:T}$ depends on two random parameters, $a_\gamma$ and $b_\gamma$ defined above. So the dependence between $V$ and $b_V$ determines how much the marginal chain for $V$ chain moves in a single iteration, while the dependence between $W$ and $(a_\gamma,b_\gamma)$ determines how much the marginal chain for $W$ moves in a single iteration. The scaled error sampler is analogous to the scaled disturbance sampler except with 
\begin{align*}
  b_W &= \beta_W + \sum_{t=2}^T(Ly_t - \sqrt{V}L\psi_t)^2/2 = \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2\\
  a_\psi&=\sum_{t=1}^T(L\psi_t)^2/2W\\
  b_\psi&=\sum_{t=1}^T(L\psi_tLy_t)/W
\end{align*}
and with the dependence in marginal chain for $W$ depending on the dependence between $b_W$ and $W$ while the dependence in the marginal chain for $V$ depending on the dependence betwee $(a_\psi, b_\psi)$ and $V$.

In Figure \ref{corplot} we see that the posterior correlation between $V$ and $b_V$ is high when $R>1$ and is low when $R<1$. This explains why both the state sampler and the scaled disturbance sampler have low ESP's for $V$ when $R>1$. Similarly the posterior correlation between $W$ and $b_W$ is high when $R<1$ and is low when $R>1$, which explains why both the state sampler and scaled error sampler have low ESP's for $W$ when $R<1$. 

{\it Insert explanation of marginal chain for $W$ in the scaled disturbance sampler and for the marginal chain for $V$ in the scaled error sampler. Right now it doesn't look right - the correlation between $V$ and $a_\psi$ or $b_\psi$ is highest when $R$ is high, but for the scaled error sampler, autocorrelation in the marginal chain for $V$ is highest when $R$ is low. However, here's a thought. Consider the marginal chain for $W$ in the scaled disturbance sampler - this chain essentially consists of two steps, a draw of $(a_\gamma,b_\gamma|W)$ and a draw of $(W|a_\gamma , b_\gamma)$. When $W$ is negatively correlated with both $a_\gamma$ and $b_\gamma$, a large draw of $W$ in the chain means that we expect small draws of $a_\gamma$ and $b_\gamma$ because of the negative correlation which, in turn, means that we expect a large draw of the next $W$ again because of the negative correlation. This may explain what we are seeing.}


<<baseESplot, fig.cap=cap, echo=FALSE, fig.height=3.75, fig.width=8, out.width=".7\\textwidth">>=
cap <- "Effective sample proportion in the posterior sampler for a time series of lengths $T=100$ and $T=1000$, for $V$ and $W$, and for the state, scaled disturbance, scaled error, wrongly scaled disturbance, and wrongly scaleed error samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Note that for plotting purposes, effective sample proportions larger than $1$ were rounded down to $1$"
vars <- c("V.ES", "W.ES")
title <- "ESP for V and W in the base algorithms, T="
##p1 <- plotfun(meltedsam, vars, c(base,wrongs), 10, title)
p2 <- plotfun(meltedsam, vars, c(base,wrongs), 100, title)
p3 <- plotfun(meltedsam, vars, c(base,wrongs), 1000, title)
##p1
p2
p3
@ 

<<corplot, echo=FALSE, fig.height=3, fig.width=8, out.width=".49\\textwidth", fig.cap=cap>>=
cap <- "Posterior correlation between $V$ or $W$ and $b_V$ or $b_W$. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high."
title <- "Posterior Correlation Between V and bv"
p1 <- plotfuncor(newpostcors, "Vbv", title)
title <- "Posterior Correlation Between W and bw"
p2 <- plotfuncor(newpostcors, "Wbw", title)
title <- "Posterior Correlation Between V and apsi"
p3 <- plotfuncor(newpostcors, "Vapsi", title)
title <- "Posterior Correlation Between W and agam"
p4 <- plotfuncor(newpostcors, "Wagam", title)
title <- "Posterior Correlation Between V and bpsi"
p5 <- plotfuncor(newpostcors, "Vbpsi", title)
title <- "Posterior Correlation Between W and bgam"
p6 <- plotfuncor(newpostcors, "Wbgam", title)
p1
p2
p3
p4
p5
p6
@ 

\subsection{GIS and CIS Results}

Based on the intuition in Section \ref{sec:DLMest} above, both the GIS and alternating algorithms should work best when at least one of the underlying base algorithms has a high ESP --- the basic idea is that when least one of the underlying algorithms has low autocorrelation, we should have low autocorrelation in the GIS algrothim using multiple DAs. This suggests that the Dist-Error GIS and alternating algorithms will have the best performance of the GIS and alternating algorithms using two DAs for both $V$ and $W$, especially for $R$ far away from one. When $R$ is near one it may offer no improvement, especially for large $T$. The State-Dist algorithms should have trouble with $V$ when $R$ is high since both the state sampler and the scaled disturbance sampler have trouble with $V$ when $R$ is high. Similarly, the State-Error GIS algorithm should have trouble with $W$ when $R$ is low since both underlying samplers have trouble with $W$ when $R$ is low. Since the triple algorithms add the state sampler into the Dist-Error algorithms, it seems plausible that it might improve mixing for one of $V$ or $W$ since for $R$ different from one, the state sampler has good mixing for at least one of $V$ of $W$. 

There are a couple of ways to gain some intuition about what we expect the Full CIS algorithm to do before seeing the results. First, we mentioned in Section \ref{sec:DLMinter} that the Full CIS and the Dist-Error GIS algorithm consist of the same steps, just rearranged. This suggests that they should perform similarly so that we expect the Full CIS algorithm to have good mixing for both $V$ and $W$ when $R$ is sufficiently different from one. We can draw the same conclusion in a different way by noticing that in the Gibbs step for $V$, the CIS algorithm interweaves between the states and the scaled errors and in the Gibbs step for $W$ it interweaves between the states and the scaled disturbances. Since the state sampler has a high ESP for $V$ when $R<1$ and the scaled disturbance sampler has a high ESP for $V$ when $R>1$ we should expect the Full CIS sampler to have a high ESP for $V$ when $R$ is different from one. Similarly, since the state sampler has a high ESP for $W$ when $R>1$ and the scaled eror sampler has a high ESP for $W$ when $R<1$, we should expect the Full CIS sampler to have a high ESP for $W$ when $R$ is different from one.


<<intESplot, fig.cap=cap, echo=FALSE, fig.height=3.75, fig.width=8, out.width=".7\\textwidth">>=
cap <- "Effective sample proportion in the posterior sampler for $V$ and $W$ in for $T=100$ and $T=1000$, in all three GIS samplers based on any two of the base samplers and the full CIS sampler. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Note that for plotting purposes, effective sample proportions larger than one were rounded down to one."
vars <- c("V.ES", "W.ES")
sams <- c("deint", "seint", "sdint", "triint", "fullcis")
title <- "ESP for V and W in the GIS and CIS algorithms, T="
##p1 <- plotfun(meltedsam, vars, sams, 10, title)
p2 <- plotfun(meltedsam, vars, sams, 100, title)
p3 <- plotfun(meltedsam, vars, sams, 1000, title)
##p1
p2
p3
@ 

We can verify most of these intuitions in Figure \ref{baseESplot}. First, the State-Dist GIS algorithm has high ESP for $W$ except for a narrow band where $R$ is near one, though this band becomes much wider as $T$ increases. The State-Dist GIS algorithm's mixing behavor for $V$ appears identical to the original state sampler --- high ESP when $R < 1$ and poor ESP when $R > 1$, and again the good region shrinks as $T$ increases. So this algorithm behaves as expected --- it takes advantage of the fact that the state and scaled disturbance DA algorithms make up a ``beauty and the beast'' pair for $W$ and thus improves mixing for $W$. However, the two underlying DA algorithms behave essentially indentically for $V$ so there is no improvement. Similarly the State-Error GIS algorithm's ESP for $W$ is essentially identical to the state and scaled error algorithms' ESP for $W$ --- high when $R$ large and low when $R$ small.  For $V$, the State-Error algorithm has a high ESP when $R$ is far enough away from one, especially when $T$ is small. The Dist-Error GIS algorithm also behaves as predicted --- when $R$ is not too close to one it has high ESP for both $V$ and $W$, though as $T$ increases $R$ has too be farther away from one in order for the ESPs to be high. The Dist-Error GIS algorithm behaves apparently identically to the Full CIS and triple GIS algorithms, with some differences when $T$ is small. The first of these is not surprising --- based on the intuition that the Dist-Error GIS and Full CIS algorithms are the same up to a reordering of each of their steps, we expected little if any difference. However, we had some hope that the triple GIS algorithm would improve upon the Dist-Error GIS algorithm somewhat by further breaking the correlation between iterations in the Markov chain. This did not happen and furthermore the State-Dist and State-Error samplers did not improve the ESP for $V$ or $W$ respectively. When the underlying DA algorithms form a ``beauty and the beast'' pair, the interweaving algorithm appears to mix just as well as the best mixing single DA algorithm. Figure \ref{altESplot} allows us to see the ESP of the alternating algorithms in order to compare them to the GIS algorithms. There appears to be little practical difference between the alternating and interweaving versions of a given algorithm based on any two or three of the base DAs --- both take advantage of the beauty and the beast nature of their underlying DAs but dependence between the two DAs prevent the GIS version from improving on the alternating version.

<<altESplot, fig.cap=cap, echo=FALSE, fig.height=3.75, fig.width=7, out.width=".7\\textwidth">>=
cap <- "Effective sample proportion in the posterior sampler for $V$ and $W$ in for $T=100$ and $T=1000$, in all four alternating samplers. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Note that for plotting purposes, effective sample proportions larger than one were rounded down to one."
vars <- c("V.ES", "W.ES")
sams <- c(ints)
title <- "ESP for V and W in the alternating algorithms, T="
##p1 <- plotfun(meltedsam, vars, sams, 10, title)
p2 <- plotfun(meltedsam, vars, sams, 100, title)
p3 <- plotfun(meltedsam, vars, sams, 1000, title)
##p1
p2
p3
@ 

\subsection{Computational time}\label{sec:LLMtime}

A more important question than how well the chain mixes from a practical standpoint is the full computational time required to adequately characterize the posterior distribution. In order to investigate this, we compute the natural log of the average time in minutes required for each sampler to achieve an effective sample size of 1000 --- in other words the log time per 1000 effective draws. All simulations were performed on {\it INSERT DETAILS ABOUT IMPACT3}. While different systems will yield different absolute times, the relative times should at least be similar. Figure \ref{baseinttimeplot} contains plots of the log time per 1000 effective draws for both $V$ and $W$ and for each of the base and interweaving samplers --- note that the scales change as $T$ changes. 

For $T=100$ and $T=1000$ the pattern we saw for ESP begins also appears for time per 1000 effective draws. The state sampler becomes very slow to reach 1000 effective draws for $V$ when $R>1$ and for $W$ when $R<1$. The scaled disturbance and scaled error samplers behave as expected --- the scaled disturbance sampler is slow for both $V$ and $W$ when $R>1$ while the scaled disturbance sapler is slow for both $V$ and $W$ when $R<1$. The Dist-Error GIS, Triple GIS and Full CIS algorithms appear to be the big winners here and are almost indistinguishable. All three algorithms are slightly slower for $V$ when $R$ is near one, and when $R$ is near or below one all three are slow for $W$. Compared to the state sampler though, all three offer large gains over most of the parameter space. When we compare the GIS algorithms to their alternating counterparts in terms of log time per 1000 effective draws, again there is little difference. Figure \ref{altinttimeplot} shows the log time per 1000 effective draws for the alternating algorithms and we see essentially the same pattern as we saw for the GIS algorithms.

<<baseinttimeplot, fig.cap=cap, echo=FALSE, fig.width=10, fig.height=3.25, out.width=".8\\textwidth">>=
cap <- "Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=100$ and $T=1000$, in the state, scaled disturbance and scaled error samplers and for all five interweaving samplers. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. The signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. For plotting purposes, times larger than the top of the scale are displayed in bright red."
vars <- c("V.time", "W.time")
sams <- c("dist", "error", "deint", "state", "seint", "sdint", "triint", "fullcis")
title <- "Log of time (minutes) per 1000 effective draws for base and interweaving samplers, T="
##p1 <- plotfuntime(meltedsam, vars, sams, 10, title, 25*60/1000)
p2 <- plotfuntime(meltedsam, vars, sams, 100, title, 60*60/1000)
p3 <- plotfuntime(meltedsam, vars, sams, 1000, title, 1100*60/1000)
##p1
p2
p3



<<altinttimeplot, fig.cap=cap, echo=FALSE, fig.width=8, fig.height=3.75, out.width=".49\\textwidth">>=
cap <- "Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=100$ and $T=1000$, in the alternating, GIS, and random kernel samplers. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. The signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. For plotting purposes, times larger than the top of the scale are displayed in bright red."
sams <- c(alts,ints)
title <- "Log of time (minutes) per 1000 effective draws for alternating samplers, T="
##p1 <- plotfuntime(meltedsam, vars, sams, 10, title, 25*60/1000)
p2 <- plotfuntime(meltedsam, vars, sams, 100, title, 60*60/1000)
p3 <- plotfuntime(meltedsam, vars, sams, 1000, title, 1100*60/1000)
##p1
p2
p3
@ 
