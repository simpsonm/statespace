<<set-parent-LLMest, echo=FALSE, cache=FALSE>>=
set_parent('../mcmcex.Rnw')
@

\section{Application: The Local Level Model}\label{sec:LLMest}

In order to illustrate how these algorithms work, we will focus on the local level model for simplicity though there are still some difficulties. The local level model (LLM) is a DLM with univariate data $y_t$ for $t=1,2,\cdots,T$ and a univariate latent state $\theta_t$ for $t=0,2,\cdots,T$ that satisfies
\begin{align}
  y_t |\theta_{0:T}& \stackrel{ind}{\sim} N(\theta_t,V) \label{llmobseq}\\
  \theta_t |\theta_{0:t-1}& \sim N(\theta_{t-1},W) \label{llmsyseq}
\end{align}
with $\theta_0\sim N(m_0,C_0)$. Here $\theta_t=E[y_t|\theta_{0:T}]$. The states are $\theta_{0:T}$, the scaled disturbances are $\gamma_{0:T}$ with $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$, and the scaled errors are $\psi_{0:T}$ with $\psi_0=\theta_0$ and $\psi_t=(y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$. The independent inverse Wishart priors on $V$ and $W$ in Section \ref{sec:DLMmodel} cash out to independent inverse gamma priors for the local level model, viz $V\sim IG(\alpha_V,\beta_V)$ and $W\sim IG(\alpha_W,\beta_W)$. 

\subsection{Base Samplers}\label{sec:LLMbase}

The joint density of $(V,W,\theta_{0:T},y_{1:T})$ is:
\begin{align*}
  p(&V,W,\theta_{0:T},y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)} \exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \theta_{t})^2\right)\right]\nonumber\\
  &W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(\theta_t - \theta_{t-1})^2\right) \right] \exp\left[-\frac{1}{2C_0}(\theta_0 - m_0)^2\right]%\label{llmstatejoint}
\end{align*}
This immediately gives the state sampler:
\begin{alg}[State Sampler for LLM]\label{llmstatealg}
  \begin{align*}
  [\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T}] \to [V^{(k+1)},W^{(k+1)}|\theta_{0:T},y_{1:T}]
  \end{align*}
\end{alg}
In step 2, $V$ and $W$ are independent with $V\sim IG(a_V,b_V)$ and $W\sim IG(a_W, b_W)$ where $a_V = \alpha_V + T/2$, $b_V = \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2$, $a_W = \alpha_W + T/2$, and $b_W = \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2$.

The scaled disturbance sampler, i.e. the DA algorithm based on the scaled disturbances, is a bit more complicated. In this context $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$, and thus $\theta_t = \sqrt{W}\sum_{t=1}^T\gamma_t + \gamma_0$ for $t=1,2,\cdots,T$. Following \eqref{dlmdistjoint}, we can write the joint posterior of $(V,W,\gamma_{0:T})$ as
\begin{align}
  p(&V,W,\gamma_{0:T}|y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)}\exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \gamma_0 - \sqrt{W}\textstyle\sum_{s=1}^t\gamma_s)^2\right)\right] \nonumber\\
  & \times W^{-(\alpha_W + 1)}\exp\left[-\frac{\beta_W}{W}\right] \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\gamma_t^2\right]\exp\left[-\frac{1}{2C_0}(\gamma_0-m_0)^2\right]\label{llmdistpost}
\end{align}
Now $V$ and $W$ are no longer conditionally independent given $\gamma_{0:T}$ and $y_{1:T}$. Instead of attempting the usual DA algorithm, we will add an extra Gibbs step and draw $V$ and $W$ separately primarily for ease of computation. This gives us the scaled disturbance sampler:
\begin{alg}[Scaled Disturbance Sampler for LLM]\label{llmdistalg}
  \begin{align*}
    [\gamma_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V^{(k+1)}|W^{(k)},\gamma_{0:T},y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\gamma_{0:T},y_{1:T}]
  \end{align*}
\end{alg}
In step 2, $V$ is drawn from the same inverse gamma distribution as in step 2 of algorithm \ref{llmstatealg}. In step 3, the draw of $W$ is more complicated. The density can be written as
\begin{align*}
  p(W|V,\gamma_{0:T},y_{1:T}) \propto & W^{-\alpha_W - 1}\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \gamma_0 - \sqrt{W}\sum_{s=1}^t\gamma_s\right)^2\right]\exp\left[-\frac{\beta_W}{W}\right].
\end{align*}
This density is not any known form and is difficult to sample from, though its functional form is similar to the generalized inverse gaussian distribution. The log density can be written as
\begin{align*}
\log p(W|V,\gamma_{0:T},y_{1:T}) =& -aW + b\sqrt{W} - (\alpha_W + 1)\log W -\beta_W/W + C 
\end{align*}
where $C$ is some constant, $a=\sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$ and $b=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$. It can be shown that $b > \left(\frac{(\alpha + 1)^3}{\beta}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}$ implies that the density is log concave. It turns out that this tends to hold over a wide region of the parameter space --- so long as $V$ is smaller or is not much larger than $W$. This allows for the use of adaptive rejection sampling in order to sample from this distribution in many cases, e.g. using \citet{gilks1992adaptive}. An alternative is to use a $t$ approximation to the conditional density as a proposal in a rejection sampler. This is much more computationally expensive when necessary, but it works ok on the log scale.

The scaled error sampler is similar to the scaled disturbance sampler and this is easy to see in the local level model. Here $\psi_0=\theta_0$ and $\psi_t = (y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$ so that $\theta_t = y_t - \sqrt{V}\psi_t$ for $t=1,2,\cdots,T$. From \eqref{dlmerrorjoint} we can write $p(V,W,\psi_{0:T}|y_{1:T})$ as
\begin{align*}
    p(&V,W,\psi_{0:T},y_{1:T}) \propto W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(Ly_t - \sqrt{V}L\psi_t)^2\right)\right]\nonumber \\
 & V^{-(\alpha_V + 1)}\exp\left[-\frac{\beta_V}{V}\right]  \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\psi_t^2\right] \exp\left[-\frac{1}{2C_0}(\psi_0-m_0)^2 \right] 
\end{align*}
where we define $Ly_t=y_t-y_{t-1}$ for $t=2,3,\cdots,T$ \& $Ly_1=y_1 - \psi_0$ and $L\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$ \& $L\psi_1=\psi_1-0$. Once again, $V$ and $W$ are no longer conditionally independent given $\psi_{0:T}$ and $y_{1:T}$. In fact, the density is analgous to \eqref{llmdistpost} with $V$ and $W$ switching places. The scaled error sampler obtained from drawing $V$ and $W$ separately is:
\begin{alg}[Scaled Error Sampler for LLM]\label{llmerroralg}
  \begin{align*}
    [\psi_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to[V^{(k+1)}|W^{(k)},\psi_{0:T},y_{1:T}]\to [W^{(k+1)}|V^{(k+1)},\psi_{0:T},y_{1:T}]
    \end{align*}
\end{alg}
In step 3, $W$ is drawn from the same inverse gamma distribution as in step 2 of algorithm \ref{llmstatealg}. Drawing $V$ in step 2 is more complicated, but exactly analogous to drawing $W$ in algorithm \ref{llmdistalg}. The log density of $V|W,\psi_{0:T},y_{1:T}$ can be written as
\begin{align*}
\log p(V|W,\psi_{0:T},y_{1:T}) =& -aV + b\sqrt{V} - (\alpha_V + 1)\log V -\beta_V/V + C 
\end{align*}
where again $C$ is some constant, but now $a=\sum_{t=1}^T(L\psi_t)^2/2W$ and $b=\sum_{t=1}^T(L\psi_tLy_t)/W$ but otherwise the form of the density is the same as that of $W|V,\gamma_{0:T},y_{1:T}$. 

We can also contruct the DA algorithms based on the ``wrongly scaled'' disturbances or errors. The wrongly scaled disturbances are defined by $\tilde{\gamma}_t = \gamma_t\frac{\sqrt{W}}{\sqrt{V}}$ for $t=1,2,\cdots,T$ and $\tilde{\gamma}_0=\gamma_0$ while the wrongly scaled errors are defined by $\tilde{\psi}_t = \psi_t\frac{\sqrt{V}}{\sqrt{W}}$ for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\psi_0$. For $\tilde{\gamma}_{0:T}$ we have
\begin{align*}
  p(V,W&|\tilde{\gamma}_{0:T},y_{1:T}) \propto W^{-\alpha_W - T/2 - 1}\exp\left[-\frac{1}{2W/V}\sum_{t=1}^T\tilde{\gamma}_t^2\right]\exp\left[-\frac{\beta_W}{W}\right]\\
  &\times V^{-\alpha_V-1}\exp\left[-\frac{\beta_V}{V}\right]\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \tilde{\gamma}_0 - \sqrt{V}\sum_{s=1}^t\tilde{\gamma}_s\right)^2\right].
\end{align*}
Thus the conditional posterior of $W$ given $V$ and $\tilde{\gamma}_{0:T}$ is the same as if we had conditioned on $\theta_{0:T}$ instead of $\tilde{\gamma}_{0:T}$. In other words
\begin{align*}
  p(W|V,\tilde{\gamma}_{0:T},y_{1:T}) \propto W^{-(\alpha_W + T/2) - 1}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}V\sum_{t=1}^T\tilde{\gamma}_t^2\right)\right]
\end{align*}
so that $V|W,\tilde{\gamma}_{0:T},y_{1:T}\sim IG(a_W, b_W)$ where $a_W = \alpha_W + T/2$ and 
\begin{align*}
  b_W = \beta_W + \frac{1}{2}V\sum_{t=1}^T\tilde{\gamma}_t^2 = \beta_W + \frac{1}{2}\sum_{t=1}^T(\theta_t - \theta_{t-1})^2.
\end{align*}
The conditional posterior of $V$ is more complicated. We have
\begin{align*}
  p(V|W,\tilde{\gamma}_{0:T},y_{1:T}) &\propto \exp\left[-\frac{1}{2W/V}\sum_{t=1}^T\tilde{\gamma}_t^2\right] V^{-\alpha_V-1}\exp\left[-\frac{\beta_V}{V}\right]\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \tilde{\gamma}_0 - \sqrt{V}\sum_{s=1}^t\tilde{\gamma}_s\right)^2\right]\\
  &\propto V^{-\alpha_V - 1}\exp\left[-\frac{a}{V} + \frac{b}{\sqrt{V}} - cV\right]
\end{align*}
where 
\begin{align*}
  a & = \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\gamma}_0)^2 > 0\\
  b & = \sum_{t=1}^T(y_t - \tilde{\gamma}_0)\sum_{s=1}^t\tilde{\gamma}_s\\
  c & = \frac{1}{2W}\sum_{t=1}^T\tilde{\gamma}_t^2 > 0.
\end{align*}
We will return to this density momentarily.

For the wrongly scaled errors, we have
\begin{align*}
  p(V,W&|\tilde{\psi}_{0:T},y_{1:T}) \propto V^{-\alpha_V - T/2 -1}\exp\left[-\frac{1}{2V/W}\sum_{t=1}^T\tilde{\psi}_t^2\right]\exp\left[-\frac{\beta_V}{V}\right]\\
  &\times W^{-\alpha_W-1}\exp\left[-\frac{1}{2W}\sum_{t=1}^T\left(\tilde{Ly}_t - \sqrt{W}(\tilde{L\psi}_t)\right)\right]
\end{align*}
where we define $\tilde{Ly}_t = y_t - y_{t-1}$ for $t=1,2,\cdots,T$ and $\tilde{Ly}_1=y_1 - \tilde{\psi}_0$, and $\tilde{L\psi}_t=\tilde{\psi}_t-\tilde{\psi}_{t-1}$ for $t=1,2,\cdots,T$ with $\tilde{L\psi}_1=\tilde{\psi}_1$. Then the conditional posterior of $V$ is the same as if we had conditioned on $\theta_{0:T}$ instead of $\tilde{\psi}_{0:T}$, i.e.
\begin{align*}
  p(V|W,\tilde{\psi}_{0:T},y_{1:T}) & \propto V^{-(\alpha_V - T/2)-1}\exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}W\sum_{t=1}^T\tilde{\psi}_t^2\right)\right]
\end{align*}
so that $V|W,\tilde{\psi}_{0:T},y_{1:T}\sim IG(a_V, b_V)$ where $a_V = \alpha_V + T/2$ and
\begin{align*}
  b_V = \beta_V + \frac{1}{2}W\sum_{t=1}^T\tilde{\psi}_t^2 = \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \theta_t)^2.
\end{align*}
The conditional posterior of $W$ is more complicated but similar to that of $V$ when we conditioned on $\tilde{\gamma}_{0:T}$. We have
\begin{align*}
  p(W|V,\tilde{\psi}_{0:T},y_{1:T}) &\propto \exp\left[-\frac{1}{2V/W}\sum_{t=1}^T\tilde{\psi}_t^2\right] W^{-\alpha_W-1}\exp\left[-\frac{1}{2W}\sum_{t=1}^T\left(\tilde{Ly} - \sqrt{W}\tilde{L\psi}\right)\right]\\
  &\propto W^{-\alpha_W - 1}\exp\left[-\frac{a}{W} + \frac{b}{\sqrt{W}} - cW\right]
\end{align*}
where now
\begin{align*}
  a & = \beta_W + \frac{1}{2}\sum_{t=1}^T\tilde{Ly}_t^2 > 0\\
  b & = \sum_{t=1}^T\tilde{Ly}_t\tilde{L\psi}_t\\
  c & = \frac{1}{2V}\sum_{t=1}^T\tilde{\psi}_t^2 > 0.
\end{align*}

So in the case of both wrongly scaled DAs we need to sample from a density of the form
\begin{align*}
  p(X) \propto X^{-\alpha -1}\exp\left[-\frac{a}{X} + \frac{b}{\sqrt{X}} - cX\right].
\end{align*}
The density of $Y=\log(X)$ is 
\begin{align*}
  p(Y) \propto \exp\left[-\alpha Y - ae^{-Y} + be^{-Y/2} - ce^Y\right].
\end{align*}
This density is easy to sample from fairly efficiently with rejection sampler using a $t$ or normal approximation as a proposal. It is also typically log concave, so adaptive rejection sampling will work as well. In particular when $b\leq 0$ or $a > \frac{3b}{16}\left(\frac{b}{16c}\right)^{1/3}$ the density of $Y$ is log concave.

\subsection{Hybrid Samplers: Interweaving, Alternating and Random Kernel}
Section \ref{sec:DLMinter} contains the details for the interweaving algorithms in the general DLM. In the local level model the only difference is that we only sample $V$ and $W$ jointly when we condition on the states. We will consider all four GIS samplers based on any two or three of the base samplers and one CIS sampler. In the GIS samplers, the order of the parameterizations will always be the states $(\theta_{0:T})$, then the scaled disturbances $(\gamma_{0:T})$, then the scaled errors $(\psi_{0:T})$. All of the GIS algorithms and the full CIS algorithm are below in Table \ref{GISalgorithms}. Note the distributional forms for each of these steps (in some cases a transformation) are in Section \ref{sec:LLMbase}. We ignore the partial CIS algorithm
\begin{table}[!h]
  \centering
\begin{enumerate}
  \item state-dist GIS algorithm:\\
    $[\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V,W|\theta_{0:T},y_{1:T}] \to [\gamma_{0:T}|V,W,\theta_{0:T},y_{1:T}] \to [V^{(k+1)}|W,\gamma_{0:T},y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\gamma_{0:T},y_{1:T}]$
  \item state-error GIS algorithm:\\
    $[\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V,W|\theta_{0:T},y_{1:T}] \to [\psi_{0:T}|V,W,\theta_{0:T},y_{1:T}] \to [V^{(k+1)}|W,\psi_{0:T},y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\psi_{0:T},y_{1:T}]$
  \item dist-error GIS algorithm:\\
    $[\gamma_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V|W^{(k)}, \gamma_{0:T,}y_{1:T}] \to [W|V, \gamma_{0:T},y_{1:T}] \to [\psi_{0:T}|V,W,\gamma_{0:T},y_{1:T}] \to [V^{(k+1)}|W,\psi_{0:T},y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\psi_{0:T},y_{1:T}]$
  \item triple GIS algorithm:\\
    $[\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V,W|\theta_{0:T},y_{1:T}] \to [\gamma_{0:T}|V,W,\theta_{0:T},y_{1:T}] \to [V|W,\gamma_{0:T},y_{1:T}] \to [W|V,\gamma_{0:T},y_{1:T}] \to [\psi_{0:T}|V,W,\gamma_{0:T},y_{1:T}] \to [V^{(k+1)}|W,\psi_{0:T},y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\psi_{0:T},y_{1:T}]$
  \item full CIS algorithm:\\
        $[\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V|W^{(k)},\theta_{0:T},y_{1:T}] \to [\psi_{0:T}|V,W,\theta_{0:T},y_{1:T}] \to [V^{(k+1)}|W,\psi_{0:T},y_{1:T}] \to [\theta_{0:T}|V^{(k+1)},W,y_{1:T}] \to [W|V^{(k+1)},\theta_{0:T},y_{1:T}] \to [\gamma_{0:T}|V^{(k+1)},W,y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\gamma_{0:T},y_{1:T}]$
\end{enumerate}
\caption{GIS and CIS algorithms for the local level model}
\label{GISalgorithms}
\end{table}

Interweaving algorithms are conceptually very similar to alternating algorithms. For every GIS algorithm, there's a corresponding alternating algorithm  where each $[DA_2|V,W,DA_1]$ step is replaced by a $[DA_2|V,W]$ step (here $DA_i$ is a data augmentation for $i=1,2$.). Table \ref{altalgorithms} contains each alternating algorithm. Note that there are two possible ``hybrid triple'' algorithms that we don't consider here where the move from $\theta_{0:T}$ to $\gamma_{0:T}$ interweaves and while the move from $\gamma_{0:T}$ to $\psi_{0:T}$ alternates and vice versa.
\begin{table}[!h]
  \centering
\begin{enumerate}
  \item State-Dist alternating algorithm:\\
    $[\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V,W|\theta_{0:T},y_{1:T}] \to [\gamma_{0:T}|V,W,y_{1:T}] \to [V^{(k+1)}|W,\gamma_{0:T},y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\gamma_{0:T},y_{1:T}]$
  \item State-Error alternating GIS algorithm:\\
    $[\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V,W|\theta_{0:T},y_{1:T}] \to [\psi_{0:T}|V,W,y_{1:T}] \to [V^{(k+1)}|W,\psi_{0:T},y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\psi_{0:T},y_{1:T}]$
  \item Dist-Error alternating GIS algorithm:\\
    $[\gamma_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V|W, \gamma_{0:T},y_{1:T}] \to [W|V, \gamma_{0:T},y_{1:T}] \to [\psi_{0:T}|V,W,y_{1:T}] \to [V^{(k+1)}|W,\psi_{0:T},y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\psi_{0:T},y_{1:T}]$
  \item Triple alternating GIS algorithm:\\
    $[\theta_{0:T}|V^{(k)},W^{(k)},y_{1:T}]\to [V,W|\theta_{0:T},y_{1:T}] \to [\gamma_{0:T}|V,W,y_{1:T}] \to [V|W,\gamma_{0:T},y_{1:T}] \to [W|V,\gamma_{0:T},y_{1:T}] \to [\psi_{0:T}|V,W,y_{1:T}] \to [V^{(k+1)}|W,\psi_{0:T},y_{1:T}] \to [W^{(k+1)}|V^{(k+1)},\psi_{0:T},y_{1:T}]$
\end{enumerate}
\caption{Alternating algorithms for the local level model}
\label{altalgorithms}
\end{table}

Table \ref{LLMalgorithms} contains each algorithm we considered for the local level model. The basic idea here is that the alternating algorithms should serve as a sort of baseline to compare the corresponding interweaving algorithms against. The GIS algorithm should be slightly faster than the alternating algorithm since the only difference is one step becoming a transformation instead of a random draw, but the difference should not be large since there is no reason to expect the scaled disturbances and the scaled errors to have low to zero dependence in the posterior. So we would like the GIS algorithms to have at least as quick mixing as the corresponding alternating algorithms. We can make this notion precise by considering the effective sample size (ESS) of the Markov chain -- we would like the GIS algorithms to have an ESS that is larger than their corresponding alternating algorithms for the same actual sample size. We omit the partial CIS algorithm from our results because, as expected, it does not perform materially different from the state-dist algorithm.

\begin{table}[!h]
  \centering
  \begin{tabular}{|l||l|l|l|l|}\hline
    Base & State & (wrongly) Scaled Disturbance & (wrongly) Scaled Error & \\\hline
    GIS & State-Dist & State-Error & Dist-Error & Triple \\
    Alt & State-Dist & State-Error & Dist-Error & Triple \\
    CIS & \multicolumn{3}{l|}{State-Error/WError-Error for $V|W$; State-Dist/WDist-Dist for $W|V$} & \\
    \hline
  \end{tabular}
  \caption{Each algorithm considered for the local level model}
  \label{LLMalgorithms}
\end{table}

\subsection{Simulation Setup}

In order to test these algorithms, we simulated a fake dataset from the local level model for various choices of $V$, $W$, and $T$. We created a grid over $V$--$W$ space with $(V,W)$ ranging from $(10^{-2},10^{-2})$ to $(10^2, 10^2)$ and we simulated a dataset for all possible combinations of $V$ and $W$ with each of $T=10, 100, 1000$. Then for each dataset, we fit the local level model using each algorithm in Table \ref{LLMalgorithms}. We used the same rule for constructing priors for each model: $\theta_0\sim N(0,10^7)$, $V\sim IG(5, 4\tilde{V})$, and $W\sim IG(5, 4\tilde{W})$, mutually independent where $(\tilde{V},\tilde{W})$ are the true values of $V$ and $W$ used to simulate the time series. Thus both the prior and likelihood roughly agree about the likely values of $V$ and $W$. 

For each dataset and each sampler, we obtained $n=3000$ draws and threw away the first $500$ as burn in. The chains were started at the true values used to simulated the time series, so we can examine the behavior of the chains to determine how well they mix but not how quickly they converge. Define the effective sample proportion (ESP) for a scalar component of the chain as the effective number of independent draws, or effective sample size (ESS) (see e.g. \citet{gelman2003bayesian}) of the component divided by the actual sample size, i.e. $ESP=ESS/n$. An $ESP=1$ indicates that the Markov chain is behaving as if it obtains iid draws from the posterior. It is possible to obtain $ESP>1$ if the draws are negatively correlated and occasionally for some of our samplers our estimates of $ESS$ are negative, but we round this up to $0$ so that the maximum $ESP$ possible is $1$ in our plots.

\subsection{Base Results}

<<plotsetup, echo=FALSE, message=FALSE>>=
library(ggplot2)
library(scales)
library(plyr)
library(xtable)
library(reshape2)
library(gridExtra)
load("../../mixing/samout.RData")
samout2 <- samout
load("../../das/OldDAs/samout.RData")
samout$stime <- 0
samout <- rbind(samout2, samout)
rm(samout2)
base <- c("error", "state", "dist")
alts <- c("sdalt", "sealt", "dealt", "trialt")
ints <- c("sdint", "seint", "deint", "triint")
kerns <- c("sdkern", "sekern", "dekern", "trikern")
cis <- c("fullcis", "partialcis")
wrongs <- c("errorda", "distda")
samout$V.ES[samout$sampler %in% kerns] <- samout$V.ES[samout$sampler %in% kerns]*2
samout$W.ES[samout$sampler %in% kerns] <- samout$W.ES[samout$sampler %in% kerns]*2
samout$V.ES[samout$sampler == "trikern"] <- samout$V.ES[samout$sampler == "trikern"]*(3/2)
samout$W.ES[samout$sampler == "trikern"] <- samout$W.ES[samout$sampler == "trikern"]*(3/2)
samout$type <- "Base" #$
samout$type[samout$sampler %in% alts] <- "Alt" 
samout$type[samout$sampler %in% ints] <- "GIS" 
samout$type[samout$sampler %in% kerns] <- "RKern" 
samout$type[samout$sampler %in% cis] <- "CIS" 
samout$type[samout$sampler %in% wrongs] <- "W-Base" 
samout$samplers <- "Base"
samout$samplers[substr(samout$sampler, 1, 2)=="sd"] <- "State-Dist" 
samout$samplers[substr(samout$sampler, 1, 2)=="se"] <- "State-Error" 
samout$samplers[substr(samout$sampler, 1, 2)=="de"] <- "Dist-Error" 
samout$samplers[substr(samout$sampler, 1, 3)=="tri"] <- "Triple" 
samout$samplers[samout$sampler=="fullcis"] <- "FullCIS"
samout$samplers[samout$sampler=="partialcis"] <- "PartialCIS"
samout$samplers[samout$sampler=="error"] <- "Error"
samout$samplers[samout$sampler=="dist"] <- "Dist"
samout$samplers[samout$sampler=="state"] <- "State"
samout$samplers[samout$sampler=="errorda"] <- "W-Error"
samout$samplers[samout$sampler=="distda"] <- "W-Dist"
samlevels <- c("State", "Dist", "Error", "State-Dist", "State-Error", "Dist-Error", 
               "Triple", "FullCIS", "PartialCIS", "W-Dist", "W-Error")
samout$samplers <- factor(samout$samplers, levels=samlevels)
samout$V.time <- samout$time/samout$V.ES
samout$W.time <- samout$time/samout$W.ES
meltedsam <- melt(samout, id=c("type", "samplers", "sampler", "V.T", "W.T", 
                            "T.T"))
Vs <- unique(meltedsam$V.T)[1:9] #$
Ws <- Vs
breaks <- Vs[seq(1,9,2)]
label_both_parsed <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), function(x) parse(text = x))
}
label_both_parsed_split <- function(variable, value){
  llply(as.character(paste(variable, value, sep = ": ")), 
        function(x) parse(text = strsplit(x, "\\.")[[1]][1]))
}
label_parsed_split <- function(variable, value){
  llply(as.character(value), function(x) parse(text = strsplit(x, "\\.")[[1]][1]))
}
plotfun <- function(meltedsam, vars, sams, T, title){
  castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable + samplers ~ ., 
                     subset=.(variable %in% vars  & T.T==T & sampler %in% sams &
                       V.T<=10^2 & W.T<=10^2))
  colnames(castedsam)[6] <- "value"
  out <- ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value/2500)) + #$
         geom_tile() +
         scale_fill_gradient("ESP", low=muted("red"), high="white",
           guide=guide_colorbar(barheight=10),
           limits=c(0,1), na.value="white") +
         facet_grid(variable~samplers, scales="free", labeller=label_parsed_split) +
         scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
         ggtitle(paste(title, T, sep="")) +
         theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
  return(out)
}
load("../../cors/newpostcors.RData")
newpostcors <- newpostcors[newpostcors$V.T <= 10^2 & newpostcors$W.T <= 10^2,]
plotfuntime <- function(meltedsam, vars, sams, T, title, top){
  castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable + samplers ~ ., 
                     subset=.(variable %in% vars  & T.T==T & sampler %in% sams &
                       V.T<=10^2 & W.T<=10^2))
  colnames(castedsam)[6] <- "value"
  out <- ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value*1000/60)) + #$
         geom_tile() +
         scale_fill_gradient("Time/ESS*1000", high=muted("red"), low="white",
           guide=guide_colorbar(barheight=10), na.value="red", limits=c(0,top)) +
         facet_grid(variable~samplers, scales="free", labeller=label_parsed_split) +
         scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
         ggtitle(paste(title, T, sep="")) +
         theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
  return(out)
}
plotfuncor <- function(newpostcors, var, title){
  dat <- newpostcors
  id <- which(colnames(newpostcors)==var)
  colnames(dat)[id] <- "value"
  out <- ggplot(data=dat, aes(x=V.T, y=W.T, fill=value)) +
      geom_tile() +
      scale_fill_gradient2("Corr", low=muted("blue"), high=muted("red"),
         limits=c(-1,1), mid="white") +
      facet_grid(.~T, scales="free", labeller=label_both) +
      scale_x_log10("V = noise", breaks=breaks) +
      scale_y_log10("W = signal", breaks=breaks) +
      ggtitle(title) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
  return(out)
}
@ 

Figure \ref{baseESplot} contains plots of ESP for $V$ and $W$ in each chain of each base sampler for each of $T=10$, $T=100$, and $T=1000$. We will focus on $T=10$ first. The state sampler has a low ESP for $V$ and a high ESP for $W$ when the signal-to-noise ratio, $W/V$, is larger than one. When the signal-to-noise ratio is smaller than one, on the other hand, the state sampler has a low ESP for $W$ and a high ESP for $V$. In the typical case where the signal to noise ratio close to one, the state sampler has a modest to low ESP for both $V$ and $W$. Note that the particular values of $V$ and $W$ do not seem to matter at all --- just their relative values, i.e. the signal-to-noise ratio $W/V$. Moving up any diagonal on the plots for $V$ and $W$ in the state sampler, $W/V$ is constant and the ESS appears roughly constant. The basic lesson here is that the state sampler has mixing issues for whichever of $V$ or $W$ is smaller. 

Figure \ref{baseESplot} tells a different story for the scaled disturbance sampler. When the signal-to-noise ratio is less than one, ESPs for both $V$ and $W$ are nearly 1, i.e. the effective sample size is nearly the actual sample size of the chain. When the signal-to-noise ratio is greater than one, however, ESP for both $V$ and $W$ becomes small, especially for $V$. Once again the absolute values of $V$ and $W$ do not matter for this behavior --- just the relative values. The scaled error sampler has essentially the opposite properties. When $W/V$ is large, it has a near 1 ESP for both $V$ and $W$. On the other hand, when $W/V$ is small is has a low ESP for both $V$ and $W$, especially for $V$. The lesson here seems to be that the scaled disturbances are the preferred data augmentation for low signal-to-noise ratios and the scaled errors  are the preferred data augmentation for high signal-to-noise ratios, while the states are preferred for signal-to-noise ratios near 1. The wrongly scaled disturbances ($\tilde{\gamma}_{0:T}$) and wrongly scaled errors ($\tilde{\psi}_{0:T}$), on the other hand, look like worse versions of the state sampler. The pattern of mixing for $V$ and $W$ over the range of the parameter space is essentially the same as the state sampler, except the wrongly scaled disturbance sampler has worse mixing for $V$ than the state sampler everywhere and similarly the wrongly scaled error sampler has worse mixing for $W$ than the state sampler everywhere.

The plots for $T=100$ and $T=1000$ in Figure \ref{baseESplot} tell basically the same story, with a twist. Increasing the length of the time series seems to exacerbate all problems without changing the basic conclusions. As $T$ increases, $W/V$ has to be smaller and smaller for the scaled disturbance sampler to have decent mixing, and similarly $W/V$ has to be larger and larger for the scaled error sampler to have decent mixing. Interestingly, the scaled error sampler appears to mix well for both $V$ and $W$ over a larger region of the space $W/V<1$ than the scaled disturbance sampler does over $W/V>1$. The state sampler is stuck between a rock and a hard place, so to speak, since as $T$ increases, good mixing for $V$ requires $W/V$ to be smaller and smaller, but good mixing for $W$ requires $W/V$ to be larger and larger. The wrongly scaled samplers are again pretty similar to the state sampler for larger $T$ except the wrongly scaled sampler tends to be worse everywhere for the variance that was used to scale --- i.e. once again the wrongly scaled disturbance sampler has worse mixing for $V$ than the state sampler while the wrongly scaled error smapler has worse mixing for $W$ than the state sampler. However, the wrongly scaled samplers do appear to have slightly better mixing than the state sampler for the variance that was {\it not} used to scale. In particular, the wrongly scaled error sampler appears to have slightly better mixing for $V$ than the state sampler over part of the parameter space when $T=100$ or $T=1000$. We summarize these results while ignoring a couple of details in Table \ref{tab:stnmix}.

\begin{table}
  \centering
  \begin{tabular}{|l|ccccc|}\hline
    Parameter & State & Dist & Error & W-Dist & W-Error \\\hline
    V & $\frac{W}{V} < 1$ & $\frac{W}{V} < 1$ & $\frac{W}{V} > 1$ & $\frac{W}{V} < 1$ & $\frac{W}{V} < 1$\\
    W & $\frac{W}{V} > 1$ & $\frac{W}{V} < 1$ & $\frac{W}{V} > 1$ & $\frac{W}{V} > 1$ & $\frac{W}{V} > 1$ \\\hline
  \end{tabular}
  \label{tab:stnmix}
  \caption{Rule of thumb for when each base algorithm has a high effective sample size for each variable as a function of the signal-to-noise ratio, $W/V$.}
\end{table}

Most of the patterns of Figure \ref{baseESplot} can be explained by Figure \ref{corplot}, which contains the estimated posterior correlations between various functions of parameters estimated using the simulations from the Triple-Alternating sampler. First we need to understand the correlations we are looking at. The state sampler consists of two steps --- a draw of $\theta_{0:T}$ given $V$ and $W$, and a draw of $(V,W)$ given $\theta_{0:T}$. From Section \ref{sec:LLMbase} we have that conditional on $\theta_{0:T}$, $V$ and $W$ are independent in the posterior and each has an inverse gamma distribution that depends on the states only through the second parameter:
\begin{align*}
  b_V &= \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2\\
  b_W &= \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2.
\end{align*}
So can view $(b_V,b_W)$ as the data augmentation instead of $\theta_{0:T}$ and thus the state sampler is
\begin{align*}
  [b_V, b_W|V^{(k)},W^{(k)},y_{1:T}] \to [V^{(k+1)},W^{(k+1)}|b_V,b_W,y_{1:T}].
\end{align*}
Thus the dependence between $(V,W)$ and $(b_V,b_W)$ in the posterior will determine how much the state sampler moves in a given iteration and, in particular, the dependence in the marginal chain for $V$ is determined by the dependence between $b_V$ and $V$ while the dependence in the marginal chain for $W$ is determined by the dependence between $b_W$ and $W$. 

For the scaled disturbance sampler, things are a bit more complicated. Now the data augmentation becomes
\begin{align*}
  b_V &= \beta_V + \sum_{t=1}^T(y_t - \gamma_0 -\sqrt{W}\sum_{s=1}^t\gamma_t^2)^2/2 = \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2\\
  a_\gamma & = \sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V\\
  b_\gamma &=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V.
\end{align*}
This again comes from Section \ref{sec:LLMbase}. The draw of $V|W,\gamma_{0:T}$ is the same inverse gamma draw as in the state sampler. The draw of $W|V,\gamma_{0:T}$ depends on two random parameters, $a_\gamma$ and $b_\gamma$ defined above. So the dependence between $V$ and $b_V$ determines how much the marginal chain for $V$ chain moves in a single iteration, while the dependence between $W$ and $(a_\gamma,b_\gamma)$ determines how much the marginal chain for $W$ moves in a single iteration. The scaled error sampler is analogous to the scaled disturbance sampler except with 
\begin{align*}
  b_W &= \beta_W + \sum_{t=2}^T(Ly_t - \sqrt{V}L\psi_t)^2/2 = \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2\\
  a_\psi&=\sum_{t=1}^T(L\psi_t)^2/2W\\
  b_\psi&=\sum_{t=1}^T(L\psi_tLy_t)/W
\end{align*}
and with the dependence in marginal chain for $W$ depending on the dependence between $b_W$ and $W$ while the dependence in the marginal chain for $V$ depending on the dependence betwee $(a_\psi, b_\psi)$ and $V$.

In Figure \ref{corplot} we see that the posterior correlation between $V$ and $b_V$ is high when $W/V>1$ and is low when $W/V<1$. This explains why both the state sampler and the scaled disturbance sampler have low ESP's for $V$ when $W/V>1$. Similarly the posterior correlation between $W$ and $b_W$ is high when $W/V<1$ and is low when $W/V>1$, which explains why both the state sampler and scaled error sampler have low ESP's for $W$ when $W/V<1$. {\it Insert explanation of marginal chain for $W$ in the scaled disturbance sampler and for the marginal chain for $V$ in the scaled error sampler. Right now it doesn't look right - the correlation between $V$ and $a_\psi$ or $b_\psi$ is highest when $W/V$ is high, but for the scaled error sampler, autocorrelation in the marginal chain for $V$ is highest when $W/V$ is low.}


<<baseESplot, fig.cap=cap, echo=FALSE, fig.height=3.75, fig.width=8, out.width=".7\\textwidth">>=
cap <- "Effective sample proportion in the posterior sampler for a time series of lengths $T=10$, $T=100$, and $T=1000$, for $V$ and $W$, and for the state, scaled disturbance, scaled error, wrongly scaled disturbance, and wrongly scaleed error samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Note that for plotting purposes, effective sample proportions larger than $1$ were rounded down to $1$"
vars <- c("V.ES", "W.ES")
title <- "ESP for V and W in the base algorithms, T="
p1 <- plotfun(meltedsam, vars, c(base,wrongs), 10, title)
p2 <- plotfun(meltedsam, vars, c(base,wrongs), 100, title)
p3 <- plotfun(meltedsam, vars, c(base,wrongs), 1000, title)
p1
p2
p3
@ 

<<corplot, echo=FALSE, fig.height=3, fig.width=8, out.width=".49\\textwidth", fig.cap=cap>>=
cap <- "Posterior correlation between $V$ or $W$ and $b_V$ or $b_W$. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high."
title <- "Posterior Correlation Between V and bv"
p1 <- plotfuncor(newpostcors, "Vbv", title)
title <- "Posterior Correlation Between W and bw"
p2 <- plotfuncor(newpostcors, "Wbw", title)
title <- "Posterior Correlation Between V and apsi"
p3 <- plotfuncor(newpostcors, "Vapsi", title)
title <- "Posterior Correlation Between W and agam"
p4 <- plotfuncor(newpostcors, "Wagam", title)
title <- "Posterior Correlation Between V and bpsi"
p5 <- plotfuncor(newpostcors, "Vbpsi", title)
title <- "Posterior Correlation Between W and bgam"
p6 <- plotfuncor(newpostcors, "Wbgam", title)
p1
p2
p3
p4
p5
p6
@ 

\subsection{GIS and CIS Results}

Based on the intuition in Section \ref{sec:DLMest} above, both the GIS and alternating algorithms should work best when at least one of the underlying base algorithms has a high ESP --- the basic idea is that when least one of the underlying algorithms has low autocorrelation, we should have low autocorrelation in the GIS algrothim using multiple DAs. This suggests that the Dist-Error GIS and alternating algorithms will have the best performance of the GIS and alternating algorithms using two DAs for both $V$ and $W$, especially for $W/V$ far away from one. When $W/V$ is near one it may offer no improvement, especially for large $T$. The State-Dist algorithms should have trouble with $V$ when $W/V$ is high since both the state sampler and the scaled disturbance sampler have trouble with $V$ when $W/V$ is high. Similarly, the State-Error GIS algorithm should have trouble with $W$ when $W/V$ is low since both underlying samplers have trouble with $W$ when $W/V$ is low. Since the triple algorithms add the state sampler into the Dist-Error algorithms, it seems plausible that it might improve mixing for one of $V$ or $W$ since for $V/W$ different from one, the state sampler has good mixing for at least one of $V$ of $W$. The full CIS algorithm, on the other hand, is unlikely to be better than the Dist-Error GIS algorithm since one algorithm has the same steps as the other, just reordered.


<<intESplot, fig.cap=cap, echo=FALSE, fig.height=3.75, fig.width=8, out.width=".7\\textwidth">>=
cap <- "Effective sample proportion in the posterior sampler for $V$ and $W$ in for $T=10$, $T=100$, and $T=1000$, in all three GIS samplers based on any two of the base samplers and the full CIS sampler. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Note that for plotting purposes, effective sample proportions larger than one were rounded down to one."
vars <- c("V.ES", "W.ES")
sams <- c("deint", "seint", "sdint", "triint", "fullcis")
title <- "ESP for V and W in the GIS and CIS algorithms, T="
p1 <- plotfun(meltedsam, vars, sams, 10, title)
p2 <- plotfun(meltedsam, vars, sams, 100, title)
p3 <- plotfun(meltedsam, vars, sams, 1000, title)
p1
p2
p3
@ 

We can verify most of these intuitions in Figure \ref{baseintESplot}. First, the State-Dist GIS algorithm has high ESP for $W$ except for a narrow band where $W/V$ is near one, though this band becomes much wider as $T$ increases. The State-Dist GIS algorithm's mixing behavor for $V$ appears identical to the original state sampler --- high ESP when $W/V < 1$ and poor ESP when $W/V > 1$, and again the good region shrinks as $T$ increases. So this algorithm behaves as expected --- it takes advantage of the fact that the state and scaled disturbance DA algorithms make up a ``beauty and the beast'' pair for $W$ and thus improves mixing for $W$. However, the two underlying DA algorithms behave essentially indentically for $V$ so there is no improvement. Similarly the State-Error GIS algorithm's ESP for $W$ is essentially identical to the state and scaled error algorithms' ESP for $W$ --- high when $W/V$ large and low when $W/V$ small  --- but for $V$, the state-error algorithm has a high ESP when $W/V$ isn't too close to one, especially when $T$ is small. The Dist-Error GIS algorithm also behaves as predicted --- when $W/V$ is not too close to one it has high ESP for both $V$ and $W$, though as $T$ increases $W/V$ has too be farther away from one in order for the ESPs to be high. The Dist-Error GIS algorithm behaves apparently identically to the full CIS and triple GIS algorithms, with some differences when $T$ is small. The first of these is not surprising --- based on the intuition that the dist-error GIS and full CIS algorithms are the same up to a reordering of each of their steps, we didn't expect much of a difference. However, we had some hope that the triple GIS algorithm would improve upon the dist-error GIS algorithm somewhat by further breaking the correlation between iterations in the Markov chain. This did not happen, and furthermore the State-Dist and State-Error samplers did not improve the ESP for $V$ or $W$ respectively. When the two underlying DA algorithms form a ``beast and the beast'' pair, the interweaving algorithm appears to mix just as well as the best mixing single DA algorithm. In Figure \ref{hybridESplot} allows us to compare the GIS algorithms to the alternating algorithms.  The main takeaway is that there does not appear to be much difference between the mixing properties of the interweaving and alternating versions of a given algorithm.

<<altESplot, fig.cap=cap, echo=FALSE, fig.height=3.75, fig.width=7, out.width=".7\\textwidth">>=
cap <- "Effective sample proportion in the posterior sampler for $V$ and $W$ in for $T=10$, $T=100$, and $T=1000$, in all four alternating samplers. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Note that for plotting purposes, effective sample proportions larger than one were rounded down to one."
vars <- c("V.ES", "W.ES")
sams <- c(ints)
title <- "ESP for V and W in the alternating algorithms, T="
p1 <- plotfun(meltedsam, vars, sams, 10, title)
p2 <- plotfun(meltedsam, vars, sams, 100, title)
p3 <- plotfun(meltedsam, vars, sams, 1000, title)
p1
p2
p3
@ 

\subsection{Computational time}

<<baseintESDplot, fig.cap=cap, echo=FALSE, fig.width=10, fig.height=3.25, out.width=".8\\textwidth">>=
cap <- "Time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=10$, $T=100$, and $T=1000$, in the state, scaled disturbance and scaled error samplers and for all four GIS samplers based on any two or three of these. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. The signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. For plotting purposes, times larger than the top of the scale are displayed in bright red."
vars <- c("V.time", "W.time")
sams <- c("dist", "error", "deint", "state", "seint", "sdint", "triint", "fullcis")
title <- "Time (minutes) per 1000 effective draws, T="
p1 <- plotfuntime(meltedsam, vars, sams, 10, title, .006*1000/60)
p2 <- plotfuntime(meltedsam, vars, sams, 100, title, .06*1000/60)
p3 <- plotfuntime(meltedsam, vars, sams, 1000, title, 1.2*1000/60)
p1
p2
p3
@ 


<<hybridESDplot, fig.cap=cap, echo=FALSE, fig.width=8, fig.height=3.75, out.width=".49\\textwidth">>=
cap <- "Time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=10$, $T=100$, and $T=1000$, in the alternating, GIS, and random kernel samplers. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. The signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. For plotting purposes, times larger than the top of the scale are displayed in bright red."
sams <- c(alts,ints)
title <- "Time (minutes) per 1000 effective draws, T="
p1 <- plotfuntime(meltedsam, vars, sams, 10, title, .006*1000/60)
p2 <- plotfuntime(meltedsam, vars, sams, 100, title, .06*1000/60)
p3 <- plotfuntime(meltedsam, vars, sams, 1000, title, 1.2*1000/60)
p1
p2
p3
@ 










%plotfuntime2 <- function(meltedsam, vars, sams, T, title, top){
%  castedsam <- dcast(meltedsam, formula=sampler + V.T + W.T + variable + samplers ~ ., 
%                     subset=.(variable %in% vars  & T.T==T & sampler %in% sams &
%                       V.T<=10^2 & W.T<=10^2))
%  colnames(castedsam)[6] <- "value"
%  out <- ggplot(data=castedsam, aes(x=V.T, y=W.T, fill=value))+ #$
%         geom_tile() +
%         scale_fill_gradient2("Ratio of time/ESS", high=muted("red"), mid="white", 
%                             low=muted("blue"), midpoint=1,
%                              guide=guide_colorbar(barheight=10)) +
%         facet_grid(variable~samplers, scales="free", labeller=label_parsed_split) +
%         scale_x_log10("V = noise", breaks=breaks) + scale_y_log10("W = signal", breaks=breaks) +
%         ggtitle(paste(title, T, sep="")) +
%         theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))
%  return(out)
%}
%
%
%altout <- samout[samout$sampler=="dealt", ]
%intout <- samout[samout$sampler=="deint", ]
%altout$V.rat <- altout$V.time/intout$V.time
%altout$W.rat <- altout$W.time/intout$W.time
%meltedalt <- melt(altout, id=c("type", "samplers", "sampler", "V.T", "W.T", 
%                            "T.T"))
%vars <- c("V.rat", "W.rat")
%sams <- "dealt"
%p1 <- plotfuntime2(meltedalt, vars, sams, 10, title)
%p2 <- plotfuntime2(meltedalt, vars, sams, 100, title)
%p3 <- plotfuntime2(meltedalt, vars, sams, 1000, title)
%p1
%p2
%p3
%altmat <- matrix(altout$V.ES/altout$time, ncol=11, nrow=11, byrow=TRUE)
%colnames(altmat) <- round(unique(altout$W.T), 3)
%rownames(altmat) <- round(unique(altout$V.T), 3)
%intmat <- matrix(intout$W.ES/intout$time, ncol=11, nrow=11, byrow=TRUE)
%colnames(intmat) <- round(unique(intout$W.T), 3)
%rownames(intmat) <- round(unique(intout$V.T), 3)
%altmat/intmat
