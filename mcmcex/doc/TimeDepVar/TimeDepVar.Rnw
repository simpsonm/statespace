<<set-parent-DLMest, echo=FALSE, cache=FALSE>>=
set_parent('../mcmcex.Rnw')
@

This document contains the math of time-dependent variances that I already worked out but am purging from the main document for now. Only the stuff that is mildy or more complicated in the time-dependent case is included here.

\subsection{The Scaled Disturbances}

\subsection{The Scaled Errors}
where we define $\mu_t =L_{V_t}\psi_t + F_tG_tF_{t-1}(y_{t-1} - L_{V_{t-1}}\psi_{t-1})$, $y_0=0$, $L_{V_0}=I_k$, and $F_0=I_k$ where $I_k$ is the $k\times k$ identity matrix. 

\subsection{Conditionally conjugate priors and the choice of DA}
After choosing the priors for $\theta_0$, $V$ and $W$ we motivated the choice by appealing to conditional conjugacy and thus computation. If this is our main concern for choosing a prior,  it's worth asking what the conditional conjugate priors are under the scaled disturbances and the scaled errors. We'll look closely at the scaled disturbances, but the scaled errors are analogous. Based on \eqref{dlmdistmodel} we can write the augmented data likelihood as
\begin{align*}
  p(y_{1:T}|\gamma_{0:T}, V_{1:T}, W_{1:T}) \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\gamma_t'\gamma_t\right]\prod_{t=1}^T|V_t|^{-1/2}\exp\left[-\frac{1}{2}\left(y_t - F_t\theta_t(\gamma_{0:T},W_{1:T})\right)'V_t^{-1}\left(y_t - F_t\theta_t(\gamma_{0:T},W_{1:T})\right)\right].
\end{align*}
Immediately we see that the conjugate prior for $V_t$ is inverse Wishart, so no change on that front. For $W_t$ on the other hand, it's unclear until we unpack $\theta_t(\gamma_{0:T},W_{1:T})$. Recall that in our definition of the scaled disturbances for $t=1,2,\cdots,T$, $\gamma_t = L_t^{-1}w_t = L_t^{-1}(\theta_t - G_t\theta_{t-1})$ where $L_t'L_t = W_t$ while $\gamma_0=\theta_0$. The reverse transformation is thus the recursion $\theta_t = L_t\gamma_t + G_t\theta_{t-1}$ for $t=1,2,\cdots,T$. This implies that for $t=0,1,\cdots,T$
\begin{align*}
  \theta_t = \sum_{s=1}^t\left(\prod_{r=s+1}^tG_r\right)L_s\gamma_s + \prod_{r=1}^tG_r\gamma_0
\end{align*}
where for $s+1>t$, $\prod_{r=s+1}^tG_r = I_k$ the $k\times k$ identity matrix. Now recall that $K_t'K_t = V_t$ and let $H_{st} = \prod_{r=s+1}^tG_r$. This allows us to write the full conditional distribution of $W_{1:T}$, ignoring the prior, as
\begin{align*}
  p(W_{1:T}&|\gamma_{0:T},\cdots)  \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(b_t \sum_{s=1}^tB_{st}L_s\gamma_s\right)'\left(b_t - \sum_{s=1}^tB_{st}L_s\gamma_s\right)\right]
\end{align*}
where we define $b_t = K_t^{-1}(y_t - F_tH_{0t}\gamma_0)$ and $B_{st} = K_t^{-1}F_tH_{st}$. This doesn't look like it has any conjugate form for $W_t$, but it looks like a normal kernel for $L_t\gamma_t$ where $L_t$ is the Cholesky decomposition of $W_t$. It is not immediately clear whether $L_t$ will have a normal distribution, but this turns out to be true. If we follow the approach of \cite{fruhwirth2008bayesian} and vectorize $L_t$ by stacking the nonzero elements of each column on top of each other, we will see that the conditionally conjugate prior for the nonzero elements of each of the $L_t$'s is a normal distribution.

In order to show this, we first have to introduce a bit of notation. Let $A=(a_{ij})$ be a $p\times q$ matrix. First we define the vectorization of $A$, $\vect(A)$, as the $pq\times 1$ column vector obtained by stacking the columns of $A$ on top of each other. If $p=q$ then we define the half vectorization of $A$, $\vech(A)$, as the $p(p+1)/2\times 1$ column vector obtained by first deleting the elements of $A$ above the main diagonal, then stacking the remaining elements on top of each other column by column. Formally,
\begin{align*}
  \vect(A) & = (a_{11}, a_{21}, \cdots, a_{p1}, a_{12}, a_{22}, \cdots a_{p2}, \cdots, a_{pq})'\\
  \vech(A) & = (a_{11}, a_{21}, \cdots, a_{p1}, a_{22}, a_{32}, \cdots a_{p2}, \cdots, a_{pp})'.
\end{align*}
In particular if $a$ is a column vector, $\vect(a)=a$ and $\vect(a')=\vect(a)'$. Suppose $p=q$ so that $A$ is square. Then there exists a unique $p(p+1)/2\times p^2$ matrix $S_p$ such that $\vech(A) = S_p\vect(A)$, called the elimination matrix. We list a few of the properties of $S_p$ here, but see \cite{magnus1980elimination} or \cite{magnus1988linear} for more details.
\begin{enumerate}
  \item $S_pS_p'=I_{p(p+1)/2}$
  \item If $A$ is lower triangular $\vect(A) = S_p'\vech(A) = S_p'S_p\vect(A)$.
  \item $S_p = \sum_{i\geq j}^pu_{ij}\vect(E_{ij})'$ where $E_{ij}$ is a $p\times p$ matrix of zeroes except for a one in the $ij$'th spot and $u_{ij}$ is a $p(p+1)/2\times 1$ column vector of zeroes except for a one in the $[(j-1)p + i - j(j-1)/2]$'th spot.
\end{enumerate}

Now since we can assume the Jacobian from $W_t\to L_t$ will be absorbed into the prior, which we are ignoring, we have
\begin{align*}
  p(L_{1:T}&|\gamma_{0:T},\cdots) \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(b_t - \sum_{s=1}^tB_{st}L_s\gamma_s\right)'\left(b_t - \sum_{s=1}^tB_{st}L_s\gamma_s\right)\right]\\
& \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(b_t - \sum_{s=1}^tB_{st}\vect(L_s\gamma_s)\right)'\left(b_t - \sum_{s=1}^tB_{st}\vect(L_s\gamma_s)\right)\right]\\
  & \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(b_t - \sum_{s=1}^tB_{st}(\gamma_s'\otimes I_p)\vect(L_s))\right)'\left(b_t - \sum_{s=1}^tB_{st}(\gamma_s'\otimes I_p)\vect(L_s)\right)\right]\\
  & \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(\sum_{s=1}^t\sum_{r=1}^t\vect(L_s)(\gamma_s'\otimes I_p)'B_{st}'B_{rt}(\gamma_r'\otimes I_p)\vect(L_r) - 2\sum_{s=1}^tb_t'B_{st}(\gamma_s'\otimes I_p)\vect(L_s)\right)\right]\\
  & \propto \exp\left[-\frac{1}{2}\left(\sum_{s=1}^T\sum_{r=1}^T\vect(L_s)'(\gamma_s\gamma_r'\otimes C_{sr})\vect(L_r) - 2\sum_{s=1}^Tc_s'(\gamma_s'\otimes I_p)\vect(L_s)\right)\right]\\
  & \propto \exp\left[-\frac{1}{2}\left(\sum_{s=1}^T\sum_{r=1}^T\vech(L_s)'S_p(\gamma_s\gamma_r'\otimes C_{sr}) S_p'\vech(L_r) - 2\sum_{s=1}^Tc_s'(\gamma_s'\otimes I_p)S_p'\vech(L_s)\right)\right]
\end{align*}
where we define $C_{sr} = \sum_{t=r\vee s}^TB_{st}'B_{rt}$ and $c_s=\sum_{t=s}^TB_{st}'b_t$ and where $a\vee b = \max(a,b)$. Here we use two properties of the Kronecker product $\otimes$. First for any matrices $A:p\times q$ and $B:q\times r$, $\vect(AB) = (B'\otimes I_p)\vect(A)$. Second, for any $p\times 1$ vectors $a$ and $b$ and any $p\times p$ matrix $A$, $(a'\otimes I_p)'A(b' \otimes I_p) = ab'\otimes A$. 

Now let $L^*=(\vech(L_1)',\cdots,\vech(L_T))$. Then
\begin{align*}
  p(L^*&|\gamma_{0:T},\cdots)\propto \exp\left[-\frac{1}{2}\left(L^* - \Omega^{-1}\omega\right)'\Omega\left(L^* - \Omega^{-1}\omega\right)\right]
\end{align*}
where $\Omega$ is a $Tp(p+1)/2\times Tp(p+1)/2$ positive definite matrix with $p(p+1)/2\times p(p+1)/2$ blocks
\begin{align*}
  \Omega_{sr}=S_p(\gamma_s\gamma_r'\otimes C_{sr})S_p'
\end{align*}
and $\omega$ is a $Tp(p+1)/2\times 1$ vector with $p(p+1)/2\times 1$ blocks
\begin{align*}
  \omega_{s}=S_p(\gamma_s\otimes I_p)\sum_{t=s}^TB_{st}'b_t.
\end{align*}

So the conjugate prior on $L_{1:T}$ is a multivariate normal distribution. This seems a strange since we expect the diagonal elements of $L_t$ to be positive since they are standard deviations, but this is no problem as long as we view this prior as a clever trick for defining a prior on $W_t=L_t'L_t$ so that the sign doesn't matter. Strictly speaking here, we have subtly changed the definition of $L_t$ to a {\it signed} Cholesky decomposition of $W_t$, and thus subtly changed the defintion of the $\gamma_t$'s to take into account the signs of the diagonal elements of $L_t$. \cite{fruhwirth2008bayesian}, \cite{fruhwirth2011bayesian} and {\it CITE THE DYNAMIC PAPER SYLVIA IS WORKING ON WITH ANGELA AND THE STOCAHSTIC VOLATILITY PAPER BY SYLVIA AND GREGOR KASTNER} use this approach to choosing priors for the system (or hierarchical) variance when working with the scaled disturbances in dynamic and non-dynamic models, but only the first considers the covariance matrix case, rather than the scalar variance case, and that paper does not give the exlicit construction of the covariance matrix of $\vech(L_t)$ using the elimination matrix.

We have two sets of covariance matrices, $W_{1:T}$ and $V_{1:T}$, and we want to put a different class of priors on each set. We can put the same sort of normal prior on $K^*$, the vectorized Cholesky decompositions of the $V_t$'s. In the univariate case the conditional posterior of $V_t$ will come out to be a generalized inverse gaussian distribution which is a bit complicated but not awful to draw from. There's mild tension here though --  depending on how we choose to write down the model we end up with a different class of prior distributions for at least $W_{1:T}$. Now the reason for this difference is ultimately computation --- it is known that sometimes using the scaled disturbances improves mixing in the Markov chain --- but ideally computational concerns should not have an effect on inference. It would be nice to unite these priors two priors under a common class without sacrificing their respective computaitonal advantages under the relevant data augmentations. 

The above discussion assumes that the covariance matrices are time dependent. Typically this will not be the case, and when it is often $W_{1:T}$ and $V_{1:T}$ will each have a hierachical prior that depends on some hyperparameters, e.g. matrices $\Psi_W$ and $\Psi_V$, each with their own hyperprior. The typical case, however, is $W$ and $V$ constant over time. The same basic reasoning applies, but the result is much simpler. First, redefine $B_{st}=K^{-1}F_tH_{st}$ and $b_t=K^{-1}(y_t - F_t H_{0t}\gamma_0)$. Then we get
\begin{align*}
    p(L|&\gamma_{0:T},\cdots) \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(K^{-1}(y_t - F_tH_{0t}\gamma_0) - K^{-1}F_t\sum_{s=1}^tH_{st}L\gamma_s\right)'\Bigg(.\Bigg)\right]\\
   & \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(b_t - \sum_{s=1}^tB_{st}L\gamma_s\right)'\left(b_t - \sum_{s=1}^tB_{st}L\gamma_s\right)\right]\\
    & \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(b_t - \sum_{s=1}^tB_{st}\vect(L\gamma_s)\right)'\left(b_t - \sum_{s=1}^tB_{st}\vect(L\gamma_s)\right)\right]\\
    & \propto \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(b_t - \sum_{s=1}^tB_{st}(\gamma_s'\otimes I_p)\vect(L)\right)'\left(b_t - \sum_{s=1}^tB_{st}(\gamma_s'\otimes I_p)\vect(L)\right)\right]\\
    & \propto \exp\left[-\frac{1}{2}\left(\vect(L)'\sum_{t=1}^T\sum_{s=1}^t\sum_{r=1}^t(\gamma_s\gamma_r'\otimes B_{st}'B_{rt})\vect(L)  - 2\sum_{t=1}^Tb_t'\sum_{s=1}^tB_{st}(\gamma_s'\otimes I_p)\vect(L)\right)\right]\\
    & \propto \exp\left[-\frac{1}{2}\left(\vech(L)'\Omega\vech(L)  - 2\omega'\vech(L)\right)\right]
\end{align*}
where we define
\begin{align*}
  \Omega &= S_p\sum_{t=1}^T\sum_{s=1}^t\sum_{r=1}^t(\gamma_s\gamma_r'\otimes B_{st}'B_{rt})S_p' = \sum_{s=1}^T\sum_{r=1}^TS_p(\gamma_s\gamma_r'\otimes C_{sr})S_p'\\
  \intertext{and}
  \omega &= S_p\sum_{t=1}^T\sum_{s=1}^t(\gamma_s\otimes I_p)B_{st}'b_t = \sum_{s=1}^TS_p(\gamma_s\otimes I_p)c_s.
\end{align*}
So again the condiitonally conjugate prior for the nonzero elements of $L$ is a multivariate normal distribution. 

For completeness, we show that the conditionally conjugate prior for $K_{1:T}$ and, in the case of time-constant variances $K$, is also a multivariate normal distribution. From \eqref{dlmerrorjoint} we have
\begin{align*}
  p(K_{1:T}|\psi_{0:T},\cdots)\propto & \exp\left[-\frac{1}{2}g(K_{1:T})\right]
\end{align*}
where 
\begin{align*}
  g(K_{1:T}) & = A + \sum_{t=1}^T\left[y_t - K_t\psi_t - F_tG_tG_{t-1}\left(y_{t-1} - K_{t-1}\psi_{t-1}\right)\right]'(F_t'W_tF_t)^{-1}\left[y_t - K_t\psi_t - F_tG_tG_{t-1}\left(y_{t-1} - K_{t-1}\psi_{t-1}\right)\right]\\
  & = A + \sum_{t=1}^T\left[y_t - K_t\psi_t - H_t\left(y_{t-1} - K_{t-1}\psi_{t-1}\right)\right]'D_t\left[y_t - K_t\psi_t - H_t\left(y_{t-1} - K_{t-1}\psi_{t-1}\right)\right]\\
\end{align*}
where $A$ is some constant with respect to $K_{1:T}$, $D_t=(F_t'W_tF_t)^{-1}$, $H_t=F_tG_tF_{t-1}$, $K_0=I_k$, $F_0=I_k$, and $y_0=0$. From this point forward we will ignore the constant $A$ since it will be absorbed into the proportionality constant for $p(K_{1:T}|\psi_{0:T},\cdots)$. Now up to an additive constant we have
\begin{align*}
  g(K_{1:T}) = & \sum_{t=1}^T\left(y_t - (\psi_t' \otimes I_k)S_p'\vech(K_t)\right)'D_t\left(y_t - (\psi_t' \otimes I_k)S_p'\vech(K_t)\right) \\
  & + \sum_{t=2}^T\left(y_{t-1} - (\psi_{t-1}' \otimes I_k )S_p'\vech(K_{t-1})\right)'H_t'D_tH_t\left(y_{t-1} - (\psi_{t-1}' \otimes I_k )S_p'\vech(K_{t-1})\right)\\
  & + \sum_{t=2}^T\left(y_t - (\psi_t'\otimes I_k)S_p'\vech(K_t)\right)'D_tH_t\left(y_{t-1} - (\psi_{t-1}'\otimes I_k)S_p'\vech(K_{t-1})\right) \\
  & + \sum_{t=2}^T\left(y_{t-1} - (\psi_{t-1}'\otimes I_k)S_p'\vech(K_{t-1})\right)'H_t'D_t\left(y_t - (\psi_t'\otimes I_k)S_p'\vech(K_t)\right)\\
  & + 2\vech(K_1)'S_p(\psi_1'\otimes I_k)'D_1H_1\psi_0.
\end{align*}
But if we write
\begin{align*}
  g(K_{1:T}) = (K^*)'\Omega K^* - 2\omega'K^*
\end{align*}
where $K^*=(\vech(K_1)',\cdots,\vech(K_T)')'$ then we can identify blocks of $\Omega$ with the cross product terms in $g$ and blocks of $\omega$ with the single product terms in $g$. Specifically, $\Omega$ and $\omega$ are defined by blocks
\begin{align*}
  \Omega_{TT} & = S_p(\psi_T\psi_T'\otimes D_T)S_p' &&\\
  \Omega_{tt} & = S_p\left[\psi_t\psi_t'\otimes (D_t + H_{t+1}'D_{t+1}H_{t+1})\right]S_p'&&\ \ \mathrm{for}\ \ t=1,2,\cdots,T-1\\
  \Omega_{t,t-1} & = S_p(\psi_t\psi_{t-1}'\otimes D_tH_t)S_p'&& \ \ \mathrm{for} \ \ t=1,2,\cdots,T-1\\
  \Omega_{t-1,t} & = S_p(\psi_{t-1}\psi_{t}'\otimes H_t'D_t)S_p' = \Omega_{t,t-1}'&& \ \ \mathrm{for} \ \ t=1,2,\cdots,T-1\\
  \omega_{1} & = S_p(\psi_1\otimes I_k)(D_1y_1 + H_2'D_2H_2y_1 + H_2'D_2y_2 - H_1'D_1\psi_0)&&\\
  \omega_{T} & = S_p(\psi_T\otimes I_k)(D_TY_T + D_TH_Ty_{T-1})&&\\
  \omega_t & = S_p(\psi_t\otimes I_k)(D_ty_t + H_{t+1}'D_{t+1}H_{t+1}y_t + D_tH_ty_{t-1} + H_{t+1}'D_{t+1}y_{t+1})&&\ \ \mathrm{for}\ \ t=2,3,\cdots,T-1
\end{align*}
where $\Omega_{t,t-i}$ is a matrix of zeroes for $i > 1$. Then given a prior proportional to $1$, $K^*|\psi_{0:T},\cdots \sim N(\Omega^{-1}\omega, \Omega^{-1})$. 

For the time constant $K$ case, redefine $D_t=(F_t'WF_t)^{-1}$. Then up to an additive constant we have
\begin{align*}
  g(K) = & \sum_{t=1}^T\left(y_t - (\psi_t' \otimes I_k)S_p'\vech(K)\right)'D_t\left(y_t - (\psi_t' \otimes I_k)S_p'\vech(K)\right) \\
  & + \sum_{t=2}^T\left(y_{t-1} - (\psi_{t-1}' \otimes I_k )S_p'\vech(K)\right)'H_t'D_tH_t\left(y_{t-1} - (\psi_{t-1}' \otimes I_k )S_p'\vech(K)\right)\\
  & + \sum_{t=2}^T\left(y_t - (\psi_t'\otimes I_k)S_p'\vech(K)\right)'D_tH_t\left(y_{t-1} - (\psi_{t-1}'\otimes I_k)S_p'\vech(K)\right) \\
  & + \sum_{t=2}^T\left(y_{t-1} - (\psi_{t-1}'\otimes I_k)S_p'\vech(K)\right)'H_t'D_t\left(y_t - (\psi_t'\otimes I_k)S_p'\vech(K)\right)\\
  & + 2\vech(K)'S_p(\psi_1'\otimes I_k)'D_1H_1\psi_0.
\end{align*}
This gives
\begin{align*}
  \Omega = & \sum_{t=1}^TS_p(\psi_t\psi_t'\otimes D_t)S_p' + \sum_{t=1}^{T-1}S_p'(\psi_t\psi_t'\otimes H_{t+1}'D_{t+1}H_{t+1})S_p' + 2\sum_{t=1}^{T-1}S_p(\psi_t\psi_{t+1}'\otimes H_{t+1}'D_{t+1})S_p'\\
  \intertext{and}
  \omega = & \sum_{t=1}^{T-1}S_p(\psi_t\otimes I_k)(D_t + H_{t+1}'D_{t+1}H_{t+1})y_t + \sum_{t=1}^{T-1}S_p(\psi_{t+1}\otimes I_k)D_{t+1}H_{t+1}y_t\\
  & +\sum_{t=1}^{T-1}S_p(\psi_t\otimes I_k)H_{t+1}'D_{t+1}y_{t+1}  + S_p\left[(\psi_T\otimes I_k)D_Ty_T - (\psi_1\otimes I_k)D_1H_1\psi_0\right]
\end{align*}
so that if $K$ has a prior proportional to $1$, then $\vech(K)|\psi_{0:T},\cdots \sim N(\Omega^{-1}\omega, \Omega^{-1})$. There are efficient routines for drawing from a $N(\Omega^{-1}\omega, \Omega^{-1})$ distribution when $\Omega$ is block tribanded as for $K$ and $K_{1:T}$ above. For example, \cite{mccausland2011simulation} describes these routines in the context of drawing the latent states ($\theta_{0:T}$) in a DLM.

