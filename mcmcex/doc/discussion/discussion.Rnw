<<set-parent-disc, echo=FALSE, cache=FALSE>>=
set_parent('../mcmcex.Rnw')
@ 

\section{Discussion}\label{sec:Discussion}
Our results on computational time in Section \ref{sec:LLMtime} should be taken with a grain of salt because we did not put much effort into efficiently sampling from $p(W|V,\gamma_{0:T},y_{1:T}$ or $p(V|W,\psi_{0:T},y_{1:T})$. Both densities have the form
\[
p(x)\propto x^{-\alpha-1}\exp\left[-ax + b\sqrt{x} - \frac{\beta}{x}\right]
\]
where $\alpha,\beta,a>0$. This density is the same as the generalized inverse Gaussian distribution (see e.g. \citet{jorgensen1982statistical}) when $b=0$, but in general this is not the case in our application. It is possible that sampling from this density can be significantly improved in which case, the relative speed of the algorithms based on either the scaled errors or the scaled disturbances will improve significantly. {\it \bf Perhaps look into the amount of time spent on the smoothing step vs elsewhere here}

In the data augmentation for multilevel models literature, a key qauntity is called the fraction of missing information (\citet{van2001art}, for example). When $\phi$ is the model parameter, $\theta$ is the data augmentation and $y$ is the data, the Bayesian fraction of missing information is defined as
\begin{align*}
  \mathcal{F}_B = I - [var(\phi|y)]^{-1}E[var(\phi|\theta,y)|y]
\end{align*}
while the EM fraction of missing information is defined as
\begin{align*}
  \mathcal{F}_{EM} = I - I_{obs}I_{aug}^{-1}
\end{align*}
where 
\begin{align*}
  I_{aug}=& \left.\mathrm{E} \left[-\left.\frac{\partial^2 \log p(\phi|\theta,y)}{\partial \phi \dot \partial \phi}\right| y,\phi,\right]\right|_{\phi=\phi^*}\\
  \intertext{is the expected augmented Fisher information matrix and}
  I_{obs} =& -\left.\frac{\partial^2\log p(\phi|y)}{\partial\phi \dot \partial\phi}\right|_{\phi=\phi^*}
\end{align*}
is the observed fisher information matrix while $\phi^*$ is the posterior mode. The rate of convergence of the EM algorithm is governed by $\mathcal{F}_{EM}$ while that of the Gibbs sampler is governed by $\mathcal{F}_{B}$ --- the larger the spectral radius of $\mathcal{F}$, the slower the corresponding algorithm converges. This result applies to hierachical models in a certain class, but not statespace models because of the dependence among the elements in the data augmentation ({\it IS THIS RIGHT? CHECK}). Nevertheless, \citet{fruhwirth2004efficient} use this result as a guide to exploring different DAs in a dynamic regression with AR(1) regression coefficient and they find that as long as the estimated autocorrelation in the coefficient is low, the multilevel model results roughly apply.

The significance of the signal-to-noise ratio in our results in likely related to this despite the fact that we do not have a result about the fraction of missing information in a time series setting. In particular, \citet{pitt1999analytic} find that the signal to noise ratio along with the AR(1) coefficient determine the convergence rate of a Gibbs sampler in an AR(1) plus noise model. In addition, they find that as the length of the time series increases, the convergence rate slows down and compute asympotitic convergence rates for an infinite time series. However, \citeauthor{pitt1999analytic} assume that both of the variances are fixed in order to derive their results. In a continuous-time model, \citet{roberts2004bayesian} find that the Gibbs sampler based on a NCP is at least as efficient as the Gibbs sampler based on the CP and sometimes it is much more efficient, depending on the true values of the parameters. It is unclear whether the time series dependence or something unique to the model is driving this, but at least this suggests that in the time series case convergence and mixing properties can sometimes be wildly different than what we might expect and calls out the need for a time series or statespace fraction of missing information.




