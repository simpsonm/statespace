% bits of tex documents that don't have a place yet




% local level model stuff





The DA algorithm based on $\gamma_{0:T}$ for the local level model is as follows: 
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{distalg}
  \item Draw $\gamma_{0:T}$ from $p(\gamma_{0:T}|V^{(k)},W^{(k)},y_{1:T})$
  \item Draw $(V^{(k+1)},W^{(k+1)})$ from $p(V,W|\gamma_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
Step 1 can be accomplished directly with the disturbance smoother of \citet{koopman1993disturbance} or indirectly by using FFBS to draw the states and then transform them to the scaled disturbances. Step 2 ends up being complicated because the joint conditional posterior of $V$ and $W$ isn't a known density. From equation \eqref{distjoint} we have
\[
p(V,W|\gamma_{0:T},y_{1:T})\propto V^{-\left(T/2 + \alpha_V + 1\right)} \exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2} \textstyle\sum_{t=1}^T\left(y_t-\gamma_0 - \sqrt{W}\textstyle\sum_{s=1}^{t}\gamma_s\right)^2\right)\right] W^{-\left(\alpha_W + 1\right)} \exp\left[-\frac{1}{W}\beta_W\right].
\]
So we have three options. First, we can make the effort to come up with an algorithm to obtain an exact sample from $(V,W|\gamma_{0:T},y_{1:T})$. Second, we can give up on sampling $(V,W)$ and instead add another Gibbs step to sample $V$ and $W$ in separate steps. This would still require some effort, but hopefully less. Third, we can give up on the exact sample and instead use a Metropolis step for $(V,W)$. We'll employ the second strategy. 

The full conditional distribution for $V$ can be written as
\begin{align*}
  p(V|W,\gamma_{0:T},y_{1:T}) \propto & V^{-T/2 - \alpha_v - 1}\exp\left[-\frac{1}{V}\left(\beta_v + \frac{1}{2}\sum_{t=1}^T\left(y_t - \gamma_0 - W\sum_{s=1}^t\gamma_s\right)^2\right)\right].
\end{align*}
It is easy to verify that this is the same as $p(V|W,\theta_{0:T},y_{1:T})$, that is an inverse gamma distribution with the same parameters defined in \eqref{IGparm}. However,
\begin{align*}
  p(W|V,\gamma_{0:T},y_{1:T}) \propto & W^{-\alpha_w - 1}\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \gamma_0 - W\sum_{s=1}^t\gamma_s\right)^2\right].
\end{align*}
This density isn't any known form and is difficult to sample from. The log density can be written as
\begin{align}
\log p(W|V,\gamma_{0:T},y_{1:T}) =& -aW + b\sqrt{W} - (\alpha_W + 1)\log W -\beta_W/W + C \label{sdlogV}
\end{align}
where $C$ is some constant, $a=\sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$ and $b=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$. It can be shown that $b^2 > \frac{32}{9\beta_w}(\alpha_w+1)^3(1 - 2sgn(b)/3)$ implies that the density is log concave where
\begin{align*}
  sgn(b) & = \begin{cases} 1 &\text{if } b>0\\
    0 & \text{if } b=0\\
    -1 & \text{if } b<0\\
    \end{cases}
\end{align*}
This condition is equivalent to $\partial^2\log p(W|.})/\partial W^2 < 0$ at the $W^*$ that maximizes $\partial^2\log p(W|.)/\partial W^2$ and hence guarantees the density is globally log-concave. It turns out that this tends to hold over a wide region of the parameter space --- so long as $V$ is smaller or isn't much larger than $W$. This allows for the use of adaptive rejection sampling in order to sample from this distribution in many cases, e.g. using \citet{gilks1992adaptive}. An alternative is to use a $t$ approximation to the conditional density as a proposal in a rejection sampler, but this is much more computationally expensive when necessary.

Based on these full conditionals, the scaled disturbance sampler obtains the $(V^{(k+1)},W^{(k+1)})$ from $(V^{(k)},W^{(k)})$ by performing the following steps:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{distalg}
  \item Draw $\theta_{0:T}$ from $p(\theta_{0:T}|V,W,y_{1:T})$
    using FFBS and form $\gamma_{0:T}$, or simulate $\gamma_{0:T}$ from $p(\gamma_{0:T}|V,W,y_{1:T})$ directly.
  \item Simulate $V$ from $p(V|\gamma_{0:T},W,y_{1:T})$, an inverse gamma distribution with parameters $a_v = \alpha_v + T/2$ and $b_v = \beta_v + \sum_{t=1}^T(y_t-\gamma_0 -\sqrt{W}\sum_{j=1}^t\gamma_j)^2/2$.
  \item Simulate $W$ from $p(W|\gamma_{0:T},V,y_{1:T})$ using adaptive rejection sampling when possible, otherwise using a $t$ approximation in order to rejection sample. 
\end{enumerate}
\end{alg}

The local level model, at least, can easily be written in terms of the scaled errors. They simplify to $\psi_t = (y_t - \theta_t)/\sqrt{V}=v_t/\sqrt{V}$ for $t=1,2,\cdots,T$. In order to confirm our intuition that $\psi_{0:T}$ is an AA for $(V,W)$, note that we can write $\psi_{0:T} = y_{0:T}/\sqrt{V} - B \theta_{0:T}$ where we define $y_0=0$ and 
\begin{align*}
B=\frac{1}{\sqrt{V}}\begin{bmatrix} \sqrt{V} & 0 & 0 & \cdots & 0 & 0 \\
  0 & 1 & 0 & \cdots & 0 & 0 \\
  0 &  0 & 1 &  \cdots & 0 & 0 \\
  \ddots &   \ddots &   \ddots &\ddots &  \ddots &   \ddots & \\
  0 & 0 & 0 & \cdots & 0 & 1 \end{bmatrix}.
\end{align*}
This means that $|J|=1/|B|=V^{T/2}$, i.e. exactly analogous to the determinant of the Jacobian for the scaled disturbances. Note that $\theta_t = y_t - \sqrt{V}\psi_t$ for $t=1,2,\cdots,T$ and $\theta_0=\psi_0$. Then from \eqref{statepost} we can write the joint distribution of $(V,W,\psi_{0:T},y_{1:T})$ as
\begin{align}
  p(V,W,\psi_{0:T},y_{1:T}) &\propto W^{-\left(T/2 + \alpha_W + 1\right)} \exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T[ y_t - y_{t-1} - \sqrt{V}\psi_t + \sqrt{V}^{\indicator{t>1}}\psi_{t-1})]^2 \right) \right]  \nonumber\\
  \times &  V^{-\left(\alpha_V + 1\right)} \exp\left[-\frac{1}{V}\beta_V\right]\exp\left[\frac{1}{2} \textstyle\sum_{t=1}^T\psi_t^2\right] \exp\left[-\frac{1}{2C_0}(\psi_0 - m_0)^2\right]\label{errorjoint}. 
\end{align}
where again $y_0=0$ and $\indicator{}$ is the indicator function. The exponent $\indicator{t>1}$ is needed in the first line because $\psi_0$ is unscaled. This bit of inelegance only appears egregious in the context of constant variance DLMs. In the general DLM, the $\psi_t$'s aren't scaled by the same standard deviation anyway and $\psi_0$ is special since it encodes the initial position of the time series.

From \eqref{errorjoint} we can write the model in terms of the scaled error parameterization as
\begin{align*}
  y_t|y_{0:t-1}\psi_{0:T},V,W & \sim N\left(y_{t-1} + \sqrt{V}\psi_t - \sqrt{V}^{\indicator{t>1}}\psi_{t-1},W\right)\\
  \psi_t & \stackrel{iid}{\sim}N(0,1)
\end{align*}
for $t=1,2,\cdots,T$. Now we see immediately that the scaled errors, $\psi_{0:T}$, are also an AA for $(V,W)$ since neither $V$ nor $W$ are in the system equation.

The DA algorithm based on $\psi_{0:T}$ for the local level model is:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{erroralg}
  \item Draw $\psi_{0:T}$ from $p(\psi_{0:T}|V^{(k)},W^{(k)},y_{1:T})$
  \item Draw $(V^{(k+1)},W^{(k+1)})$ from $p(V,W|\psi_{0:T},y_{1:T})$.
\end{enumerate}
\end{alg}
Once again step one can be accomplished directly with \citeauthor{koopman1993disturbance}'s disturbance smoother or indirectly using FFBS. Step 2 is also once again complicated since the joint conditional posterior of $V$ and $W$ isn't a known density. Equation \eqref{errorjoint} gives
\begin{align*}
p(V,W|\psi_{0:T},y_{1:T})\propto &W^{-\left(T/2 + \alpha_W + 1\right)} \exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T\left( y_t - y_{t-1} - \sqrt{V}\psi_t + \sqrt{V}^{\indicator{t>1}}\psi_{t-1})\right)^2 \right) \right] \\
&\times V^{-\left(\alpha_V + 1\right)} \exp\left[-\frac{1}{V}\beta_V\right]
\end{align*}
We'll sample $V$ and $W$ in two separate Gibbs steps, but again note that we could put in work to sample $V$ and $W$ jointly or use a metropolis step for $V$ and $W$ jointly. The full conditional distribution of $W$ is
\begin{align*}
  p(W|V,\psi_{0:T},y_{1:T}) \propto & W^{-T/2 - \alpha_W - 1}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\sum_{t=1}^T\left(y_t - y_{t-1} - \sqrt{V}\psi_t + \sqrt{V}^{\indicator{t>0}}\psi_{t-1}\right)^2\right)\right].
\end{align*}
In other words, an inverse gamma distribution with the same parameters for $W$ as in \eqref{IGparm}. The full conditional distribution for $V$ is more complicated, on the other hand:
\[
p(V|W,\psi_{0:T},y_{1:T})\propto \exp\left[-\frac{1}{W}\left(\frac{1}{2}\textstyle\sum_{t=1}^T\left( y_t - y_{t-1} - \sqrt{V}\psi_t + \sqrt{V}^{\indicator{t>1}}\psi_{t-1})\right)^2 \right) \right] V^{-\left(\alpha_V + 1\right)} \exp\left[-\frac{1}{V}\beta_V\right]
\]
This is analogous to the full conditional density for $W$ with the scaled disturbances. The log density can be written as
\begin{align*}
\log p(V|W,\psi_{0:T},y_{1:T}) =& -aV + b\sqrt{V} - (\alpha_V + 1)\log V -\beta_V/V + C
\end{align*}
where $C$ is again some constant, and here $a=\sum_{t=1}^T(L\psi_t)^2/2W$ and $b=\sum_{t=1}^T(L\psi_tLy_t)/W$ with $Ly_t=y_t-y_{t-1}$ for $t=2,3,...,T$, $Ly_1= y_1 - \psi_0$ and $L\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$, $L\psi_1=\psi_1-0$. Since this density is the same form as in \eqref{sdlogV}, it's also log concave when $b^2 > \frac{32}{9\beta_V}(\alpha_V+1)^3(1 - 2sgn(b)/3)$ with the updated definitions of $a$ and $b$.

Now we can write down the full scaled error algorithm for obtaining $(V^{(k+1)},W^{(k+1)})$ from $(V^{(k)},W^{(k)})$:
\begin{alg}\mbox{}\\[-\baselineskip]
\begin{enumerate}\label{erroralg}
\item Simulate $\theta_{0:T}$ from $p(\theta_{0:T}|V^{(k))},W^{(k)},y_{0:T})$
  using FFBS and form $\psi_{0:T}$ or simulate $\psi_{0:T}$ directly..
\item Simulate $V^{(k+1)}$ from $p(V|\psi_{0:T},W^{(k)},y_{1:T})$ using adaptive rejection sampling when possible, otherwise using a $t$ approximation in order to rejection sample.
\item Simulate $W^{(k+1)}$ from $p(W|\psi_{0:T},V^{(k+1)},y_{1:T})$, an inverse gamma distribution with parameters $a_W = \alpha_w + T/2$ and $b_W = \beta_w + \sum_{t=1}^T\left(y_t- y_{t-1} - \sqrt{V}\psi_t + \sqrt{V}^{\indicator{t>1}}\psi_{t-1})\right)^2/2$ with $y_0=0$.
\end{enumerate}
\end{alg}
