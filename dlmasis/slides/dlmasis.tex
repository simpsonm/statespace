\documentclass[xcolor=dvipsnames]{beamer}
\makeatletter\def\Hy@xspace@end{}\makeatother 
\usepackage{graphicx, color, amssymb, amsmath, bm, rotating, graphics,
epsfig, multicol, amsthm}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}
\usepackage[authoryear]{natbib}
%\newcommand{\newblock}{}  %needed to make beamer and natbib play nice
\usepackage{tikz}
\usetikzlibrary{fit}					% fitting shapes to coordinates
\usetheme{Boadilla}
\usecolortheme[named=Red]{structure}
\setbeamercovered{transparent}
\beamertemplatenavigationsymbolsempty
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\N{\mathcal{N}}
\graphicspath{{../doc/plots/}}
\title[Interweaving MCMC Strats for DLMs]{Interweaving Markov Chain Monte Carlo Strategies for Efficient
Estimation of Dynamic Linear Models}
%\subtitle{}
\author[M. Simpson]{Matthew Simpson}
\date{}
\institute[Deps of Stat \& Econ, ISU]{Departments of Statistics and Economics, Iowa State University}


%\title[short title]{long title}
%\subtitle[short subtitle]{long subtitle}
%\author[short name]{long name}
%\date[short date]{long date}
%\institution[short name]{long name}

% very important to use option [fragile] for frames containing code output!

\begin{document}

\begin{frame}
\titlepage
\begin{center}
with Jarad Niemi and Vivekananda Roy\\~\\
\scriptsize{Department of Statistics, Iowa State University}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Interweaving: A Motivating Example}
Adapted from \citet{yu2011center}, suppose:
\begin{align*}
y|\theta, \mu & \sim \N(\theta, V) \\
\theta|\mu & \sim \N(\mu, W) 
\end{align*}
with $V$, $W$ known and $p(\mu)\propto 1$.\\~\\
Posterior: $\mu|y \sim \N(y, V+W)$\\~\\
DA algorithm:
\begin{align*}
\theta|\mu,y &\sim \N\left(\frac{V\mu + Wy}{V+W}, \frac{VW}{V+W}\right)\\
\mu |\theta, y &\sim \N(\theta, W)
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Interweaving: A Motivating Example}
Let $\gamma = \theta - \mu$. Then:
\begin{align*}
y|\gamma, \mu & \sim \N(\mu + \gamma, V) \\
\gamma|\mu & \sim \N(0, W) 
\end{align*}
DA algorithm based on $\gamma$:
\begin{align*}
\gamma|\mu,y &\sim \N\left(\frac{W(\mu - y)}{V+W}, \frac{VW}{V+W}\right)\\
\mu |\gamma, y &\sim \N(y-\gamma, V)
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Interweaving: A Motivating Example}
Alternate between two DAs (alternating algorithm):
\begin{align*}
{\color{blue}[\theta|\mu,y]} \to {\color{blue}[\mu|\theta,y]} \to [\gamma|\mu,y] \to [\mu|\gamma,y]
\end{align*}
Weave two DAs together (interweaving algorithm):
\begin{align*}
{\color{blue}[\theta|\mu,y]} \to {\color{blue}[\mu|\theta,y]} \to {\color{red}[\gamma|\mu,\theta,y]} \to [\mu|\gamma,y]
\end{align*}
The interweaving algorithm obtains {\bf IID} draws from the posterior of $\mu$.
\end{frame}

\begin{frame}
  \frametitle{Outline}
  Goal: apply interweaving to DLMs\\~\\
  \begin{enumerate}
    \item Intro to interweaving\\~\\
    \item The model\\~\\
    \item DAs for the model, new and old\\~\\
    \item Simulation results in the local level model
  \end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Global Interweaving Strategy (GIS)}
From \citet{yu2011center}: target posterior distribution $p(\phi|y)$ with two DAs $\theta$ and $\gamma$ such that
\begin{align*}
\int p(\theta,\phi|y) d\theta = p(\phi|y) \ \ \ \ \mbox{and}\ \ \ \int p(\gamma,\phi|y) d\gamma = p(\phi|y) 
\end{align*}
Then:
\begin{align*}
[\theta|\phi,y] \to [\gamma|\theta,y] \to [\phi|\gamma,y]
\end{align*}
or more commonly:
\begin{align*}
[\theta|\phi,y] \to [\phi|\theta,y] \to {\color{blue}[\gamma|\phi,\theta,y]} \to [\phi|\gamma,y]
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Ancillary-Sufficiency Interweaving Strategy (ASIS)}
GIS where one DA is a sufficient augmentation (SA) and the other is an ancillary augmentation (AA).\\~\\
\begin{itemize}
\item$\theta$ is an SA if $p(y|\theta,\phi)=p(y|\theta)$ (AKA centered augmentation)\\~\\
\item$\theta$ is an AA if $p(\theta|\phi)=p(\theta)$ (AKA non-centered augmentation)\\~\\
\end{itemize}
Componentwise Interweaving Strategy (CIS):\\~\\
\begin{itemize}
\item Motivation: Finding an SA--AA pair is often difficult for $\phi=(\phi_1,\phi_2)$, but not for $\phi_1$ and $\phi_2$ separately.\\~\\
\item Basic idea: GIS (or ASIS) for sub-blocks of $\phi$.\\~\\
\item Can use the same DA in multiple sub-blocks of $\phi$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Why interweaving?}
From \citet{yu2011center}:\\~\\
\begin{itemize}
  \item  GIS has a geometric rate of convergence no worse than the worst of the two underlying DA algorithms, and the bound gets better the less $\theta$ and $\tilde{\theta}$ are correlated in the posterior.\\~\\
  \item If the two DAs are independent in the posterior, then GIS results in {\color{blue}\textit{\textbf{IID}}} draws from the posterior. \\~\\
  \item Often GIS has better convergence than the associated alternating algorithm.\\~\\
  \item Under some conditions the ASIS algorithm is the same as the optimal PX-DA algorithm of \citet{liu1999parameter}.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Interweaving in Time Series Models}
\begin{itemize}
\item \citet{yu2011center} use a time series of Poisson counts to illustrate CIS.\\~
\item \citet{kastner2013ancillarity} use ASIS to improve estimation of the stochastic volatility model. \verb0R0 package: \verb0stochvol0.\\~
\item Lots of relevant work on reparameterization: \\~
\begin{itemize}
\item AR(1) plus noise: \citet{pitt1999analytic}\\~
\item Stochastic volatility models: \citet{shephard1996statistical,fruhwirth2003bayesian,roberts2004bayesian,bos2006inference,strickland2008parameterisation,fruhwirth2008heston}\\~
\item Poisson counts: \citet{fruhwirth2006auxiliary}\\~
\item Dynamic regression: \citet{fruhwirth2004efficient}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{The Dynamic Linear Model} 
  For $t=1,2,...,T$
  \begin{align*}
    y_t  =&F_t\theta_t +  v_t  \qquad \mbox{(observation equation)}\\
    \theta_t =& G_t\theta_{t-1} + w_t \qquad \mbox{(system equation)}
  \end{align*} 
  with $v_t\stackrel{iid}{\sim}\N_k(0,V)$ independent of $w_t\stackrel{iid}{\sim}\N_p(0,W)$.\\~\\

\begin{figure}
  \centering
    \tikzstyle{state}=[circle, thick, minimum size=1.2cm, draw=black!80]
    \tikzstyle{obs}=[circle, thick, minimum size=1.2cm, draw=black!80]
  \begin{tikzpicture}[>=latex,text height=1.5ex,text depth=0.25ex]
    \matrix[row sep=0.5cm,column sep=0.5cm]{
    % First line: Observations
    &
    \node (y_t-1) [obs]{$y_{t-1}$}; &
    &
    \node (y_t) [obs]{$y_{t}$}; &
    &
    \node (y_t+1) [obs]{$y_{t+1}$}; &
    \\
    % Second line: States
    \node (theta_t-2) {$\cdots$}; &
    \node (theta_t-1) [state]{$\theta_{t-1}$}; &
    &
    \node (theta_t) [state]{$\theta_{t}$}; &
    &
    \node (theta_t+1) [state]{$\theta_{t+1}$}; &
    \node (theta_t+2) {$\cdots$}; \\
    };
    
    % The diagram elements are now connected through arrows:
    \path[->]
    (theta_t-2) edge (theta_t-1)
    (theta_t-1) edge (theta_t)
    (theta_t) edge (theta_t+1)
    (theta_t+1) edge (theta_t+2)
    (theta_t-1) edge (y_t-1)
    (theta_t) edge (y_t)
    (theta_t+1) edge (y_t+1)
    ;
  \end{tikzpicture}
  \end{figure}

For convenience define $y\equiv(y_1',\cdots,y_T')'$ and $\theta\equiv(\theta_0',\cdots,\theta_T)'$.

\end{frame}

\begin{frame}
  \frametitle{The Dynamic Linear Model} 
Let $\phi$ denote the unknown parameter. Potentially $F_t(\phi)$ and $G_t(\phi)$, but for simplicity assume they are constant and $\phi=(V,W)$.\\~\\

Priors: independently assume \\~\\
\begin{itemize}
\item[]$\theta_0\sim \N_p(m_0,C_0)$, $V\sim IW(\Lambda_V,\lambda_V)$, and $W\sim IW(\Lambda_W,\lambda_W)$\\~\\
\end{itemize}


Marginal model for $y$: 
\begin{align*}
  y|V,W \stackrel{ind}{\sim} N_{Tk}(D\tilde{m}, \tilde{V} + \tilde{W} + \tilde{C})
\end{align*}
where $\tilde{m}=\tilde{m}_{Tp\times 1} = (m_0', m_0', \cdots m_0')'$, $\tilde{V}=I_T\otimes V$, $\tilde{W}$ depends on $W$ and $\tilde{C}$ depends on $C_0$ and where $D$, $\tilde{W}$, and $\tilde{C}$ also depend on the $F_t$'s and $G_t$'s.

$p(\phi|y)\propto$
\begin{align*}
p(\phi)|\tilde{V} + \tilde{W} + \tilde{C}|^{-1/2}\exp\left[-\frac{1}{2}\left(y-D\tilde{m}\right)'(\tilde{V} + \tilde{W} + \tilde{C})^{-1}\left(y-D\tilde{m}\right)\right]
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Data Augmentations for the DLM}
Standard DA: states $\theta$. In terms of $\theta$, the model is:
\begin{align*}
y_t|\theta,V,W \stackrel{ind}{\sim} & \N_k(F_t\theta_t,V)\\ 
\theta_t|\theta_{0:(t-1)},V,W \sim & \N_p(G_t\theta_{t-1},W)
\end{align*} 
{\color{blue}$\theta$ is an SA for $W|V$ and an AA for $V|W$.}\\~\\

Scaled disturbances $\gamma\equiv(\gamma_0',\cdots,\gamma_T)'$ \citep{fruhwirth2004efficient}.
\begin{itemize}
\item[]$\gamma_0=\theta_0$ and $\gamma_t=L_W^{-1}(\theta_t - G_t\theta_{t-1})=L_W^{-1}w_t$ where $L_W$ is the Cholesky decomposition of $W$.
\end{itemize}
Let $\theta_0(\gamma,W)=\gamma_0$ and $\theta_t(\gamma,W)=L_W\gamma_t + G_t\theta_{t-1}(\gamma,W)$. Then:
\begin{align*}
y_t|\gamma,V,W \stackrel{ind}{\sim} & \N_k(F_t\theta_t(\gamma,W),V) \\
\gamma_t|V,W \stackrel{iid}{\sim} & \N_p(0,I_p)
\end{align*} 

{\color{blue}$\gamma$ is an AA for $(V,W)$.}

\end{frame}

\begin{frame}
\frametitle{New Data Augmentations for the DLM}
Scaled errors $\psi\equiv(\psi_0',\cdots,\psi_T')'$:
\begin{itemize}
\item[]$\psi_0=\theta_0$ and $\psi_t=L_V^{-1}(y_t - F_t\theta_t)=L_V^{-1}v_t$ where $L_V$ is the Cholesky decomposition of $V$.
\end{itemize}
For convenience, assume $F_t$ is square and invertible for all $t$.
 
\ \ (So $dim(y_t)=dim(\theta_t)$.)\\~

In terms of $\psi$ the model is:
\begin{align*}
y_t|\psi,V,W \stackrel{ind}{\sim} &\N_p(\mu_t(\psi,V),F_tWF_t')\\
\psi_t|V,W \stackrel{iid}{\sim} &\N_p(0,I_p)
\end{align*} 
where $\mu_t(\psi,V)$ is complicated.\\~\\

{\color{blue}$\psi$ is an AA for $(V,W)$.}
\end{frame}

\begin{frame}
\frametitle{New Data Augmentations for the DLM}
Wrongly-scaled disturbances $\tilde{\gamma}\equiv(\tilde{\gamma}_0',\cdots,\tilde{\gamma}_T')'$:
\begin{itemize}
\item[]$\tilde{\gamma}_0=\theta_0$ and $\tilde{\gamma}_t=L_V^{-1}(\theta_t - G_t\theta_{t-1})=L_V^{-1}w_t$
\end{itemize}
\begin{align*}
  y_t|\tilde{\gamma},V,W \stackrel{ind}{\sim} \N_p\left(F_t\theta_t(\tilde{\gamma},V), V\right), \qquad 
  \tilde{\gamma}_t \stackrel{ind}{\sim} \N_p(0,L_V^{-1}W(L_V^{-1})')
\end{align*}
where $\theta_0(\tilde{\gamma},V)=\tilde{\gamma}_0$ and $\theta_t(\tilde{\gamma},V) = L_V\tilde{\gamma}_t + G_t\theta_{t-1}(\tilde{\gamma},V)$.\\~\\

Wrongly-scaled errors $\tilde{\psi}\equiv(\tilde{\psi}_0',\cdots,\tilde{\psi}_T')'$:
\begin{itemize}
\item[]$\tilde{\psi}_0=\theta_0$ and $\tilde{\psi}_t=L_W^{-1}(y_t - F_t\theta_{t})=L_W^{-1}v_t$
\end{itemize}
\begin{align*}
  y_t|V,W,\tilde{\psi},y_{1:t-1} \sim \N_p(\tilde{\mu}_t(\tilde{\gamma},W), F_tWF_t'), \qquad
  \tilde{\psi}_t  \stackrel{iid}{\sim} \N_p(0,L_W^{-1}V(L_W^{-1})')
\end{align*}
where $\tilde{\mu}_t(\tilde{\gamma},W)$ is complicated.\\~\\

{\color{blue} $\tilde{\gamma}$ is an SA for $W|V$ and $\tilde{\psi}$ is an SA for $V|W$.}
\end{frame}

\begin{frame}
\frametitle{The Elusive Search for a Sufficient Augmentation}
Suppose $\eta$ is an SA for the DLM such that:\\
\begin{align*}
 \left. \begin{bmatrix}\eta \\ y \end{bmatrix}\right|\phi \sim N\left(\begin{bmatrix} \alpha_\eta \\ D\tilde{m} \end{bmatrix}, \begin{bmatrix}
   \Omega_\eta & \Omega_{y,\eta}' \\
   \Omega_{y,\eta} & \tilde{V} + \tilde{W} + \tilde{C} \end{bmatrix}\right).
\end{align*}\\~\\
Let $A=\Omega_{y,\eta}'\Omega_{\eta}^{-1}$ and $\Sigma = \tilde{V} + \tilde{W} + \tilde{C} - A\Omega_{\eta}A'$. Then $A$, $\Sigma$, and $\alpha_{\eta}$ are constants with respect to $\phi$ and if $A'A$ is invertible, then\\
\begin{align*}
&p(\phi|\eta,y) \propto p(\phi)|(A'A)^{-1}A'(\tilde{V} + \tilde{W} + \tilde{C} - \Sigma)A(A'A)^{-1}|^{-1/2}\\
&\times \exp\left[-\frac{1}{2}(\eta - \alpha_{\eta})'[(A'A)^{-1}A'(\tilde{V} + \tilde{W} + \tilde{C} - \Sigma)A(A'A)^{-1}]^{-1}(\eta - \alpha_{\eta})\right]
\end{align*}
\end{frame}

\begin{frame}
\frametitle{MCMC Strategies}
The standard DA Algorithm--the state sampler: {\color{blue}$[\theta|V,W,y] \to [V,W|\theta,y]$}
\begin{itemize}
\item $p(\theta|V,W,y)$ is Gaussian --- draw using the mixed Cholesky factor algorithm (MCFA) \citep{mccausland2011simulation}.
\item Conditional on $(\theta,y)$, $V$ and $W$ are independent inverse Wishart.\\~\\
\end{itemize}

Scaled disturbance sampler: {\color{blue}$[\gamma|V,W,y] \to [V|W,\gamma,y] \to [W|V,\gamma,y]$}
\begin{itemize}
\item $p(\gamma|V,W,y)$ is Gaussian, but easier to draw $\theta$ using MCFA and transform.
\item $p(V,W|\gamma,y)$ is complicated, but $p(V|W,\gamma,y)$ is inverse Wishart.
\item $p(W|V,\gamma,y)$ is still complicated, but less so.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MCMC Strategies}
Scaled error sampler: {\color{blue}$[\psi|V,W,y] \to [V|W,\psi,y] \to [W|V,\psi,y]$}
\begin{itemize}
\item $p(\psi|V,W,y)$ is Gaussian and can use MCFA directly.
\item $p(V,W|\psi,y)$ is complicated, but $p(V|W,\psi,y)$ is less complicated. 
\item $p(W|V,\psi,y)$ is inverse Wishart.\\~\\
\end{itemize}

Wrongly-scaled samplers are analogous to their correctly-scaled counterparts.\\~\\

Alternating and GIS easy to construct once sampling from the difficult densities is accomplished.
\end{frame}

\begin{frame}
\frametitle{MCMC Strategies}
CIS sampler: 
\begin{itemize}
\item $\gamma$ is an AA and $\tilde{\gamma}$ is an SA for $W|V$.
\item $\psi$ is an AA and $\tilde{\psi}$ is an SA for $V|W$.
\end{itemize}
So:
\begin{align*}
&[\psi|V,W,y]\to[V|W,\psi,y]\to{\color{blue}[\tilde{\psi}|V,W,\psi,y]}\to[V|W,\tilde{\psi},y]\to\\
&{\color{blue}[\tilde{\gamma}|V,W,\tilde{\psi},y]}\to[W|V,\tilde{\gamma},y]\to{\color{blue}[\gamma|V,W,\tilde{\gamma},y]}\to[W|V,\gamma,y]
\end{align*}
But this is equivalent to:
\begin{align*}
&[\psi|V,W,y]\to[V|W,\psi,y]\to{\color{blue}[\theta|V,W,\psi,y]}\to[V|W,\theta,y]\to\\
&[W|V,\theta,y]\to{\color{blue}[\gamma|V,W,\theta,y]}\to[W|V,\gamma,y]
\end{align*}
Steps of this sampler can be rearranged to obtain the SE-SD GIS sampler:
\begin{align*}
&[\psi|V,W,y]\to[V|W,\psi,y]\to[W|V,\psi,y]\to\\
&{\color{blue}[\gamma|V,W,\psi,y]}\to[V|W,\gamma,y]\to[W|V,\gamma,y]
\end{align*}

\end{frame}

\begin{frame}[fragile]
\frametitle{Evaluating the Strategies: the Local Level Model}
Model: for $t=1,2,...,T$
\begin{align*}
    y_t|\theta  \stackrel{ind}{\sim}&\N_1(\theta_t,V) \qquad (\mbox{observation equation})\\
    \theta_t|\theta_{0:(t-1)} \sim& \N_1(\theta_{t-1},W) \qquad (\mbox{system equation})
  \end{align*} 

Then $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$ have the form:
\begin{align*}
p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b\sqrt{x} -c/x\right], \qquad a,c,\alpha,x>0, b\in\Re
\end{align*}
Similarly $p(W|V,\tilde{\psi},y)$ and $p(V|W,\tilde{\gamma},y)$ have the form:
\begin{align*}
p(y)\propto y^{-\alpha-1}\exp\left[ -ay + b/\sqrt{y} -c/y\right], \qquad a,c,\alpha,y>0, b\in\Re
\end{align*}
Rejection sampling with a Cauchy proposal works for $\log(x)$ and $\log(y)$.\\~\\

Often $p(x)$ is log concave $\to$ adaptive rejection sampling (\verb0R0 package \verb0ars0).
\end{frame}

\begin{frame}
\frametitle{Evaluating the Strategies: LLM - Simulation Setup}

Let $V^*$ and $W^*$ denote the true values used to simulate the time series.\\~\\

Independent priors:
\begin{itemize}
\item $\theta_0\sim N(0, 10^7)$, $V\sim IG(5, 4V^*)$ and $W\sim IG(5, 4W^*)$.\\~\\
\end{itemize}

Simulation Setup:
\begin{itemize}
\item Simulated data: $T=10$, $T=100$ \& $T=1000$ and $V^*$, $W^*$ $=10^{i/2}$ with $i=-4,-3,\cdots,4$.
\item Each sampler was used to fit the model to each dataset using one Markov chain started at $(V^*,W^*)$.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{LLM Results --- ESP for T=100}
\centering
\includegraphics[width=0.75\textwidth]{basecisESplot100}\\
\includegraphics[width=0.49\textwidth]{altintESplotV100}
\includegraphics[width=0.49\textwidth]{altintESplotW100}
\end{frame}

\begin{frame}
\frametitle{LLM Results --- ESP for T=1000}
\centering
\includegraphics[width=0.75\textwidth]{basecisESplot1000}\\
\includegraphics[width=0.49\textwidth]{altintESplotV1000}
\includegraphics[width=0.49\textwidth]{altintESplotW1000}
\end{frame}

\begin{frame}
\frametitle{LLM Results --- ESP summarized}
Rule of thumb for when each base algorithm has a high ESP for each variable as a function of the true signal-to-noise ratio, $R^*=W^*/V^*$. \\~\\~\\
 \begin{center}
  \begin{tabular}{lccccc}\hline
    Parameter & State & SD & SE & WSD & WSE \\\hline
    V & $R^* < 1$ & $R^* < 1$ & $R^* > 1$ & $R^* < 1$ & $R^* < 1$\\
    W & $R^* > 1$ & $R^* < 1$ & $R^* > 1$ & $R^* > 1$ & $R^* > 1$ \\
      &           &          &           &           &           \\\hline
    Parameter & State-SD        & State-SE       & SD-SE        & Triple            & CIS \\\hline
    V         & $R^* < 1$           & $R^* \not\approx 1$ & $R^* \not\approx 1$ & $R^* \not\approx 1$ & $R^* \not\approx 1$ \\
    W         & $R^* \not\approx 1$ & $R^* > 1$           & $R^* \not\approx 1$ & $R^* \not\approx 1$ & $R^* \not\approx 1$\\\hline
  \end{tabular}\\~\\~\\
\end{center}

Note that as the length of the time series increases, the farther away from one $R^*$ has to be for the sampler to have a high ESP.
\end{frame}

\begin{frame}
\frametitle{LLM Results - log time (minutes) per 1000 effective draws}
\centering
\includegraphics[width=0.75\textwidth]{basecistimeplot100}\\
\includegraphics[width=0.49\textwidth]{altgisVtimeplot100}
\includegraphics[width=0.49\textwidth]{altgisWtimeplot100}
\end{frame}

\begin{frame}
\frametitle{Moving Forward: Computational Issues}
Major bottleneck: drawing from $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$:
\begin{align*}
p(x)\propto {\color{blue}x^{-\alpha-1}}\exp[-ax + b\sqrt{x} {\color{blue}- c/x}] 
\end{align*}
What about when $V$ and $W$ are matrices?\\~

$\to$ Need a better prior. E.g. $\pm \sqrt{V} \sim \N(0,\Lambda_V)$\\~\\

Then in the LLM $p(V|W,\theta,y)=p(V|W,\gamma,y)$ is GIG:
\begin{align*}
p(y) \propto y^{\alpha-1}\exp\left[-\frac{1}{2}(ay + b/y)\right] \qquad \alpha\in\Re; a,b,y>0
\end{align*}
What about when $V$ is a matrix?
\end{frame}

\begin{frame}
\frametitle{Moving Forward: Augmenting $F_t$}
Suppose $F_t=\begin{bmatrix}1 & x_t\end{bmatrix}$ and $G_t=1$:
\begin{align*}
y_t &= \begin{bmatrix}1 & x_t\end{bmatrix}\begin{bmatrix}\alpha_t \\ \beta_t\end{bmatrix} + v_t, \qquad v_t\stackrel{iid}{\sim}\N_1(0,V)\\
\begin{bmatrix}\alpha_t \\ \beta_t\end{bmatrix} &= \begin{bmatrix}\alpha_{t-1} \\ \beta_{t-1}\end{bmatrix} + \begin{bmatrix}w_{1,t} \\ w_{2,t}\end{bmatrix}, \qquad \begin{bmatrix}w_{1,t} \\ w_{2,t}\end{bmatrix}\sim\N_2(0,W)
\end{align*}
Easy way to augment the model:
\begin{align*}
\begin{bmatrix}y_t \\ y_t^*\end{bmatrix} &= \begin{bmatrix}1 & x_t\\ 0 & 1\end{bmatrix}\begin{bmatrix}\alpha_t \\ \beta_t\end{bmatrix} + \begin{bmatrix}v_t \\ v_t^* \end{bmatrix}, \qquad \begin{bmatrix} v_t\\ v_t^* \end{bmatrix}\stackrel{iid}{\sim}\N_2\left(0,\begin{bmatrix}V & 0 \\ 0 & 1\end{bmatrix}\right)\\
\begin{bmatrix}\alpha_t \\ \beta_t\end{bmatrix} &= \begin{bmatrix}\alpha_{t-1} \\ \beta_{t-1}\end{bmatrix} + \begin{bmatrix}w_{1,t} \\ w_{2,t}\end{bmatrix}, \qquad \begin{bmatrix}w_{1,t} \\ w_{2,t}\end{bmatrix}\sim\N_2(0,W)
\end{align*}
and add a step to draw $y_{1:T}^*|V,W,\alpha_{1:T},\beta_{1:T},y_{1:T}$.\\~\\

But is this the best way? Or even a good way?
\end{frame}

\begin{frame}[allowframebreaks]
        \frametitle{References}
        \bibliographystyle{plainnat}
        \bibliography{../doc/dlmasis}
\end{frame} 

\end{document}

\end{document}





  \frametitle{Takeaways from Base Algorithm Simulations}
  When the signal-to-noise ratio ($W/V$) is low ($<1$), the scaled disturbance sampler has high $ESP$ for both $V$ and $W$; when it's high ($>1$), the scaled disturbance sampler has low $ESP$ for both $V$ and $W$.\\~\\
  
  When the signal-to-noise ratio ($W/V$) is low ($<1$), the scaled error sampler has high $ESP$ for both $V$ and $W$; when it's high ($>1$), the scaled error sampler has low $ESP$ for both $V$ and $W$.\\~\\
  
  The state sampler agrees with the scaled disturbance sampler about $V$ and with the scaled error sampler about $W$. It's at it's best for $(V,W)$ when the signal-to-noise ratio is near $1$.\\~\\
  
  Despite not being an SA-AA pair, $\gamma_{0:T}$ and $\psi_{0:T}$ make a nice beauty and the beast pair.
  


  The ``beauty and the beast'' intuition works for any given parameter, i.e. for $W$, the Dist-Error and State-Dist algorithms have the best ESP, but the State-Error algorithms has worse ESP in some regions of the parameter space.\\~\\
  
\begin{frame}
\frametitle{Recommendations in the Local Level Model}
If $T$ is small ($<100$), use the state sampler.\\~\\

If $T$ is large, use the Dist-Error GIS sampler, but spend some time obtaining efficient draws from the complex densities --- $p(W|\gamma_{0:T},V,y_{1:T}$ and $p(V|\psi_{0:T},W,y_{1:T})$.\\~\\
\pause

Possibly use metropolis steps for $(V,W)$ jointly in the Dist-Error GIS sampler --- untested, but likely has decent mixing properties and avoids the expensive sampling steps.
\end{frame}



