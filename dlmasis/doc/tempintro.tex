\documentclass[12pt]{article}
\usepackage{JASA_manu} %formats document like ASA wants
\usepackage{natbib} %formats citations like ASA wants
\usepackage{amssymb, amsmath, amsthm, graphics, graphicx, color, fullpage}
\usepackage{thmtools} %to format the Algorithm environment correctly
\usepackage[hidelinks]{hyperref}
\usepackage{nameref, cleveref} %for named references
\usepackage{zref-xr, zref-user} %for hyperlinked references to outside docs

\zexternaldocument*{dlmasisapp} %supplementary materials
%use \zref{label} to reference items there

% define algorithm environment
\declaretheoremstyle[
notefont=\bfseries, notebraces={}{},
bodyfont=\normalfont\itshape,
headformat=\NAME:\NOTE
]{nopar}
\declaretheorem[style=nopar, name=Algorithm,
refname={Algorithm,Algorithms},
Refname={Algorithm,Algoritms},
numbered=no]{alg*}

\newtheorem{thm}{Theorem}[subsection]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\vect}{vec}

\graphicspath{{plots/}}
\newcommand{\matt}[1]{{\color{red} Matt: #1}}
\newcommand{\jarad}[1]{{\color{red} Jarad: #1}}

\usepackage[page,header]{appendix}
\usepackage{titletoc}


\begin{document}


\title{Interweaving Markov Chain Monte Carlo Strategies for Efficient
Estimation of Dynamic Linear Models}
\author{Matthew Simpson\textsuperscript{1}, Jarad Niemi\textsuperscript{2}, Vivekananda Roy\textsuperscript{2}}
\date{\today}
\maketitle

\begin{center}
 \vspace{-.5em}
 {\small \textsuperscript{1} Iowa State University, Deptartments of Statistics and Economics, Ames, IA 50011-1210\\[-.3em]
 {\tt simpsonm@iastate.edu}}
 
 \vspace{.5em}

{\small \textsuperscript{2} Iowa State University, Department of Statistics, Ames, IA 50011-1210\\[-.3em]
{\tt <niemi,vroy>@iastate.edu}}

\end{center}



\section{Introduction}

The Data Augmentation (DA) algorithm of \citet{tanner1987calculation} and the closely related EM algorithm of \citet{dempster1977maximum} have become widely used strategies for computing posterior distributions and maximum likelihood estimates. There is a long history of using ideas from the EM literature to inform the construction of DA algorithms and vice versa \citep{meng1997algorithm,van2010cross}, and a large literature has grown up around the various possible improvements to both algorithms \citep{meng1997algorithm,meng1999seeking,liu1999parameter,hobert2008theoretical,yu2011center} though much of the work on constructing and using DAs has focused on hierarchical models \citep{gelfand1995efficient,roberts1997updating,meng1998fast,van2001art,bernardo2003non,papaspiliopoulos2007general,papaspiliopoulos2008stability}. Relatively little attention has been paid to time series models despite similarities between some time series and hierarchical models, though there are some examples \citep{pitt1999analytic,fruhwirth2003bayesian,fruhwirth2006auxiliary}. 

We seek to improve DA schemes in dynamic linear models (DLMs), i.e. linear Gaussian state-space models. Much of the existing literature focuses on non-Gaussian state-space models --- in particular the stochastic volatility model and models based on it \citep{shephard1996statistical,fruhwirth2003bayesian,roberts2004bayesian,bos2006inference,strickland2008parameterisation,fruhwirth2008heston,kastner2013ancillarity}, but few directly work with the class of DLMs we consider \citep{fruhwirth1994data,carter1994gibbs,fruhwirth2004efficient}. In particular, we introduce several new DAs for a general class of DLMs and use them to apply the interweaving strategies of \citet{yu2011center}. We also show that no practical sufficient augmentation exists for the DLM, which restricts the sort of interweaving algorithms we can construct. Using the local level model, we fit the model to simulated data using a variety of the MCMC strategies we discuss in order to assess the relative performance of the algorithms.

The rest of the paper is organized as follows. In Section \ref{sec:DA} we review the data augmentation literature while in Section \ref{sec:DLM} we introduce the dynamic linear model, discuss the subclass of DLMs we consider, and explore some of the key properties of the model. Section \ref{sec:DAs} explores several possible DAs for our class of DLMs and shows that any sufficient augmentation is likely to be difficult to use. Section \ref{sec:Algs} discusses the various MCMC strategies available for the DLM while Section \ref{sec:LLM} applies these algorithms to the local level model. Finally, Section \ref{sec:Discuss} discusses these results and suggests directions for further research while Section \ref{sec:Supp} contains an index to the supplementary materials.

\section{Variations of data augmentation}\label{sec:DA}

Suppose $p(\phi|y)$ is a probability density, in this case the posterior distribution of some parameter $\phi$ given data $y$. %We use $p(.)$ to denote the probability density of the enclosed random variables.
Then a data augmentation algorithm adds $\theta$ with joint distribution $p(\phi,\theta|y)$ such that $\int_{\Theta}p(\phi,\theta|y)d\theta = p(\phi|y)$. The DA algorithm is similar to a Gibbs sampler, except it constructs a Markov chain for $\phi$ instead of $(\phi, \theta)$. In this DA algorithm, the $k+1$'st state of $\phi$ is obtained from the $k$'th state as follows (we implicitly condition on the data $y$ in all algorithms and only superscript the previous and new draws of the model parameters of interest):
\begin{alg*}[DA]Data Augmentation\label{alg:DA}
  \begin{align*}
  [\theta|\phi^{(k)}] \to [\phi^{(k+1)}|\theta]
\end{align*}
\end{alg*}
\noindent where $[\theta|\phi^{(k)}]$ means a draw of $\theta$ from $p(\theta|\phi^{(k)},y)$ and $[\phi^{(k+1)}|\theta]$ means a draw from $p(\phi|\theta,y)$ where $y$ is implicitly conditioned on in this and all algorithm descriptions. The DA, $\theta$, need not be interesting in any scientific sense --- it can be viewed purely as a computational construct. However for cases where the DA is intrinsically interesting, the DA algorithm does incidentally obtain joint draws from $p(\phi,\theta|y)$ and in that sense can be viewed as a Gibbs sampler. 

\subsection{Reparameterization and alternating DAs}

\jarad{If we like the SA-AA terminology, we should introduce it first and define what it is.}
One well known method of improving mixing and convergence in MCMC samplers (and convergence in EM algorithms) is reparameterization of the model (see  \citet{papaspiliopoulos2007general} and references therein). The parameterization $(\phi,\theta)$ is a centered parameterization (CP) if $p(y|\theta,\phi)=p(y|\theta)$ and $\theta$ is called a centered augmentation (CA). If $p(\theta|\phi)=p(\theta)$, the parameterization is a noncentered parameterization (NCP) and $\theta$ is called a noncentered augmentation (NCA) for $\phi$. A CA is sometimes called a {\it sufficient augmentation} (SA) and a NCA is sometimes called an ancillary augmentation (AA) \cite{yu2011center}. We prefer the latter terminology because it immediately suggests the intuition that an SA is like a sufficient statistic while an AA is like an ancillary statistic and hence Basu's theorem suggests that they are conditionally independent given $\phi$ \cite{basu1955statistics}.

The key reasoning behind the emphasis on SAs and AAs is that typically when the DA algorithm based on the SA has nice mixing and convergence properties, the DA algorithm based on the AA has poor mixing and convergence properties and vice-versa. This property suggests combining the two such DA algorithms to construct an improved sampler. One intuitive approach is to alternate between the two augmentations within a Gibbs sampler \citep{papaspiliopoulos2007general}. Suppose we have a second distinct DA $\gamma$ such that $\int_\Gamma p(\phi,\gamma|y)d\gamma = p(\phi|y)$. Then the alternating algorithm for sampling from $p(\phi|y)$ is as follows:
\begin{alg*}[Alt]Alternating Algorithm\label{alg:Alt}
  \begin{center}
    \begin{tabular}{lllllll}
  $[\theta|\phi^{(k)}]$& $\to$& $[\phi|\theta]$& $\to$& $[\gamma|\phi]$& $\to$& $[\phi^{(k+1)}|\gamma]$
    \end{tabular}
  \end{center}
\end{alg*}
\noindent One iteration of the alternating algorithm consists of one iteration of the DA algorithm based on $\theta$ to obtain an intermediate value of $\phi$, followed by one iteration of the DA algorithm based on $\gamma$ started at the intermediate value of $\phi$.

Recall that a typical problem with slow MCMC is that there is high autocorrelation in the Markov chain for $\phi$. In Algorithm \nameref{alg:DA}, when $\phi$ and $\theta$ are highly dependent in the joint posterior, the draws from $p(\theta|\phi,y)$ and then from $p(\phi|\theta,y)$ will hardly move the chain, resulting in high autocorrelation. In an alternating algorithm, there are essentially two chances to substantially move the chain -- one for $\theta$ in the draws from $p(\theta|\phi,y)$ and from $p(\phi|\theta,y)$ and one for $\gamma$ in the draws from $p(\gamma|\phi,y)$ and from $p(\phi|\gamma,y)$.  In particular, when $\theta$ is an SA and $\gamma$ is an AA or vice versa, then it is often the case that there is low dependence either between $\phi$ and $\theta$ or between $\phi$ and $\gamma$ and thus the alternating strategy will improve mixing.

\subsection{Interweaving: an alternative to alternating}\label{sec:Intro:int}

Another option is to {\it interweave} the two DAs together \cite{yu2011center}. A general interweaving strategy (GIS) is an MCMC algorithm that obtains $\phi^{(k+1)}$ from $\phi^{(k)}$ as follows:
\begin{alg*}[GIS]General Interweaving Strategy\label{alg:GIS}
  \begin{align*}
    [\theta|\phi^{(k)}] \to [\gamma|\theta] \to [\phi^{(k+1)}|\gamma].
  \end{align*}
\end{alg*}
\noindent The GIS algorithm obtains the next iteration of the parameter $\phi$ in three steps: 1) draw $\theta$ conditional on $\phi^{(k)}$, 2) draw $\gamma$ conditional on $\theta$, and 3) draw $\phi^{(k+1)}$ conditional on $\gamma$. This looks similar to the usual DA algorithm except a second DA is ``weaved'' in between the draw of the first DA and of the parameter vector. 

The second step of the GIS algorithm is often accomplished by sampling $\phi|\theta$ and then $\gamma|\theta,\phi$. If we expand out step two, then the GIS algorithm becomes:
\begin{alg*}[eGIS]Expanded GIS\label{alg:GIS2}
  \begin{center}
    \begin{tabular}{lllllll}
      $[\theta|\phi^{(k)}]$& $\to$& $[\phi|\theta]$& $\to $&$[\gamma|\theta,\phi]$& $\to$& $[\phi^{(k+1)}|\gamma]$
    \end{tabular}
  \end{center}
\end{alg*}
\noindent
In addition, $\gamma$ and $\theta$ are often, but not always, one-to-one transformations of each other conditional on $(\phi,y)$, i.e. $\gamma = M(\theta;\phi,y)$ where $M(.;\phi,y)$ is a one-to-one function, and thus $[\gamma|\theta,\phi]$ is deterministic.
The key difference between Algorithm \nameref{alg:GIS} and Algorithm \nameref{alg:Alt} can be seen in step three Algorithm \nameref{alg:GIS2}: instead of drawing from $p(\gamma|\phi,y)$, the GIS algorithm draws from $p(\gamma|\theta,\phi,y)$, connecting the two DAs together while the alternating algorithm keeps them separate.

\citet{yu2011center} call a GIS approach where one of the DAs is an SA and the other is an AA an ancillary sufficient interweaving strategy (ASIS).
They show that the GIS algorithm has a geometric rate of convergence no worse than the worst of the two underlying DA algorithms and in some cases better than the the corresponding alternating algorithm. 
In particular, their Theorem 1 suggests that the weaker the dependence between the two DAs in the posterior, the more efficient the GIS algorithm. 
With \emph{a posteriori} independent data augmentations, the GIS algorithm will obtain iid draws from the posterior for $\phi$. 
This helps motivate their focus on ASIS --- conditional on the model parameter, an SA and an AA are independent under the conditions of Basu's theorem \cite{basu1955statistics}, which suggests that the dependence between the two DAs will be limited in the posterior. 
In fact, when the prior on $\phi$ is nice in some sense, \citet{yu2011center} show that the ASIS algorithm is the same as the optimal parameter expanded data augmentation (PX-DA) algorithm of \citet{liu1999parameter}, which is closely related to marginal and conditional augmentation \cite{meng1999seeking,hobert2008theoretical}.

%\jarad{didn't we just explain the intuition?}
%To gain some intuition about why interweaving can be superior to alternating, we can compare the \nameref{alg:Alt} and \nameref{alg:GIS2} algorithms. Similar to the alternating algorithm, there are two opportunities for the Markov chain to move in the GIS algorithm, one each for $\theta$ and $\gamma$. So when the two underlying DA algorithms perform poorly in opposite regions of the parameter space, like, either steps 1 and 2 or steps 3 and 4 will substantially move the chain, limiting how poor Algorithm \nameref{alg:GIS} can perform -- much like Algorithm \nameref{alg:Alt}. However, \citet{yu2011center} show that the GIS algorithm is often much more efficient than the alternating algorithm. When the posterior dependence between the two DAs is low, step 2 in Algorithm \nameref{alg:GIS} (i.e. steps 2 and 3 in Algorithm \nameref{alg:GIS2}) is enough to almost completely break the dependence in the chain. For Algorithm \nameref{alg:Alt}, it is typically not feasible to find a data augmentation such that its step 2 or step 3 completely breaks the dependence in the chain --- this would require finding a DA such that the model parameter and the DA are essentially independent which, in turn, would likely mean that drawing from the conditional posterior of the parameter given the DA is nearly as difficult as drawing from the marginal posterior of the model parameter.

In addition to the GIS, it is possible to define a componentwise interweaving strategy (CIS) that interweaves within specific steps of a Gibbs sampler as well. A CIS algorithm for $\phi=(\phi_1, \phi_2)$ essentially employs interweaving for each block of $\phi$ separately, e.g.
\begin{alg*}[CIS]Componentwise Interweaving Strategy\label{alg:CIS}
  \begin{center}
    \begin{tabular}{llllll}
      $[\theta_1|\phi_1^{(k)},\phi_2^{(k)}]$ & $\to$  & $[\gamma_1|\phi_2^{(k)},\theta_1]$ & $\to$ & $[\phi_1^{(k+1)}|\phi_2^{(k)},\gamma_1]$ &$\to$ \\
      $[\theta_2|\phi_1^{(k+1)},\phi_2^{(k)},\gamma_1]$ &$\to$ & $[\gamma_2|\phi_1^{(k+1)},\theta_2]$ & $\to$ & $[\phi_2^{(k+1)}|\phi_1^{(k+1)},\gamma_2]$ &
    \end{tabular}
  \end{center}
\end{alg*}\noindent
where $\theta_i$ and $\gamma_i$ are distinct data augmentations for $i=1$ and $i=2$, but potentially $\gamma_1=\theta_2$  or $\gamma_2=\theta_1$. The first row draws $\phi_1$ conditional on $\phi_2$ using interweaving in a Gibbs step, while the second row does the same for $\phi_2$ conditional on $\phi_1$. The algorithm can easily be extended to greater than two blocks within $\phi$. The main attraction of CIS is that it is often easier to find an AA--SA pair of DAs for $\phi_1$ conditional on $\phi_2$ and another pair for $\phi_2$ conditional on $\phi_1$ than it is to find and AA--SA pair for $\phi=(\phi_1,\phi_2)$ jointly.


\clearpage
\bibliographystyle{ECA_jasa_matt}  % proper bibliography style for ASA
\bibliography{dlmasis}
\end{document}
