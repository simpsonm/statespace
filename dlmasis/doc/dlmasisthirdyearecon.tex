\documentclass[10pt]{article}
\usepackage{JASA_manu} %formats document like ASA wants
\usepackage{natbib} %formats citations like ASA wants
\usepackage{amssymb, amsmath, amsthm, graphics, graphicx, color, fullpage}
\usepackage{psfrag,epsf}
\usepackage{enumerate}
\usepackage{thmtools} %to format the Algorithm environment correctly
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true,    % Colours links instead of ugly boxes
  urlcolor     = blue,    % Colour for external hyperlinks
  linkcolor    = blue,    % Colour of internal links
  citecolor    = red      % Colour of citations
}
\usepackage{nameref, cleveref} %for named references
\usepackage[page,header]{appendix}
\usepackage{titletoc}
\usepackage{subcaption} % for subfigures

% define algorithm environment
\declaretheoremstyle[
notefont=\bfseries, notebraces={}{},
bodyfont=\normalfont\itshape,
headformat=\NAME:\NOTE
]{nopar}
\declaretheorem[style=nopar, name=Algorithm,
refname={Algorithm,Algorithms},
Refname={Algorithm,Algoritms},
numbered=no]{alg*}

\newtheorem{lem}{Lemma}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\vect}{vec}

\graphicspath{{plots/}}

\newcommand{\blind}{0} %%change the 0 to a 1 to compile blinded version
%% Note: this affects some article text in the Discussion section.
%% Also note: rerun bibtex

%% From jcgs template
% \addtolength{\oddsidemargin}{-.75in}%
% \addtolength{\evensidemargin}{-.75in}%
% \addtolength{\textwidth}{1.5in}%
% \addtolength{\textheight}{1.3in}%
% \addtolength{\topmargin}{-.8in}%


\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\if0\blind
{
  \title{\bf Interweaving Markov Chain Monte Carlo Strategies for Efficient
    Estimation of Dynamic Linear Models}
  \author{Matthew Simpson\thanks{
    The authors thank the participants of the Economics, Finance, and Business workshop at the Bayes 250 conference for helpful comments, though all errors are our own.}\hspace{.2cm}\\
    Departments of Statistics and Economics, Iowa State University\\~\\
    Jarad Niemi \\
    Department of Statistics, Iowa State University\\~\\
    Vivekananda Roy\\
    Department of Statistics, Iowa State University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Interweaving Markov Chain Monte Carlo Strategies for Efficient
    Estimation of Dynamic Linear Models}
\end{center}
  \medskip
} \fi

\bigskip


\begin{abstract}
In dynamic linear models (DLMs) with unknown fixed parameters, a standard Markov chain Monte Carlo (MCMC) sampling strategy is to alternate sampling of latent states conditional on fixed parameters and sampling of fixed parameters conditional on latent states. In some regions of the parameter space, this standard data augmentation (DA) algorithm can be inefficient. To improve efficiency, we seek to employ the interweaving strategies of \citet{yu2011center} that combine separate DAs by weaving them together. For this, we introduce a number of novel alternative DAs for a general class of DLMs: the scaled errors, wrongly-scaled errors, and wrongly-scaled disturbances. With the latent states and the less commonly used scaled disturbances, this yields five unique DAs to employ in MCMC algorithms. Each DA implies a unique MCMC sampling strategy and they can be combined into interweaving or alternating strategies that improve MCMC efficiency. We assess the strategies using the local level DLM and demonstrate that several strategies improve efficiency relative to the standard approach, the most efficient being either interweaving or alternating the scaled errors and scaled disturbances.
\end{abstract}


\noindent%
{\bf Key Words:} Ancillary augmentation; Centered parameterization; Data augmentation; Non-centered parameterization; Reparameterization; Sufficient augmentation; Time series; State-space model

\spacingset{2}

\section{Introduction}

The Data Augmentation (DA) algorithm of \citet{tanner1987calculation} and the closely related EM algorithm of \citet{dempster1977maximum} have become widely used strategies for computing posterior distributions and maximum likelihood estimates and there is a long history of using ideas from the EM literature to inform the construction of DA algorithms and vice versa \citep{meng1997algorithm,van2010cross}. While DA and EM algorithms are useful, they often suffer from slow convergence in practice and a large literature has grown up around the various possible improvements to both algorithms \citep{meng1997algorithm,meng1999seeking,liu1999parameter,hobert2008theoretical,yu2011center} though much of the work on constructing improved algorithms has focused on hierarchical models \citep{gelfand1995efficient,roberts1997updating,meng1998fast,van2001art,bernardo2003non,papaspiliopoulos2007general,papaspiliopoulos2008stability}. Relatively little attention has been paid to time series models despite similarities between some time series and hierarchical models, though there are some exceptions \citep{pitt1999analytic,fruhwirth2003bayesian,fruhwirth2006auxiliary}. 

We seek to improve DA schemes in dynamic linear models (DLMs), i.e. linear Gaussian state-space models. The standard DA scheme uses the latent states and alternates between drawing from the full conditional distributions of the latent states and the model parameters \citep{fruhwirth1994data,carter1994gibbs}. The existing literature on improving DA schemes tends to focus on non-Gaussian state-space models --- particularly the stochastic volatility model and models based on it \citep{shephard1996statistical,fruhwirth2003bayesian,roberts2004bayesian,bos2006inference,strickland2008parameterisation,fruhwirth2008heston,kastner2013ancillarity}, but a few work with the class of DLMs we consider \citep{fruhwirth2004efficient}. One recent development in the DA literature is an ``interweaving'' strategy for using two separate DAs in a single algorithm by weaving them together in a particular fashion \citep{yu2011center}. This strategy draws on the strengths of both underlying DA algorithms in order to construct an MCMC algorithm which is at least as efficient as the worst of the two DA algorithms and typically at least as efficient as the best. We implement interweaving algorithms in a general class of DLMs and in order to do so we introduce several new DAs for this class of models. We also show under some assumptions that no practical sufficient augmentation (centered augmentation) exists for the DLM, which restricts the sort of interweaving algorithms we can construct. Using the local level model, we fit the model to simulated data using a variety of the MCMC strategies we discuss in order to assess their relative performance.

The rest of the paper is organized as follows. In Section \ref{sec:DA} we review the data augmentation literature while in Section \ref{sec:DLM} we introduce the dynamic linear model, discuss the subclass of DLMs we consider, and explore some of the key properties of the model. Section \ref{sec:DAs} explores several possible DAs for our class of DLMs and shows that any sufficient augmentation is likely to be difficult to use. Section \ref{sec:Algs} discusses the various MCMC strategies available for the DLM while Section \ref{sec:LLM} applies these algorithms to the local level model. Finally, Section \ref{sec:Discuss} discusses these results and suggests directions for further research. In the appendix, sections \ref{sec:Proof}, \ref{sec:DLMfullcond}, \ref{sec:MCFA}, \ref{sec:F}, \ref{sec:scaledraw}, \ref{sec:wscale}, \ref{sec:PostCorr} and \ref{sec:plots} all contain material referenced in the main body of the paper.

\section{Variations of data augmentation}\label{sec:DA}

Suppose $p(\phi|y)$ is a probability density, for example the posterior distribution of some parameter $\phi$ given data $y$. %We use $p(.)$ to denote the probability density of the enclosed random variables.
Then a data augmentation algorithm adds a data augmentation $\theta$ with joint distribution $p(\phi,\theta|y)$ such that $\int_{\Theta}p(\phi,\theta|y)d\theta = p(\phi|y)$. The DA algorithm is similar to a Gibbs sampler except it constructs a Markov chain for $\phi$ instead of $(\phi, \theta)$. In this DA algorithm, the $k+1$'st state of $\phi$ is obtained from the $k$'th state as follows (we implicitly condition on the data $y$ in all algorithms and only superscript the previous and new draws of the model parameters of interest):
\begin{alg*}[DA]Data Augmentation\label{alg:DA}
  \begin{align*}
  [\theta|\phi^{(k)}] \to [\phi^{(k+1)}|\theta]
\end{align*}
\end{alg*}
\noindent where $[\theta|\phi^{(k)}]$ means a draw of $\theta$ from $p(\theta|\phi^{(k)},y)$ and $[\phi^{(k+1)}|\theta]$ means a draw from $p(\phi|\theta,y)$. The DA need not be interesting in any scientific sense --- it can be viewed purely as a computational construct. 

\subsection{Reparameterization and alternating DAs}

One well known method of improving mixing and convergence in MCMC samplers as well as convergence in EM algorithms is reparameterization of the model (see  \citet{papaspiliopoulos2007general} and references therein). The DA $\theta$ is called a {\it sufficient augmentation} (SA) for the model parameter $\phi$ if $p(y|\theta,\phi)=p(y|\theta)$. Similarly $\theta$ is called an {\it ancillary augmentation} (AA) for $\phi$ if $p(\theta|\phi)=p(\theta)$. An SA is sometimes called a centered augmentation or centered parameterization in the literature while an AA is sometimes called a non-centered augmentation or non-centered parameterization. Like \citet{yu2011center} we prefer the SA and AA terminology because it suggests a connection with Basu's theorem \citep{basu1955statistics}, which we will return to in Section \ref{sec:Intro:int}.

A key reason behind the emphasis on SAs and AAs is that typically when the DA algorithm based on the SA has nice mixing and convergence properties, the DA algorithm based on the AA has poor mixing and convergence properties and vice-versa. This property suggests combining the two such DA algorithms to construct an improved sampler. One intuitive approach is to alternate between the two augmentations within a Gibbs sampler \citep{papaspiliopoulos2007general}. Suppose we have a second distinct DA $\gamma$ such that $\int_\Gamma p(\phi,\gamma|y)d\gamma = p(\phi|y)$, then the alternating algorithm for sampling from $p(\phi|y)$ is as follows:
\begin{alg*}[Alt]Alternating Algorithm\label{alg:Alt}
  \begin{center}
    \begin{tabular}{lllllll}
  $[\theta|\phi^{(k)}]$& $\to$& $[\phi|\theta]$& $\to$& $[\gamma|\phi]$& $\to$& $[\phi^{(k+1)}|\gamma]$
    \end{tabular}
  \end{center}
\end{alg*}
\noindent One iteration of the alternating algorithm consists of one iteration of the DA algorithm based on $\theta$ to obtain an intermediate value of $\phi$, followed by one iteration of the DA algorithm based on $\gamma$ started at the intermediate value of $\phi$.

When $\phi$ and $\theta$ are highly dependent in their joint posterior, the draws from $p(\theta|\phi,y)$ and $p(\phi|\theta,y)$ will hardly move the chain in Algorithm \nameref{alg:DA}, resulting in high autocorrelation. In an alternating algorithm, there are essentially two chances to substantially move the chain -- one using $\theta$ and the other using $\gamma$. In particular, when $\theta$ is an SA and $\gamma$ is an AA or vice versa, then often there is low dependence either between $\phi$ and $\theta$ or between $\phi$ and $\gamma$ and thus the alternating strategy will improve mixing over the DA algorithms based on $\theta$ or $\gamma$.

\subsection{Interweaving: an alternative to alternating}\label{sec:Intro:int}

Another option is to {\it interweave} the two DAs together \citep{yu2011center}. A general interweaving strategy (GIS) is an MCMC algorithm that obtains $\phi^{(k+1)}$ from $\phi^{(k)}$ as follows:
\begin{alg*}[GIS]General Interweaving Strategy\label{alg:GIS}
  \begin{align*}
    [\theta|\phi^{(k)}] \to [\gamma|\theta] \to [\phi^{(k+1)}|\gamma].
  \end{align*}
\end{alg*}
\noindent The GIS algorithm obtains the next iteration of the parameter $\phi$ in three steps: 1) draw $\theta$ conditional on $\phi^{(k)}$, 2) draw $\gamma$ conditional on $\theta$, and 3) draw $\phi^{(k+1)}$ conditional on $\gamma$. This looks similar to the usual DA algorithm except a second DA is ``weaved'' in between the draw of the first DA and of the parameter vector. 

The second step of the GIS algorithm is often accomplished by sampling $\phi|\theta$ and then $\gamma|\theta,\phi$. If we expand this out, then the GIS algorithm becomes:
\begin{alg*}[eGIS]Expanded GIS\label{alg:eGIS}
  \begin{center}
    \begin{tabular}{lllllll}
      $[\theta|\phi^{(k)}]$& $\to$& $[\phi|\theta]$& $\to $&$[\gamma|\theta,\phi]$& $\to$& $[\phi^{(k+1)}|\gamma]$
    \end{tabular}
  \end{center}
\end{alg*}
\noindent
In addition, $\gamma$ and $\theta$ are often, but not always, one-to-one transformations of each other conditional on $(\phi,y)$, i.e. $\gamma = M(\theta;\phi,y)$ where $M(.;\phi,y)$ is a one-to-one function, and thus $[\gamma|\theta,\phi]$ is deterministic.
The key difference between Algorithm \nameref{alg:GIS} and Algorithm \nameref{alg:Alt} can be seen in step three of Algorithm \nameref{alg:eGIS}: instead of drawing from $p(\gamma|\phi,y)$, the GIS algorithm draws from $p(\gamma|\theta,\phi,y)$, connecting the two DAs together while the alternating algorithm keeps them separate.

\citet{yu2011center} call a GIS approach where one of the DAs is an SA and the other is an AA an ancillary sufficient interweaving strategy (ASIS). They show that the GIS algorithm has a geometric rate of convergence no worse than the worst of the two underlying DA algorithms and in some cases better than the the corresponding alternating algorithm. In particular, their Theorem 1 suggests that the weaker the dependence between the two DAs in the posterior, the more efficient the GIS algorithm. With \emph{a posteriori} independent data augmentations, the GIS algorithm obtains iid draws from the posterior for $\phi$. This helps motivate their focus on ASIS and the choice of terminology --- conditional on the model parameter, an SA and an AA are independent under the conditions of Basu's theorem \citep{basu1955statistics}, which suggests that the dependence between the two DAs will be limited in the posterior. In fact, when the prior on $\phi$ is nice in some sense, \citet{yu2011center} show that the ASIS algorithm is the same as the optimal parameter expanded data augmentation (PX-DA) algorithm \citep{liu1999parameter}, which is closely related to marginal and conditional augmentation \citep{meng1999seeking,hobert2008theoretical}.

In addition to the GIS, it is possible to define a componentwise interweaving strategy (CIS) that interweaves within specific steps of a Gibbs sampler as well. A CIS algorithm for $\phi=(\phi_1, \phi_2)$ essentially employs interweaving for each block of $\phi$ separately, e.g.
\begin{alg*}[CIS]Componentwise Interweaving Strategy\label{alg:CIS}
  \begin{center}
    \begin{tabular}{llllll}
      $[\theta_1|\phi_1^{(k)},\phi_2^{(k)}]$ & $\to$  & $[\gamma_1|\phi_2^{(k)},\theta_1]$ & $\to$ & $[\phi_1^{(k+1)}|\phi_2^{(k)},\gamma_1]$ &$\to$ \\
      $[\theta_2|\phi_1^{(k+1)},\phi_2^{(k)},\gamma_1]$ &$\to$ & $[\gamma_2|\phi_1^{(k+1)},\theta_2]$ & $\to$ & $[\phi_2^{(k+1)}|\phi_1^{(k+1)},\gamma_2]$ &
    \end{tabular}
  \end{center}
\end{alg*}\noindent
where $\theta_i$ and $\gamma_i$ are distinct data augmentations for $i=1,2$, but potentially $\gamma_1=\theta_2$  or $\gamma_2=\theta_1$. The first row draws $\phi_1$ conditional on $\phi_2$ using interweaving in a Gibbs step, while the second row does the same for $\phi_2$ conditional on $\phi_1$. The algorithm can easily be extended to greater than two blocks within $\phi$. The main attraction of CIS is that it is often easier to find an AA--SA pair of DAs for $\phi_1$ conditional on $\phi_2$ and another pair for $\phi_2$ conditional on $\phi_1$ than it is to find and AA--SA pair for $\phi=(\phi_1,\phi_2)$ jointly.

\section{Dynamic linear models} \label{sec:DLM}

The general dynamic linear model is well studied \citep{harrison1999bayesian,petris2009dynamic,prado2010time} and is defined as
\begin{align*}
y_t &= F_t\theta_t + v_t && v_t \stackrel{ind}{\sim} N_k(0,V_t) && (\mbox{observation equation}) \\
 \theta_t &= G_t\theta_{t-1} + w_t && w_t \stackrel{ind}{\sim} N_p(0,W_t) && (\mbox{system equation}) 
\end{align*}
where $N_d(\mu,\Sigma)$ is a $d$-dimensional multivariate normal distribution with mean $\mu$ and covariance $\Sigma$ and the observation errors, $v_{t}$ for $t=1,2,\cdots,T$, and system disturbances, $w_{t}$ for $t=1,2,\cdots,T$, are independent. The observed data is $y\equiv y_{1:T} \equiv (y_1',y_2',\cdots, y_T')'$ while the latent states are $\theta \equiv \theta_{0:T} \equiv (\theta_0',\theta_1',\cdots, \theta_T')'$ and are the standard DA for this model. For each $t=1,2,\cdots,T$, $F_t$ is a $k\times p$ matrix and $G_t$ is a $p\times p$ matrix. Let $\phi$ denote the vector of unknown parameters in the model. Then possibly $F_{t}$, $G_{t}$, $V_{t}$, and $W_{t}$ are all functions of $\phi$ for $t=1,2,\cdots,T$.

The subclass of DLMs we will focus on sets $V_t=V$ and $W_t=W$ and treats $F_{t}$ and $G_{t}$ as known for all $t$. Our results can be extended when $V_t$ or $W_t$ is time-varying or when $F_t$ or $G_t$ depend on unknown parameters, but we ignore those cases for simplicity. As a result $\phi=(V,W)$ is our unknown parameter and we can write the model as
\begin{align}
  y_t|\theta,V,W \stackrel{ind}{\sim} & N_k(F_t\theta_t,V) &
  \theta_t|\theta_{0:t-1},V,W  \sim & N_p(G_t\theta_{t-1},W) \label{dlmbotheqs}
\end{align}
for $t=1,2,\cdots T$. We use the standard conditionally conjugate priors, that is $\theta_0$, $V$, and $W$ independent with $\theta_0 \sim N_p(m_0, C_0)$, $V \sim IW(\Lambda_V, \lambda_V)$ and $W \sim IW(\Lambda_W, \lambda_W)$ where $m_0$, $C_0$, $\Lambda_V$, $\lambda_V$, $\Lambda_W$, and $\lambda_W$ are known hyperparameters and $IW(\Lambda, \lambda)$ denotes the inverse Wishart distribution with degrees of freedom $\lambda$ and positive definite scale matrix $\Lambda$.

\subsection{The implied marginal model}\label{sec:DLM:marg}

While the DLM is typically defined using the latent states as a DA, strictly speaking they are unnecessary for the model's definition. Since we view $\theta$ purely as a nuisance parameter added for computational convenience, we wish to characterize the marginal model for $y$. From equation \eqref{dlmbotheqs}, we can rewrite the model by recursive substitution:
\begin{align*}
  y_t &= v_t + F_t\left(w_t + G_tw_{t-1} + G_tG_{t-1}w_{t-2} + ... + G_tG_{t-1}\cdots G_{2}w_1 + G_tG_{t-1}\cdots G_1\theta_0\right).
\end{align*}
Then conditional on $\phi=(V,W)$ each $y_t$ is a linear combination of normal random variables. After marginalizing out $\theta$, $y$ has a normal distribution such that $\mathrm{E}[y_t|\phi] =  F_tH_tm_0$,
\begin{align*}
  \mathrm{Var}[y_t|\phi] =  V + F_t(K_tWK_t' + H_tC_0H_t')F_t' ,\quad \mathrm{and} \quad
  \mathrm{Cov}[y_s,y_t|\phi] = F_s(K_sWK_t' + H_sC_0H_t')F_t',
\end{align*}
where $H_t = G_tG_{t-1}\cdots G_1$ and $K_t = I_p + G_t + G_tG_{t-1} + \cdots + G_tG_{t-1}\cdots G_2$. Next define $D_t = F_tG_tG_{t-1}\cdots G_1$. Then let $\tilde{V}=I_T\otimes V$ and $D$ be block diagonal with elements $D_1,\ldots,D_T$, 
%\begin{align*}
%\tilde{V}_{Tk\times Tk} & = \begin{bmatrix}
%V & 0 & \cdots & 0\\
%0 & V & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots\\
%0 & 0 & \cdots & V
%\end{bmatrix}, &
%D_{Tp\times Tk} &= \begin{bmatrix}
%D_1 & 0 & \cdots & 0\\
%0 & D_2 & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots\\
%0 & 0 & \cdots & D_T
%\end{bmatrix},
%\end{align*}
\begin{align*}
\tilde{W}_{Tk\times Tk} &= \begin{bmatrix} K_1'F_1' & K_2'F_2' & \cdots K_T'F_T' \end{bmatrix}' W \begin{bmatrix} K_1'F_1' & K_2'F_2' & \cdots K_T'F_T' \end{bmatrix}, &\\
\tilde{C}_{Tk\times Tk} &= \begin{bmatrix} H_1'F_1' & H_2'F_2' & \cdots H_T'F_T' \end{bmatrix}' C_0 \begin{bmatrix} H_1'F_1' & H_2'F_2' & \cdots H_T'F_T' \end{bmatrix},&
\end{align*}
and $\tilde{m}_{Tp\times 1} = (m_0', m_0', \cdots m_0')'$. Now we have the data model for $y$ without any data augmentation:
\begin{align}
  y|V,W \stackrel{ind}{\sim} N_{Tk}(D\tilde{m}, \tilde{V} + \tilde{W} + \tilde{C}). \label{margmodel}
\end{align}
%Given a prior $p(\phi)$, the posterior density we are interested in is $p(\phi|y)\propto p(y|\phi)p(\phi)$.

\section{Augmenting the DLM}\label{sec:DAs}

The standard definition of the DLM includes $\theta$ and unsurprisingly, $\theta$ is the standard DA used in estimation of the DLM. Any other DA $\gamma$ would suffice to estimate $\phi$ --the only restriction is that $p(\phi|y) = \int_{\Gamma} p(\phi,\gamma|y)d\gamma$. We now introduce one data augmentation that is known, scaled disturbances, and three other novel augmentations: scaled errors, wrongly-scaled disturbances, and wrongly-scaled errors. The primary purpose of these augmentations is for use in interweaving algorithms, but each DA will also implicitly define a DA algorithm.

\subsection{The scaled disturbances}\label{sec:DAs:dist}

A natural way to create new DAs is by reparameterizing old DAs. \citet{papaspiliopoulos2007general} note that typically the standard augmentation results in an SA for the parameter $\phi$. All that would be necessary for an ASIS algorithm, then, is to construct an AA for $\phi$. We immediately run into a problem because the standard DA for a DLM is $\theta$ but in equation \eqref{dlmbotheqs} $V$ is in the observation equation so that $\theta$ is not an SA for $(V,W)$ while $W$ is in the system equation so that $\theta$ is not an AA for $(V,W)$ either. In order to find an SA we need to somehow move $V$ from the observation equation to the system equation and similarly to find an AA we need to somehow move $W$ from the system equation to the observation equation.

We follow \citet{papaspiliopoulos2007general}'s suggestion to construct a pivotal quantity in order to find an ancillary augmentation. In order to obtain a pivotal quantity for a variance parameter we can center the sufficient augmentation and scale it by the square root of the variance parameter. Notice from equation \eqref{dlmbotheqs} that if we hold $V$ constant then $\theta$ is an SA for $W$ conditional on $V$, i.e. for $W|V$. Similarly $\theta$ is an AA for $V|W$. This suggests that if we center and scale $\theta_{t}$ by $W$ appropriately for all $t$ we will have an ancillary augmentation for $V$ and $W$ jointly, thus creating the {\it scaled disturbances} (SDs).

In the general DLM the disturbances are vectors, so to define the scaled disturbances let $L_W$ denote the Cholesky decomposition of $W$, i.e. the lower triangle matrix $L_W$ such that $L_WL_W' =W$. Then we will define the scaled disturbances $\gamma\equiv\gamma_{0:T}\equiv(\gamma_0',\gamma_1',\cdots,\gamma_T')'$ by $\gamma_0=\theta_0$ and $\gamma_t = L_W^{-1}(\theta_t-G_t\theta_{t-1})$ for $t=1,2,\cdots,T$. There are actually $p!$ different versions of the scaled disturbances depending on how we order the elements of $\theta_t$, as \citet{meng1998fast} note for EM algorithms in a different class of models. We make no attempt to determine which ordering should be used.
%\matt{There isn't always a most natural ordering. Consider a multivariate local level model}
%Here we will use the most natural ordering.
The reverse transformation is defined recursively by $\theta_0(\gamma,L_W)=\gamma_0$ and $\theta_t(\gamma,L_W)=L_W\gamma_t + G_t\theta_{t-1}(\gamma,L_W)$ for $t=1,2,\cdots,T$. Ultimately under the scaled disturbance parameterization we can write the model as
\begin{equation}
  y_t|\gamma,V,W  \stackrel{ind}{\sim} N_k\left(F_t\theta_t(\gamma,L_W), V\right), \qquad
  \gamma_t  \stackrel{iid}{\sim}N_p(0,I_p) \label{dlmdistmodel}
\end{equation}
for $t=1,2,\cdots,T$ where $I_p$ is the $p\times p$ identity matrix. Neither $V$ nor $W$ are in the system equation so the scaled disturbances are an AA for $(V,W)$. The SDs are well known --- the disturbance smoother of \citet{koopman1993disturbance} finds the conditional posterior of the scaled disturbances given the model parameters and the data and \citet{fruhwirth2004efficient} use the SDs in a dynamic regression model with stationary regression coefficients.

\subsection{The scaled errors}\label{sec:DAs:error}
The scaled disturbances immediately suggest an analogous augmentation called the {\it scaled errors} (SEs), i.e. $v_t=y_t - F_t\theta_t$ appropriately scaled by $V$. Let $L_V$ denote the Cholesky decomposition of $V$ so that $L_VL_V'=V$, then we can define a version of the scaled errors as $\psi_t = L_V^{-1}(y_t - F_t\theta_t)$ for $t=1,2,\cdots,T$ and $\psi_0 = \theta_0$. This time there are $k!$ versions of the scaled errors depending on how $y_t$ is ordered.

%It is not straightforward to write down the model in terms of $\psi=(\psi_0',\psi_1',\cdots,\psi_T)'$ instead of $\theta$ and determine $p(\psi|V,W)$ in general. When $dim(y_t)\!=\!k\!=\!p\!=\!dim(\theta_t)$, $F_t$ is $p\times p$ and is invertible for $t=1,2,\cdots,T$, $\psi$ is a one-to-one transformation of $\theta$ and the problem is easier. This restriction is not necessary --- $\theta_t$, $y_t$ or both could be augmented in order to construct an $\tilde{F}_t$ which is square and invertible using a more complicated data augmentation scheme, but we pass over this complication and assume that $F_t$ is $k\times k$ and invertible. In Appendix D of the supplementary materials, we return to this issue. 

Assuming $F_t$ is invertible for all $t$ (this restriction is relaxed in Appendix D), then $\theta_t = F_t^{-1}(y_t - L_V\psi_t)$ for $t=1,2,\cdots,T$ while $\theta_0=\psi_0$. Define $\mu_1 = L_V\psi_1 + F_1G_1\psi_0$ and $\mu_t =L_V\psi_t + F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{V}\psi_{t-1})$ for $t=2,3,\cdots,T$. Then the scaled error parameterization is 
\begin{align*}
  y_t|V,W,\psi,y_{1:t-1} \sim N_p(\mu_t, F_tWF_t'), \qquad \psi_t  \stackrel{iid}{\sim} N_p(0,I_k)
\end{align*}
for $t=1,2,\cdots,T$ where $I_k$ is the $k\times k$ identity matrix. Since neither $V$ nor $W$ are in the system equation, we immediately see that the scaled errors are an AA for $(V,W)$. However, both $V$ and $W$ are in the observation equation so that $\psi$ is not an SA for $V|W$ nor for $W|V$.

\subsection{The ``wrongly-scaled'' DAs}
Two other novel augmentations can be obtained by scaling the SD and SE by the ``wrong'' variance so long as $V$ and $W$ have the same dimension. Define $\tilde{\gamma}_t=L_V^{-1}(\theta_t - G_t\theta_{t-1})$ and $\tilde{\psi}_t=L_W^{-1}(y_t - \theta_t)$ for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\tilde{\gamma}_0=\theta_0$. Then the {\it wrongly-scaled disturbances} (WSDs) are $\tilde{\gamma}\equiv\tilde{\gamma}_{0:T}\equiv(\tilde{\gamma}_0',\tilde{\gamma}_1',\cdots,\tilde{\gamma}_T')'$ and the {\it wrongly-scaled errors} (WSEs) are $\tilde{\psi}\equiv\tilde{\psi}_{0:T}\equiv(\tilde{\psi}_0',\tilde{\psi}_1',\cdots,\tilde{\psi}_T')'$. W

We can write the model in terms of $\tilde{\gamma}$ as
\begin{align*}
  y_t|\tilde{\gamma},V,W \stackrel{ind}{\sim} N_p\left(F_t\theta_t(\tilde{\gamma},L_V), V\right), \qquad  
  \tilde{\gamma}_t \stackrel{ind}{\sim}N_p(0,L_V^{-1}W(L_V^{-1})')
\end{align*}
for $t=1,2,\cdots,T$ where $\theta_t(\tilde{\gamma},L_V)$ denotes the transformation from $\tilde{\gamma}$ to $\theta$ defined by the wrongly-scaled disturbances. Since $L_V$ is the Cholesky decomposition of $V$, the observation equation does not contain $W$, so $\tilde{\gamma}$ is an SA for $W|V$. Since $W$ and $L_V$ are both in the system equation, $\tilde{\gamma}$ is not an AA for $V|W$ nor for $W|V$. 

Similarly, we can write the model in terms of $\tilde{\psi}$ as
\begin{align*}
  y_t|V,W,\tilde{\psi},y_{1:t-1} \sim N_p(\tilde{\mu}_t, F_tWF_t'), \qquad 
  \tilde{\psi}_t  \stackrel{iid}{\sim} N_p(0,L_W^{-1}V(L_W^{-1})')
\end{align*}
for $t=1,2,\cdots,T$ where we define $\tilde{\mu}_1 = L_W\tilde{\psi}_1 - F_1G_1\tilde{\psi_0}$ and for $t=2,3,\cdots,T$ $\tilde{\mu}_t =L_W\tilde{\psi}_t - F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{W}\tilde{\psi}_{t-1})$. Since $\tilde{\mu}_t$ only depends on $W$ and not on $V$, $V$ is absent from the observation equation and thus $\tilde{\psi}$ is an SA for $V|W$. Once again, since both $W$ and $V$ are in the system equation $\tilde{\psi}$ is not an AA for either $V$ or $W$.

\subsection{The elusive search for a sufficient augmentation}

Having found two ancillary augmentations for the DLM, we would like to find a sufficient augmentation in order to construct an ASIS for sampling from the posterior distribution. The following lemma suggests that this may be difficult if not impossible.

\begin{lem}\label{noSA}
Suppose $\eta$ is an SA for the DLM such that conditional on $\phi$, $\eta$ and $y$ are jointly normally distributed, that is
\begin{align*}
 \left. \begin{bmatrix}\eta \\ y \end{bmatrix}\right|\phi \sim N\left(\begin{bmatrix} \alpha_\eta \\ D\tilde{m} \end{bmatrix}, \begin{bmatrix}
   \Omega_\eta & \Omega_{y,\eta}' \\
   \Omega_{y,\eta} & \tilde{V} + \tilde{W} + \tilde{C} \end{bmatrix}\right).
\end{align*}
Let $A=\Omega_{y,\eta}'\Omega_{\eta}^{-1}$ and $\Sigma = \tilde{V} + \tilde{W} + \tilde{C} - A\Omega_{\eta}A'$. Then $A$, $\Sigma$, and $\alpha_{\eta}$ are constants with respect to $\phi$ and if $A'A$ is invertible, then
\begin{align*}
p(\phi|\eta,y) \propto & p(y|\eta,\phi)p(\eta|\phi)p(\phi) \propto p(\eta|\phi)p(\phi) \\
\propto & p(\phi)|(A'A)^{-1}A'(\tilde{V} + \tilde{W} + \tilde{C} - \Sigma)A(A'A)^{-1}|^{-1/2}\\
&\times \exp\left[-\frac{1}{2}(\eta - \alpha_{\eta})'[(A'A)^{-1}A'(\tilde{V} + \tilde{W} + \tilde{C} - \Sigma)A(A'A)^{-1}]^{-1}(\eta - \alpha_{\eta})\right].
\end{align*}
\end{lem}
\noindent Recall from Section \ref{sec:DLM:marg} that the the marginal model for the data in the DLM is $y\sim N_{Tk}(D\tilde{m}, \tilde{V} + \tilde{W} + \tilde{C})$. Then $\alpha_\eta$, $\Omega_\eta$ and $\Omega_{y,\eta}$ are additional parameters included to define the full joint distribution of $y$ and $\eta$. The posterior density we wish to sample from comes from equation \eqref{margmodel} and is similar to $p(\phi|\eta,y)$ except less complicated. So once we find an SA, in order to use it we might need to obtain draws from a density that appears just as hard to sample from as the posterior density we are already trying to approximate.

Lemma \ref{noSA} does not quite rule out the existence of a useful SA, but it does suggest that it will be difficult to find one. This result brings to mind \citet{van2001art}'s contention that there is an art to constructing data augmentation algorithms --- our goal is not only to find an MCMC algorithm that has nice convergence and mixing properties, but also one that is easy to implement, and this second criteria is much more difficult to quantify. The problem we run into is unlikely to be unique to the time series setting but rather seems driven by trying to find an SA for a pair of variances: one on the data level and the other on the latent data level.

\section{MCMC Strategies for the DLM}\label{sec:Algs}

This section discusses how to construct various MCMC algorithms for approximating the posterior distribution of the DLM. We focus on {\it what} to do, not {\it why}. Derivations of the relevant full conditional distributions are available in Appendix B. We occasionally come across a full conditional density that is difficult to sample from --- the details about why this happens and how to overcome it are in Appendices E and F.

\subsection{Base algorithms}\label{sec:Algs:base}
Using any of the DAs introduced in Section \ref{sec:DAs}, we can construct several DA algorithms which we call {\it base algorithms} to distinguish them from the alternating and interweaving algorithms we will construct later. A well known method to estimate the parameters in a DLM uses the DA algorithm using the latent states $\theta$ as the DA \citep{fruhwirth1994data,carter1994gibbs} which we refer to as the {\it state sampler}. In order to construct this algorithm, we need two pieces --- the conditional posterior of $V$ and $W$ given $\theta$ and the conditional posterior of $\theta$ given $V$ and $W$. The density $p(\theta|V,W,y)$ is multivariate normal and any algorithm that obtains a random draw from it is called a simulation smoother in the literature. One commonly used simulation smoother is the forward filtering, backward sampling algorithm (FFBS) \citep{fruhwirth1994data,carter1994gibbs} which uses the Kalman filter, but there are several alternatives including \citet{koopman1993disturbance} and \citet{de1995simulation}. The smoothers introduced in \citet{mccausland2011simulation} and \citet{rue2001fast}, dubbed ``all without a loop'' smoothers by \citet{kastner2013ancillarity} are particularly computationally efficient. Both methods exploit the tridiagonal structure of the precision matrix of the joint normal distribution for $\theta$ in order to speed up the computation of its Cholesky factor and are typically faster than methods based on the Kalman filter and other methods \citep{mccausland2011simulation}. The method of \citet{rue2001fast} is called the Cholesky factor algorithm (CFA). We use the method of \citet{mccausland2011simulation} and call it the mixed Cholesky factor algorithm (MCFA) because it is similar to the CFA except instead of computing the Cholesky factor of the precision matrix, then obtaining a random draw of the latent states, it mixes up the sub-steps required in those two steps. We omit the details of this algorithm in the context of the DLM, but they can be found in Appendix C.

In order to complete the state sampler, we need to obtain a draw from $p(V,W|\theta,y)$. It is easy to show $V$ and $W$ have independent inverse Wishart distributions conditional on $\theta$ and $y$. In particular,
\begin{align*}
  V|\theta,y &\sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right), &
  W|\theta,y &\sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right), %\label{eq:VW|theta}
\end{align*}
where $v_t = y_t - F_t\theta_t$, and $w_t = \theta_t - G_t\theta_{t-1}$. Putting the pieces together, the state sampler is the following DA algorithm:
\begin{alg*}[State]State Sampler
\label{alg:DLMstate}
\begin{enumerate}
\item Use the MCFA to sample $\theta \sim p(\theta|V,W,y)$.
\item Sample $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$ and $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$ independently.
\end{enumerate}
\end{alg*}
\noindent As we will show in Section \ref{sec:LLM}, the Markov chain constructed using the state sampler can mix poorly in some regions of the parameter space. For example, in a dynamic regression through the origin with stationary regression coefficient, if the variance of the latent states is too small relative to the variance of the data, mixing will be poor for $W$ \citep{fruhwirth2004efficient}.

Next, we can use $\gamma$ in order to construct a second DA algorithm called the {\it scaled disturbance sampler}. In the smoothing step we need to obtain a draw from $p(\gamma|V,W,y)$. This density is also Gaussian but does not have a tridiagonal precision matrix so in order to obtain a draw from it we use the MCFA to draw from $p(\theta|V,W,y)$, then use the definition of the scaled disturbances to transform from $\theta$ to $\gamma$. The density $p(V,W|\gamma,y)$ is rather complicated and does not appear easy to draw from, but it is easy to show that $V|W,\gamma,y \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$ where again $v_t = y_t - F_t\theta_t$ and $\theta_t$ is a function of $\gamma$ and $W$. However, it is not easy to draw from $p(W|\gamma,y)$ so we abandon drawing $V$ and $W$ jointly. The density $p(W|V,\gamma,y)$ is simpler and, at least in the local level model, can be sampled from with tolerable efficiency. As a result Algorithm \nameref{alg:DLMdist}, the scaled disturbance sampler, has three steps instead of the usual two. 
\begin{alg*}[SD]Scaled Disturbance Sampler\label{alg:DLMdist}
\begin{enumerate}
\item Use the MCFA to sample $\theta \sim p(\theta|V,W,y)$.
\item Draw $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$.
\item Transform $\theta$ to $\gamma$.
\item Draw $W \sim p(W|V,\gamma,y)$.
\end{enumerate}
\end{alg*}\noindent
We draw $V$ before $W$, but there is no intrinsic reason for this order. We first draw $\theta$ and draw $V$ before transforming to $\gamma$. We can do this because the draw of $V$ is the same whether we condition on $\gamma$ or $\theta$ and it reduces the computational cost of the algorithm. The last step is the difficult one but we demonstrate how to accomplish it when $W$ is a scalar in the local level model in Appendix E.

The DA algorithm based on the scaled errors is called the {\it scaled error sampler} (Algorithm \nameref{alg:DLMerror}) and is similar to the scaled disturbance sampler with a couple of key differences. First, the simulation smoothing step in the scaled error sampler can be accomplished directly with the MCFA because the precision matrix of the conditional posterior of $\psi$ retains the necessary tridiagonal structure. Second, the full conditional distribution of $W$ is the familiar inverse Wishart density and the full conditional of $V$ is the complicated density. The density of $V|W,\psi,y$ is in the same class as that of $W|V,\gamma,y$. In fact there is a strong symmetry here --- the joint conditional posterior of $(V,W)$ given $\gamma$ is from the same family of densities as that of $(W,V)$ given $\psi$ so that $V$ and $W$ essentially switch places when we condition on the scaled errors instead of the scaled disturbances.
\begin{alg*}[SE]Scaled Error Sampler\label{alg:DLMerror}
\begin{enumerate}
\item Use the MCFA to sample $\psi \sim  p(\psi|V,W,y)$
\item Draw $V \sim p(V|W,\psi,y)$.
\item Draw $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
\end{enumerate}
\end{alg*}

The wrongly-scaled DA algorithms are close analogues to their correctly scaled cousins. Starting with the {\it wrongly-scaled disturbance sampler} (Algorithm \nameref{alg:DLMwdist}), the simulation smoothing step to draw from $p(\tilde{\gamma}|V,W,y)$ is similar to that of the scaled disturbance sampler --- the density is Gaussian, but the precision matrix is not tridiagonal, so we draw $\theta$ using the MCFA and transform to obtain a draw of $\tilde{\gamma}$. The density of $V,W|\tilde{\gamma},y$ is too complicated to draw from directly, as was the case when we used the scaled disturbances. In this case, the full conditional distribution of $W$ is the same as its distribution when we condition on the states while the density of $V|\tilde{\gamma},y$ is once again difficult to draw from. The density of $V|W,\tilde{\gamma},y$ is easier to work with, at least in the local level model example in Section \ref{sec:LLM}. 
\begin{alg*}[WSD]Wrongly-Scaled Disturbance Sampler\label{alg:DLMwdist}
\begin{enumerate}
\item Use MCFA to draw $\theta \sim p(\theta|V,W,y)$.
\item Transform $\theta$ to $\tilde{\gamma}$.
\item Draw $V \sim p(V|W,\tilde{\gamma},y)$.
\item Draw $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
\end{enumerate}
\end{alg*}\noindent
Now the third step is difficult and we demonstrate how to accomplish it in the local level model in Appendix E. We could switch the order in which $V$ and $W$ are drawn in this algorithm so that we can draw $W$ before transforming $\theta$ to $\tilde{\gamma}$. This would make each iteration slightly cheaper and probably would not affect the mixing and convergence properties of the algorithm, however we are more interested in comparing the mixing and convergence properties of the various samplers, so we always sample $V$ before $W$ when we cannot sample them jointly.

The {\it wrongly-scaled error sampler} (Algorithm \nameref{alg:DLMwerror}) is closely related to both the wrongly-scaled disturbance sampler and the scaled error sampler. The density of $\tilde{\psi}|V,W,y$ is Gaussian with a tridiagonal precision matrix, so the simulation smoothing step can be accomplished using the MCFA. The density $p(V,W|\tilde{\psi},y)$ is from the same class as $p(W,V|\tilde{\gamma},y)$ so that $V$ and $W$ essentially switch places when we condition on $\tilde{\psi}$ instead of $\tilde{\gamma}$. In particular, $V|W,\tilde{\psi},y$ has an inverse Wishart density and the density of $W|V,\tilde{\psi},y$ is from the same class as that of $V|W,\tilde{\gamma},y$.
\begin{alg*}[WSE]Wrongly-Scaled Error Sampler\label{alg:DLMwerror}
\begin{enumerate}
\item Use MCFA to draw $\tilde{\psi} \sim p(\theta|V,W,y)$.
\item Draw $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$.
\item Draw $W \sim p(W|V,\tilde{\psi},y)$
\end{enumerate}
\end{alg*}

\subsection{Alternating algorithms}\label{sec:Algs:alt}
Using the full conditional distributions defined in Section \ref{sec:Algs:base}, we can construct several alternating algorithms based on any two of the DA algorithms. The algorithms have the form of Algorithm \nameref{alg:Alt} on page \pageref{alg:Alt}. For example, the {\it State-SD alternating sampler} which alternates between the states and the scaled disturbances, obtains the $k+1$'st iteration of $(V,W)$ from the $k$'th as follows:
\begin{align*}
&[\theta|V^{(k)},W^{(k)}] \to [V^{(k+0.5)},W^{(k+0.5)}|\theta] \to\\
&[\gamma|V^{(k+0.5)},W^{(k+0.5)}] \to [V^{(k+1)}|W^{(k+0.5)},\gamma] \to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
The first line is an iteration of the state sampler while the second line is an iteration of the scaled disturbance sampler. No work is necessary to link up the two iterations --- we simply plug in the values of $V$ and $W$ obtained from the state sampler iteration into the draw of $\gamma$ from step one of the scaled disturbance sampler iteration. Each other alternating algorithm is analogous and can be constructed without complication. The order in which the base algorithms are used within an alternating algorithm could in principle affect the convergence properties of the algorithm. In practice we find this typically is not important. 

The naming convention we use for these algorithms is to list each DA in the order in which they appear in the alternating sampler, separated by hyphens. We shorten the scaled disturbances to ``SD'', the scaled errors to ``SE'', and the wrongly-scaled version of each to ``WSD'' and ``WSE'' respectively. So for example, the alternating sampler which alternates between the scaled disturbances and the wrongly-scaled disturbances, in that order, we call the {\it SD-WSD alternating sampler} or {\it SD-WSD Alt}. The main purpose of these algorithms is to use as a baseline for evaluating the GIS algorithms based on the same sets of DAs.

\subsection{GIS Algorithms}\label{sec:Algs:GIS}
In addition to the alternating algorithms of the previous section, we can use the various DAs of Section \ref{sec:DAs} in order to construct interweaving algorithms. We will start with Algorithm \nameref{alg:eGIS} on page \pageref{alg:eGIS}. Given the full conditional distributions listed in Section \ref{sec:Algs:base}, the only additional ingredients we need are the definitions of the various available DAs in order to perform the one-to-one transformations from any one DA to another. For example, in the {\it State-SD GIS sampler} we obtain $(V^{(k+1)},W^{(k+1)})$ from $(V^{(k)},W^{(k)})$ as follows:
\begin{align*}
&[\theta|V^{(k)},W^{(k)}] \to [W^{(k+0.5)},V^{(k+0.5)}|\theta] \to\\
&[\gamma|V^{(k+0.5)},W^{(k+0.5)},\theta] \to [V^{(k+1)}|W^{(k+0.5)},\gamma] \to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
In the first step of the second line we transform $\theta$ to $\gamma$ by means of the defining equations for $\gamma$: $\gamma_0=\theta_0$ and $\gamma_t = L_W^{-1}(\theta_t - G_t\theta_{t-1})$ for $t=1,2,\cdots,T$ where $L_W$ is the Cholesky decomposition of $W$.

There are often some small improvements that can be made simply by thinking clearly about what the GIS algorithm is doing. For example in the above version of the State-SD GIS sampler, the draw of $V$ in step two of line one and the draw of $V$ in step two of line two are redundant --- they come from the same distribution and only the last one is ever used in later steps. As a result, we can remove the second draw so that the State-SD GIS sampler is as follows:
\begin{alg*}[State-SD GIS]State-Scaled Disturbance GIS Sampler
\label{alg:DLMstateerror}
\begin{enumerate}
\item Use the MCFA to sample $\theta \sim p(\theta|V,W,y)$.
\item Sample $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$ and $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$ independently.
\item Transform $\theta$ to $\gamma$.
\item Sample $W \sim p(W|V,\gamma,y)$.
\end{enumerate}
\end{alg*}\noindent

The naming convention for GIS algorithms is similar to that of alternating algorithms --- DAs appear in the name in the order that they appear in the algorithm, separated by hyphens, e.g. a GIS algorithm based on the states, scaled disturbances, and scaled errors in that order would be called the {\it State-SD-SE GIS sampler}. There is no additional difficulty encountered by using a GIS with greater than two DAs. Also like alternating algorithms, the performance of GIS algorithms may depend on the order in which the DAs are used, but in our experience this tends to make no different, which is consistent with what \citet{yu2011center} report.

\subsection{CIS algorithms}\label{sec:Algs:CIS}
Next we consider CIS algorithms which have the form of Algorithm \nameref{alg:CIS} on page \pageref{alg:CIS}. The advantage of using CIS is that it is sometimes possible to find an AA-SA pair of DAs for each part of the parameter vector even when no such pair of DAs exist for the entire vector. From Section \ref{sec:DAs}, we know that the scaled disturbances and the wrongly-scaled disturbances form an AA-SA pair for $W|V$ while the scaled errors and the wrongly-scaled errors form an AA-SA pair for $V|W$.  A CIS sampler based on these AA-SA pairs obtains $(V^{(k+1)},W^{(k+1)})$ from $(V^{(k)},W^{(k)})$ as follows:
\begin{align*}
&[\psi|V^{(k)},W^{(k)}] \to [V^{(k+0.5)}|W^{(k)},\psi] \to [\tilde{\psi}|V^{(k+0.5)},W^{(k)},\psi] \to [V^{(k+1)}|W^{(k)},\tilde{\psi}]\to\\
&[\tilde{\gamma}|V^{(k+1)},W^{(k)},\tilde{\psi}] \to [W^{(k+0.5)}|V^{(k)},\tilde{\gamma}] \to [\gamma|V^{(k+1)},W^{(k+0.5)},\tilde{\gamma}]\to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
The first line is essentially a Gibbs step for drawing $V$ that interweaves between $\psi$ and $\tilde{\psi}$ while the second line is essentially a Gibbs step for drawing $W$ that interweaves between $\gamma$ and $\tilde{\gamma}$. In the second line we use the SA before the AA in order to minimize the number of transformations we have to make in every iteration. 

Notice that each time one of the wrongly-scaled DAs appears in the CIS sampler, it would make no difference if the states were used instead because $p(V|W,\tilde{\psi},y)=p(V|W,\theta,y)$ and $p(W|V,\tilde{\gamma},y)=p(W|V,\theta,y)$ --- in both cases the density is inverse Wishart. This is despite the fact that we know from Section \ref{sec:DAs} the states are only an SA for $W|V$ but not for $V|W$. Using this fact we obtain a slightly different version of the CIS sampler in Algorithm \nameref{alg:DLMcis}.
\begin{alg*}[CIS]Componentwise Interweaving Sampler\label{alg:DLMcis}
\begin{enumerate}
\item Use the MCFA to sample $\psi \sim p(\psi|V,W,y)$.
\item Sample $V \sim p(V|W,\psi,y)$.
\item Transform $\psi$ to $\theta$.
\item Sample $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$.
\item Sample $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
\item Transform $\theta$ to $\gamma$.
\item Sample $W \sim p(W|V,\gamma,y)$.
\end{enumerate}
\end{alg*}

We can rearrange the order in which the Gibbs steps for $V$ and $W$ are taken within the CIS algorithm, change the order in which the AAs and SAs are used with the Gibbs steps to obtain another version of the CIS algorithm. Then we can rearrange some of the steps of this version of the CIS algorithm without changing the algorithm to obtain:
\begin{align*}
&[\gamma|V^{(k)},W^{(k)}] \to [V^{(k+0.5)}|W^{(k)},\gamma] \to[W^{(k+0.5)}|V^{(k+0.5)},\gamma] \to \\
&[\psi|V^{(k+0.5)},W^{(k+0.5)}]\to [V^{(k+1)}|W^{(k+0.5)},\psi]\to [W^{(k+1)}|V^{(k+1)},\psi].
\end{align*}
This is a slightly rearranged version of the SD-SE GIS sampler. Since in most cases simply rearranging the steps of an MCMC sampler does little to impact the convergence and mixing properties of the sampler, we expect the CIS and SD-SE GIS samplers to perform about the same along this axis so that computational cost per iteration and ease of implementation are the only real considerations involved in choosing between the two.

In our original definition of the CIS sampler for the DLM we used the scaled disturbances as the AA for $W$ and the scaled errors and the AA for $V$. We could have reversed this or used the same AA for both $V$ and $W$ since both the scaled errors and scaled disturbances are AAs for $(V,W)$, or we can have used $\theta$ as the AA for $V$. In each of these cases, the resulting algorithm would reduce to either the state sampler or a {\it partial CIS} algorithm, also introduced by \citet{yu2011center}. A partial CIS algorithm is similar to a CIS algorithm except it uses an ordinary DA step instead of an interweaving step for one of the parameter blocks. Given the available DAs, we have one CIS algorithm to go along with 5 base samplers and a large number of GIS and alternating samplers. We now desire to characterize the efficiency of these samplers, both in terms of computational cost and in terms of the mixing and convergence of the Markov chain. In the next section we will do this using class of DLMs known as the local level model.

\section{Application: The Local Level Model}\label{sec:LLM}

\subsection{The local level model and its DAs}

The local level model (LLM) is a DLM with univariate data $y_t$ for $t=1,2,\cdots,T$ and a univariate latent state $\theta_t$ for $t=0,1,\cdots,T$. In the general DLM notation, $F_t=1=G_t=1$ for all $t$ while $V$ and $W$ are scalar. We can write the model as
\begin{align*}
  y_t |\theta,V,W& \stackrel{ind}{\sim} N(\theta_t,V), &
  \theta_t |\theta_{0:t-1},V,W& \sim N(\theta_{t-1},W)
\end{align*}
for $t=1,2,\cdots,T$. The priors on $(\theta_0,V,W)$ from Section \ref{sec:DLM} become $\theta_0\sim N(m_0,C_0)$, $V\sim IG(\alpha_V,\beta_V)$ and $W\sim IG(\alpha_W,\beta_W)$ with $\theta_0$, $V$ and $W$ mutually independent and $IG(\alpha,\beta)$ is the inverse gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$.

We can define the various DAs from Section \ref{sec:DAs} in the context of the local level model. The usual DA, the states, is simply $\theta$. From the states we obtain the scaled disturbances as $\gamma_0=\theta_0$ and $\gamma_t = (\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$. Similarly, the scaled errors are $\psi_0=\theta_0$ and $\psi_t = (y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$. The wrongly-scaled disturbances are then $\tilde{\gamma}_{0}=\theta_0$ and $\tilde{\gamma}_t = (\theta_t - \theta_{t-1})/\sqrt{V}$ while the wrongly-scaled errors are $\tilde{\psi}_0=\theta_0$ with $\tilde{\psi}_t = (y_t - \theta_t)/\sqrt{W}$ for $t=1,2,\cdots,T$. 

\subsection{Full conditionals}\label{sec:LLM:fullcond}

Here we list the full conditional distributions required to construct each of the MCMC samplers in Section \ref{sec:Algs} for the LLM. Derivations of most of these conditionals can be found in Appendix B. For all algorithms, we use the MCFA to draw the DA or to draw $\theta$ and then transform to the DA. For $V$, $p(V|W,\theta,y)=p(V|W,\gamma,y)=p(V|W,\tilde{\psi},y)$ so that in each case $V$ has an $IG(a_V,b_V)$ distribution where $a_V = \alpha_V + T/2$ and $b_V = \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2$. Similarly for $W$, $p(W|V,\theta,y)=p(W|V,\psi,y)=p(W|V,\tilde{\gamma},y)$ so that in each case $W$ has an $IG(a_W, b_W)$ distribution where $a_W = \alpha_W + T/2$ and $b_W = \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2$.

In order to construct the algorithms based on $\gamma$, we also need $p(W|V,\gamma,y)$. Similarly to construct the algorithms based on $\psi$, we need $p(V|W,\psi,y)$. In Appendix B, we show that both of these densities have the form $p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b\sqrt{x} -c/x\right]$. For $p(W|V,\gamma,y)$, $\alpha=\alpha_W$, $a=a_\gamma \equiv \sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$, $b=b_\gamma \equiv\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$, and $c=\beta_W$. Similarly, for $p(V|W,\psi,y)$, $\alpha=\alpha_V$, $a=a_{\psi}\equiv\sum_{t=1}^T(\mathcal{L}\psi_t)^2/2W$, $b=b_{\psi}\equiv\sum_{t=1}^T(\mathcal{L}\psi_t\mathcal{L}y_t)/W$, and $c=\beta_V$ where we define $\mathcal{L}y_t=y_t-y_{t-1}$ for $t=2,3,\cdots,T$, $\mathcal{L}y_1=y_1 - \psi_0$, $\mathcal{L}\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$ and $\mathcal{L}\psi_1=\psi_1-0$. In Appendix C, we show that this density is often log concave in which case adaptive rejection sampling \citep{gilks1992adaptive} works well. When the density is not log concave, we use a $t$ approximation in a rejection sampler for $\log(x)$.

The constructions of Algorithms \nameref{alg:DLMwdist} and \nameref{alg:DLMwerror} also require $p(W|V,\tilde{\psi},y)$ and $p(V|W,\tilde{\gamma},y)$ respectively. Both densities have the form $p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b/\sqrt{x} -c/x\right]$, which is closely related to the difficult density from the correctly scaled samplers above.  For $p(V|W,\tilde{\gamma},y)$ we show in Appendix B that $\alpha=\alpha_V$, $a = a_{\tilde{\gamma}}\equiv\frac{1}{2W}\sum_{t=1}^T\tilde{\gamma}_t^2$, $b =b_{\tilde{\gamma}}\equiv \sum_{t=1}^T(y_t - \tilde{\gamma}_0)\sum_{s=1}^t\tilde{\gamma}_s$, and $c =c_{\tilde{\gamma}}\equiv \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\gamma}_0)^2$ while for $p(W|V,\tilde{\psi},y)$ we show that $\alpha=\alpha_W$,   $a =a_{\tilde{\psi}}\equiv \frac{1}{2V}\sum_{t=1}^T\tilde{\psi}_t^2$,  $b =b_{\tilde{\psi}}\equiv \sum_{t=1}^T\mathcal{L}\tilde{y}_t\mathcal{L}\tilde{\psi}_t$, and $c =c_{\tilde{\psi}}\equiv \beta_W + \frac{1}{2}\sum_{t=1}^T\mathcal{L}\tilde{y}_t^2$. This density is harder to sample from because adaptive rejection sampling does not work very well, so we construct a rejection sampler on the log scale using a $t$ approximation in Appendix F.

\subsection{Simulation Setup}

In order to test these algorithms, we simulated data from the local level model for various choices of $V$, $W$, and $T$. We created a full factorial design with $V$ and $W$ each taking the values $10^{i/2}$ where $i=-4,-3,\cdots,4$ and with $T$ taking the values $10, 100, 1000$. Then for each dataset, we fit the local level model using a variety of the algorithms discussed in this paper. We used the same rule for constructing priors for each model: $\theta_0\sim N(0,10^7)$, $V\sim IG(5, 4V^*)$, and $W\sim IG(5, 4W^*)$, mutually independent where $(V^*,W^*)$ are the true values of $V$ and $W$ used to simulate the time series. Note that the prior means are equal to the true values of $V$ and $W$, so both the prior and likelihood and thus the posterior roughly agree about the likely values of $V$ and $W$. Ultimately, the reason for varying the prior with the true parameter values is that the results are more interesting --- intuitively, it turns out the behavior of each of these samplers depends on where in the parameter space the posterior distribution puts most of its mass.

For each dataset and each sampler we obtained $n=3000$ draws and threw away the first $500$. The chains were started at the true values used to simulate the time series, so we can examine the behavior of the chains to determine how well they mix but not how quickly they converge. Define the effective sample proportion ($ESP=ESS/n$) for a scalar component of the chain as the effective sample size ($ESS$) \citep{gelman2013bayesian} of the component divided by the actual sample size $n$. When $ESP=1$ the Markov chain is behaving as if it obtains iid draws from the posterior. It is possible that $ESP>1$ if the draws are negatively correlated which occasionally happens in our simulations. When it does happen the correlation is weak and we round down to one for plotting purposes.

\subsection{Simulation results}

Figure \ref{fig:ESPa} contains plots of ESP for $V$ and $W$ in each chain of each base samplers for $T=100$ --- we omit the $T=10$ and $T=1000$ plots for brevity, but they can be found in Appendix H. Table \ref{tab:stnmix} summarizes the results for the base samplers on the top. Let $R^*=V^*/W^*$ denote the true signal to noise ratio. The State sampler tends to have a low ESP for $V$ and high ESP for $W$ when $R^*>1$ with the behavior switched when $R^*<1$. The WSD and WSE samplers are similar to the state sampler except WSD has lower ESP for $V$ and WSE has lower ESP for $W$. The SD sampler has low ESP for both $V$ and $W$ when $R^*>1$ while the SE sampler has low ESP for both when $R^*<1$ and in particular for $V$. We omit the results here, but as $T$ increases, in all samplers the region of the parameter space with high ESP shrinks and in the low ESP regions, ESP drops closer to zero. In Appendix G, we discuss how the pattern of correlations between various quantities in the posterior distribution roughly determines the pattern of ESPs we see in Figure \ref{ESplot}. 


\begin{figure}[!h]
\centering
\begin{subfigure}[b]{0.59\textwidth}%
\includegraphics[width=\textwidth]{basecisESplot100}
\caption{}
\label{fig:ESPa}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altintESplotV100}
\caption{}
\label{fig:ESPb}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altintESplotW100}
\caption{}
\label{fig:ESPc}
\end{subfigure}
\caption{Effective sample proportion in the posterior sampler for a time series of length $T=100$, for $V$ and $W$ in the each sampler. Figure \ref{fig:ESPa} contains ESP for $V$ and $W$ for the base samplers, Figure \ref{fig:ESPb} contains ESP in the GIS and CIS samplers, and Figure \ref{fig:ESPc} contains ESP in the Alt samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high.}
\label{ESplot}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{lccccc}\hline
     & State           & SD                 & SE                  & WSD                 & WSE \\\hline
   V & $R^* < 1$       & $R^* < 1$           & $R^* > 1$           & $R^* < 1$           & $R^* < 1$ \\ 
   W & $R^* > 1$       & $R^* < 1$           & $R^* > 1$           & $R^* > 1$           & $R^* > 1$ \\ \hline
     & State-SD        & State-SE            & SD-SE              & Triple              & CIS \\\hline
   V & $R^* < 1$       & $R^* \not\approx 1$ & $R^* \not\approx 1$ & $R^* \not\approx 1$ & $R^* \not\approx 1$ \\
  W  &$R^* \not\approx 1$& $R^* > 1$          & $R^* \not\approx 1$ & $R^* \not\approx 1$ & $R^* \not\approx 1$\\\hline
  \end{tabular}
  \caption{Rule of thumb for when each sampler has a high ESP for each variable as a function of the true signal-to-noise ratio, $R^*=W^*/V^*$. Note that as the length of the time series increases, the farther away from one $R^*$ has to be for the sampler to have a high ESP.}
  \label{tab:stnmix}
\end{table}

We fit the LLM to the simulated datasets using several GIS samplers and a CIS sampler as well. Since the wrongly-scaled samplers behaved similarly to the state sampler and neither of the underlying DAs were a SA for $V$ and $W$ jointly, we ignored them in the construction of the GIS samplers. Instead, we constructed the State-SD, State-SE, SD-SE, and Triple (State-SD-SE) GIS samplers, as well as the CIS sampler. Figure \ref{fig:ESPb} has plots of ESP for each of the GIS and CIS algorithms while Figure \ref{fig:ESPc} has plots of ESP for each of the Alt algorithms. Table \ref{tab:stnmix} summarizes the results on the bottom.

Essentially, each GIS and Alt algorithm has high ESP when at least one of the base algorithms has high ESP. For example, the State-SD GIS and Alt algorithms have high ESP for $W$ except for a narrow band where $R^*$ is near one while ESP is high for $W$ in the state sampler when $R^*>1$ and in the SD sampler when $R^*<1$. Similarly in the State-SD GIS and Alt algorithms, mixing for $V$ is identical to the State and SD samplers since neither sampler improves on the other in any region of the parameter space. Both the State-SD GIS and Alt algorithms take advantage of the fact that the state and SD DA algorithms make up a ``beauty and the beast'' pair for $W$ and thus improves mixing in the marginal chain for $W$. However, GIS does not appear to improve on Alt --- unlike in ASIS, the posterior dependence between the two DAs must be too large.

We mentioned in Section \ref{sec:Algs:CIS} that the CIS and the SD-SE GIS algorithms consist of the same steps, just rearranged. This suggests that they should perform similarly and in fact the SD-SE GIS algorithm behaves essentially identically to the CIS and Triple GIS algorithms. That the SD-SE and Triple GIS algorithms behave identically should not be surprising --- the State sampler behaves like the SD sampler for $V$ and the SE sampler for $W$.  We include simulations with differing sizes of $T$ using these samplers in Appendix H, but the upshot is that, like with the base samplers, increasing the length of the time series worsens ESP for both $V$ and $W$ in all samplers and in particular shrinks the area of the parameter space in which ESP is high.

\subsection{Computational time}\label{sec:LLM:time}

From a practical standpoint a more important question than how well the chain mixes is the full computational time required to adequately characterize the target posterior distribution. In order to investigate this, we compute the natural log of the average time in minutes required for each sampler to achieve an effective sample size of 1000 --- in other words the log minutes per 1000 effective draws. All simulations were performed on a server with Intel Xeon X5675 3.07 GHz processors. While different systems will yield different absolute times, the relative times should be similar. Figure \ref{baseinttimeplot} contains plots of the log minutes per 1000 effective draws for both $V$ and $W$ and for each of the samplers.

\begin{figure}[!h]
\centering
\begin{subfigure}[b]{0.59\textwidth}
\includegraphics[width=\textwidth]{basecistimeplot100}
\caption{}
\label{fig:timea}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altgisVtimeplot100}
\caption{}
\label{fig:timeb}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altgisWtimeplot100}
\caption{}
\label{fig:timec}
\end{subfigure}
\caption{Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=100$ in each sampler. Figure \ref{fig:timea} contains the base samplers, Figure \ref{fig:timeb} contains the GIS and CIS samplers, while Figure \ref{fig:timec} contains the Alt samplers. Log times larger than three $\log \mathrm{min}$ are rounded down to three for plotting purposes.}
\label{baseinttimeplot}
\end{figure}

For $T=100$ the pattern we saw for ESP also appears for log minutes per 1000 effective draws. The State sampler becomes slow to reach 1000 effective draws for $V$ when $R^*>1$ and for $W$ when $R^*<1$. The SD and SE samplers behave as expected --- the SD sampler is slow for both $V$ and $W$ when $R^*>1$ while the SD sampler is slow for both $V$ and $W$ when $R^*<1$. The SD-SE GIS, Triple GIS and CIS algorithms appear to be the big winners here and are almost indistinguishable. All three algorithms are slightly slower for both $V$ and $W$ when $R^*$ is near one, though for larger $T$,  when $R^*$ is near or below one all three are slow for $W$ (plots available in Appendix H). Compared to the state sampler, all three offer large gains over most of the parameter space. There appears to be no difference between a GIS algorithm and the corresponding alternating algorithm in terms of log time per 1000 effective draws, so the SD-SE Alt and Triple Alt algorithms are both just as efficient as the best interweaving algorithms. In more complicated models, this may not be the case though --- the GIS version of an algorithm is computationally cheaper than the Alt version since it consists of three of the four same steps, and in the fourth step the Alt algorithm has to obtain a random draw while the GIS algorithm typically only has to make a transformation.

\section{Discussion}\label{sec:Discuss}

In order to explore reparameterizing the DA and apply the interweaving strategies of \citet{yu2011center} in dynamic linear models, we start with two DAs --- the usual DA, called the latent states, and the scaled disturbances, often called the non-centered disturbances, obtained by centering and scaling the latent states to create a pivotal quantity. In addition, we introduce three new DAs for the DLM: the scaled errors, the wrongly-scaled disturbances, and the wrongly-scaled errors. Using these DAs, we construct alternating algorithms and GIS algorithms, obtained by interweaving between two separate DAs in a manner similar to an alternating algorithm, and a CIS algorithm, obtained by applying GIS in a Gibbs-like structure to sub-blocks of the parameter vector. We also seek to apply ASIS, which is a GIS where the two DAs are required to be a sufficient augmentation and an ancillary augmentation. However, we find under some assumptions that any sufficient augmentation for a general class of DLMs yields a full conditional distribution for the model parameters that is as difficult to sample from as the target posterior distribution of the model parameters. With available DAs, we construct each possible DA algorithm, several GIS algorithms and their corresponding Alt algorithms, and a CIS algorithm for the general DLM. Using the local level model as an example, we explore the mixing properties of many of the available DA, GIS, Alt, and CIS algorithms by fitting the model to simulated datasets. We find that the true signal-to-noise ratio, $R^*=V^*/W^*$, is important for determining when each algorithm performs well, and in addition that there appears to be no substantive difference between a GIS algorithm an its corresponding Alt algorithm. In fact, the three best performing algorithms under most circumstances are the SD-SE GIS algorithm, the SD-SE Alt algorithm and the CIS algorithm. In some regions of the parameter space all three algorithms are substantially faster than the commonly used State sampler, though in other regions the difference is small.

The importance of the true signal-to-noise ratio in DLMs to the mixing and convergence properties of various MCMC algorithms has been anticipated in the literature. In the AR(1) plus noise model, \citet{pitt1999analytic} find that the signal-to-noise ratio along with the AR(1) coefficient determine the convergence rate of a Gibbs sampler. In addition, they find that the convergence rate decreases as the length of the time series increases, which is consistent with our empirical findings in the local level model. When \citet{fruhwirth2004efficient} study the dynamic regression model with a stationary AR(1) process on the regression coefficient, they use both the states and the scaled disturbances (non-centered disturbances) and several other DAs motivated by some results for Gibbs samplers in the hierarchical model literature. When they examine the behavior of the resulting DA algorithms, \citet{fruhwirth2004efficient} find that the relative behavior of the SD sampler and the State sampler depends on a function of the true signal-to-noise ratio that also depends on the true value of the autocorrelation parameter and the distribution of the covariate. In addition none of the other DA algorithms they consider are more efficient than both the state sampler and the SD sampler at the same time. Given this previous work, above it is likely that in the general DLM the signal-to-noise ratio will in some way determine how well each algorithm performs even if we do not know the precise manner in which it affects mixing and convergence behavior. This is likely a consequence of the relevance of the Bayesian fraction of missing information and the related EM fraction of missing information to the performance of the DA and EM algorithms (see \citet{van2001art} for a good explication of both concepts).

A major computational bottleneck in most of our algorithms occurs when we have to draw from $p(W|V,\gamma,y)$, $p(V|W,\psi,y)$, $p(V|W,\tilde{\gamma},y)$ or $p(W|V,\tilde{\psi},y)$. The densities $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$ have the form
\[
p(x)\propto x^{-\alpha-1}\exp\left[-ax + b\sqrt{x} - c/x\right],
\]
while the densities $p(W|V,\tilde{\psi},y)$ and $p(V|W,\tilde{\gamma},y)$ have the form
\[
p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b/\sqrt{x} -c/x\right]
\]
where $\alpha,a,c>0$ and $b\in\Re$. When $b=0$ we have a special case of the generalized inverse Gaussian (GIG) distribution, so perhaps the methods used to speed up draws from the GIG can be used here \citep{jorgensen1982statistical,dagpunar1989easily,devroye2012random}. On the other hand, it might be worth putting effort into drawing $V$ and $W$ jointly conditional the DA. Using the scaled disturbances, the conditional distribution of $V$ given $W$ is inverse gamma in the LLM and inverse Wishart in the general DLM, so it is easy to derive the marginal density $p(W|\gamma,y)$ up to a proportionality constant. In our LLM example, this density turns out to be very difficult to sample from and in particular, it is not easy to come up with a generally good approximation for rejection sampling or for a Metropolis step. This is because the augmented data likelihood and the prior for the variance we used to scale the states combine to create a complicated full conditional for that variance. We chose inverse Wishart priors for $V$ and $W$ partially because they are standard and partially because their conditional conjugacy with the states is computationally convenient, but outside of the state sampler there may be a more convenient prior. In addition, there are well known inferential problems with the inverse Wishart prior in the hierarchical model literature, e.g. \citet{gelman2006prior}\if0\blind and \citet{alvarez2014cov}\fi, though it is unclear whether this transfers over to DLMs or more generally any time series model. An alternative is to use the conditionally conjugate prior conditional on the scaled disturbances, or whichever DA we prefer. In the LLM, the conditionally conjugate prior for $\sqrt{W}$ using the scaled disturbances as the DA is a Gaussian distribution --- strictly speaking this prior is on $\pm \sqrt{W}$. If we use this prior for $\pm\sqrt{V}$ as well, the $V$ step in the scaled disturbance sampler becomes a draw from the generalized inverse Gaussian distribution. This prior has been used by \citet{fruhwirth2011bayesian} and \citet{fruhwirth2008bayesian} to speed up computation while using the scaled disturbances in hierarchical models and by \citet{fruhwirth2010stochastic} for time series models with a DA similar to the scaled disturbances. We omit the results here, but using this prior on both variances does not alter our mixing results for any of the MCMC samplers. There is a trade-off in computation time to consider --- for example when using the scaled disturbances, the draw of $W|V,\gamma,y$ is sped up by using the Gaussian prior on $\pm\sqrt{W}$ since it becomes a Gaussian draw while the $V|W,\gamma,y$ is slower since it becomes a generalized inverse Gaussian draw instead of an inverse gamma. The gains outweigh the costs, at least in the local level model. In the general DLM, however, it is unclear whether this will hold because of the additional complications stemming from $V$ and $W$ being matrices. 



\appendix
%\renewcommand\thefigure{\thesection.\arabic{figure}}    
\section{Proof of lemma 1}\label{sec:Proof}

First the normality assumption implies
\begin{align*}
  y|\eta,\phi &\sim N(D\tilde{m} + \Omega_{y,\eta}'\Omega_\eta^{-1}(\eta - \alpha_\eta), \tilde{V} + \tilde{W} + \tilde{C}- \Omega_{y,\eta}'\Omega_{\eta}^{-1}\Omega_{y,\eta})\\
  \eta|\phi &\sim N(\alpha_\eta, \Omega_\eta).
\end{align*}
Now for $\eta$ to be a sufficient augmentation we need $D\tilde{m} + \Omega_{y,\eta}'\Omega_\eta^{-1}(\eta - \alpha_\eta)$ and $\tilde{V} + \tilde{W} + \tilde{C} - \Omega_{y,\eta}'\Omega_{\eta}^{-1}\Omega_{y,\eta}$ to be functionally independent of $\phi$. This requires that
\begin{align*}
  D\tilde{m} - \Omega_{y,\eta}'\Omega_\eta^{-1}\alpha_\eta + \Omega_{y,\eta}'\Omega_\eta^{-1}\eta  = b + A\eta
\end{align*}
where $A=\Omega_{y,\eta}'\Omega_\eta^{-1}$ and $b=D\tilde{m} - A\alpha_\eta$ must both be free of $\phi$. As a result $A\alpha_\eta$ is also free of $\phi$ and thus so is $\alpha_{\eta}$.

Then using the second equation, we now require $\Sigma$ free of $\phi$ where $\Sigma = \tilde{V} + \tilde{W} + \tilde{C} - A\Omega_{\eta}A'$. This ensures that $\Omega_{\eta,y}$ is not the zero matrix since $\tilde{V} + \tilde{W} + \tilde{C}$ is not free of $\phi$. Rearranging we have $A\Omega_{\eta}A' = \tilde{V} + \tilde{W} + \tilde{C} - \Sigma$. Consider $\tilde{\eta}=A\eta$, which is also a sufficient augmentation since it is just a linear transformation by a constant matrix. Then we have
\begin{align*}
y|\tilde{\eta},\phi & \sim N(b + A\eta, \Sigma)\\
\tilde{\eta}|\phi & \sim N(A\alpha_\eta, A\Omega_\eta A')
\intertext{in other words}
y|\tilde{\eta},\phi & \sim N(b + \tilde{\eta}, \Sigma)\\
\tilde{\eta}|\phi & \sim N(A\alpha_{\eta}, \tilde{V} + \tilde{W} + \tilde{C} - \Sigma).
\end{align*}
Thus the posterior density of $\phi$ given $\tilde{\eta}$ can be written as
\begin{align*}
  p(\phi|\tilde{\eta}, y) &\propto p(y|\tilde{\eta},\phi)p(\tilde{\eta}|\phi)p(\phi) \propto p(\tilde{\eta}|\phi)p(\phi) \\
&\propto p(\phi)|\tilde{V} + \tilde{W} + \tilde{C} - \Sigma|^{-1/2}\exp\left[-\frac{1}{2}(\tilde{\eta} - A\alpha_{\eta})'(\tilde{V} + \tilde{W} + \tilde{C} - \Sigma)^{-1}(\tilde{\eta} - A\alpha_{\eta})\right].
\end{align*}
Now given that $A'A$ is invertible and the properties of multivariate normal distributions, the density of $p(\phi|\eta,y)$ follows from $\eta=(A'A)^{-1}A'\tilde{\eta}$.

\section{Full conditional distributions in the general DLM for various DAs}\label{sec:DLMfullcond}

The class of DLMs we consider is defined as follows:
\begin{align}
y_t &= F_t\theta_t + v_t && v_t \stackrel{ind}{\sim} N_k(0,V) && (\mbox{observation equation}) \label{dlmtdobseq}\\
 \theta_t &= G_t\theta_{t-1} + w_t && w_t \stackrel{ind}{\sim} N_p(0,W) && (\mbox{system equation}) \label{dlmtdsyseq}
\end{align}
for $t=1,2,\cdots T$ with the priors $\theta_0 \sim N_p(m_0, C_0)$, $V \sim IW(\Lambda_V, \lambda_V)$ and $W \sim IW(\Lambda_W, \lambda_W)$ with $(\theta_0,V,W)$ mutually independent. Then the full joint distribution of $(V,W,\theta,y)$ is
\begin{align}
  p(&V,W,\theta,y) \propto \exp\left[-\frac{1}{2}(\theta_0-m_0)'C_0^{-1}(\theta_0-m_0)\right] \nonumber\\
  &\times   |V|^{-(\lambda_V + k + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - F_t\theta_t)'V^{-1}(y_t - F_t\theta_t)\right] \nonumber\\
   & \times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t-G_t\theta_{t-1})'W^{-1}(\theta_t-G_t\theta_{t-1})\right]\label{dlmjoint}
 \end{align}
where $\tr(.)$ is the matrix trace operator.

In the following subsections, we provide derivations of the full conditional distributions for when using states, scaled disturbances or scaled errors as the data augmentation. 

\subsection{States}\label{subsec:states}

With the usual DA, the full conditional distributions can be derived from equation \eqref{dlmjoint}. First, the full conditional distribution of $\theta$ is as follows:
\begin{align*}
p(\theta&|V,W,y) \propto p(V,W,\theta,y) \propto \exp\left[-\frac{1}{2}(\theta_0-m_0)'C_0^{-1}(\theta_0-m_0)\right] \\
  &\times \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - F_t\theta_t)'V^{-1}(y_t - F_t\theta_t)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t-G_t\theta_{t-1})'W^{-1}(\theta_t-G_t\theta_{t-1})\right].
\end{align*}
It turns out that this density is Gaussian. In Section \ref{sec:MCFA}, we show how to use the mixed Cholesky factorization algorithm (MCFA) in order to efficiently determine and draw from this distribution.

The full conditional of $(V,W)$ is:
\begin{align*}
  p(V,W&|\theta,y) \propto p(V,W,\theta,y) \propto  |V|^{-(\lambda_V + k + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - F_t\theta_t)'V^{-1}(y_t - F_t\theta_t)\right]\\
   & \times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t-G_t\theta_{t-1})'W^{-1}(\theta_t-G_t\theta_{t-1})\right]\\
&\propto |V|^{-(\lambda_V + k + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\left(\Lambda_V +  \sum_{t=1}^T(y_t - F_t\theta_t)(y_t - F_t\theta_t)'\right) V^{-1}\right)\right]\\
 &\times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\left(\Lambda_W  + \sum_{t=1}^T(\theta_t-G_t\theta_{t-1})(\theta_t-G_t\theta_{t-1})'\right)W^{-1}\right)\right].
 \end{align*}
In other words, $V$ and $W$ are conditionally independent given $y$ and $\theta$ with
\begin{align*}
  V|\theta,y &\sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right), &
  W|\theta,y &\sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right) 
\end{align*}
where $v_t = y_t - F_t\theta_t$ and $w_t = \theta_t - G_t\theta_{t-1}$.

\subsection{Scaled disturbances}\label{subsec:SDs}

Let $L_W$ denote the Cholesky decomposition of $W$, i.e. the lower triangle matrix $L_W$ such that $L_WL_W' =W$. Then the scaled disturbances are $\gamma=\gamma_{0:T}=(\gamma_0',\gamma_1',\cdots,\gamma_T')'$ defined by $\gamma_0=\theta_0$ and $\gamma_t = L_W^{-1}(\theta_t-G_t\theta_{t-1})$ for $t=1,2,\cdots,T$. The reverse transformation is defined recursively by $\theta_0=\gamma_0$ and $\theta_t=L_W\gamma_t + G_t\theta_{t-1}$ for $t=1,2,\cdots,T$. Then the Jacobian is block lower triangular with the identity matrix and $T$ copies of $L_W$ along the diagonal blocks, so $|J| = |L_W|^T=|W|^{T/2}$. From equation \eqref{dlmjoint} we can write the full joint distribution of $(V,W,\gamma,y)$ as
 \begin{align}
  p(&V,W,\gamma,y) \propto \exp\left[-\frac{1}{2}(\gamma_0-m_0)'C_0^{-1}(\gamma_0-m_0)\right] \exp\left[-\frac{1}{2}\gamma_t'\gamma_t\right] \nonumber\\
  &\times |W|^{-(\lambda_W + p + 2)/2} |V|^{-(\lambda_V + k + T + 2)/2} \exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]  \nonumber\\
  &\times \exp\left[-\frac{1}{2}\left(\tr\left(\Lambda_VV^{-1}\right) + \sum_{t=1}^T\left[y_t-F_t\theta_t(\gamma,W)\right]'V^{-1}\left[y_t-F_t\theta_t(\gamma,W)\right]\right)\right]\label{dlmdistjoint}. 
 \end{align}
where $\theta_t(\gamma,W)$ denotes the recursive back transformation defined by the scaled disturbances. The full conditional distribution of $\gamma$ is then
\begin{align*}
  p(\gamma&|V,W,y) \propto p(V,W,\gamma,y) \propto \exp\left[-\frac{1}{2}(\gamma_0-m_0)'C_0^{-1}(\gamma_0-m_0)\right] \exp\left[-\frac{1}{2}\gamma_t'\gamma_t\right]\\
&\times \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\left[y_t-F_t\theta_t(\gamma,W)\right]'V^{-1}\left[y_t-F_t\theta_t(\gamma,W)\right]\right)\right]. 
\end{align*}
This density is Gaussian, but difficult to draw from. We use the MCFA to draw from $\theta|V,W,y$ instead, then transform from $\theta$ to $\gamma$ using the definition of $\gamma$.

Under this parameterization, the full conditional distribution of $(V,W)$ is
 \begin{align*}
  p(&V,W,|\gamma,y) \propto  p(V,W,\gamma,y) |W|^{-(\lambda_W + p + 2)/2} |V|^{-(\lambda_V + k + T + 2)/2} \exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]  \nonumber\\
  &\times \exp\left[-\frac{1}{2}\left(\tr\left(\Lambda_VV^{-1}\right) + \sum_{t=1}^T\left[y_t-F_t\theta_t(\gamma,W)\right]'V^{-1}\left[y_t-F_t\theta_t(\gamma,W)\right]\right)\right]. 
 \end{align*}
The back transformation from $\theta$ to $\gamma$ sets $\theta_0=\gamma_0$ and for $t=1,2,\cdots,T$
\begin{align*}
\theta_t &= L_W\gamma_t + G_t\theta_{t-1}\\
&= L_W\gamma_t + \sum_{s=0}^{t-2}G_tG_{t-1}\hdots G_{t-s}L_W\gamma_{t-s-1} + G_tG_{t-1}\hdots G_1\gamma_0\\
&= \sum_{s=0}^{t-1}\tilde{G}_{s,t}L_W\gamma_{t-s} + \tilde{G}_{t,t}\gamma_0
\end{align*}
where $\tilde{G}_{s,t} = G_tG_{t-1}\cdots G_{t-s + 1}$ for $s >0$ and $\tilde{G}_{0,t}=I_p$, the $p\times p$ identity matrix.. Then we can rewrite the conditional distribution of $(V,W)$ as
 \begin{align*}
  p(&V,W,|\gamma,y) \propto  p(V,W,\gamma,y) \propto |W|^{-(\lambda_W + p + 2)/2} |V|^{-(\lambda_V + k + T + 2)/2} \exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right] \exp\left[-\frac{1}{2}\left(\tr\left(\Lambda_VV^{-1}\right)\right)\right]  \nonumber\\
  &\times  \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\left[y_t-F_t\sum_{s=0}^{t}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]'V^{-1}\left[y_t-F_t\sum_{s=0}^{t-1}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]\right)\right]. 
 \end{align*}
This density is fairly complicated, so we resort to the full conditionals of $V$ and $W$ separately. The full conditional of $V$ is familiar:
 \begin{align*}
  p(&V|W,\gamma,y) \propto  p(V,W|\gamma,y) \propto |V|^{-(\lambda_V + k + T + 2)/2} \times \exp\left[-\frac{1}{2}\left(\tr
\left[\Lambda_V + \sum_{t=1}^Tv_tv_t'\right]V^{-1}\right)\right]
 \end{align*}
where $v_t = y_t - F_t\sum_{s=0}^{t}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0 = y_t - F_t\theta_t$. This implies that
\begin{align*}
  V|W,\gamma,y &\sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)
\end{align*}
which is the same distribution as for $V|\theta,y$. 

The full conditional density of $W$ is more complicated:
 \begin{align*}
  p(W&|V,\gamma,y) \propto  p(V,W,\gamma,y) \propto |W|^{-(\lambda_W + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\\
&\times  \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\left[y_t-F_t\sum_{s=0}^{t}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]'V^{-1}\left[y_t-F_t\sum_{s=0}^{t-1}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]\right)\right]. 
 \end{align*}
In the local level model, the density is even simpler. The local level model assumes that $y_t$ and $\theta_t$ are univariate for all $t$ and that $F_t=G_t=1$. In addition, the prior on $W$ reduces to an inverse gamma prior, $IG(\alpha_W,\beta_W)$. In this case, the full conditional density of $W$ becomes
\begin{align*}
  p(W&|V,\gamma,y) \propto W^{-\alpha_W   - 1}\exp\left[-\frac{1}{W}\beta_W  \right]  \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\left[y_t-\sum_{s=0}^{t}\gamma_{t-s}\sqrt{W}\right]'V^{-1}\left[y_t-\sum_{s=0}^{t-1}\gamma_{t-s}\sqrt{W}\right]\right)\right]\\
&\propto W^{-\alpha_W - 1}\exp\left[-a_\gamma W + b_\gamma \sqrt{W} -\frac{\beta_W}{W}\right]. 
\end{align*}
where $a_\gamma =\sum_{t=1}^T(\sum_{s=1}^t\gamma_j)^2/2V$ and $b_\gamma =\sum_{t=1}^T(y_t-\gamma_0)(\sum_{s=1}^t\gamma_j)/V$. In Section \ref{sec:scaledraw} we show how to efficiently obtain a random draw from this density.

\subsection{Scaled errors}\label{subsec:SEs}
Let $L_V$ denote the Cholesky decomposition of $V$, that is $L_VL_V'=V$, then we can define the scaled errors as $\psi_t = L_V^{-1}(y_t - F_t\theta_t)$ for $t=1,2,\cdots,T$ and $\psi_0 = \theta_0$. Here we assume that $k=p$ and that $F_t$ is invertible for all $t$. Then the back transformation is $\theta_t = F_t^{-1}(y_t - L_V\psi_t)$ for $t=1,2,\cdots,T$ and $\theta_0=\psi_0$. The Jacobian of this transformation is block diagonal with a single copy of the identity matrix along with the $F_t^{-1}L_V$'s along the diagonal, so $|J|=(\prod_{t=1}^T|F_t|^{-1})|V|^{T/2}$. Then from equation \eqref{dlmjoint} we can write the joint distribution of $(V, W, \psi, y)$ as
\begin{align}
    p(&V,W,\psi,y) \propto \exp\left[-\frac{1}{2}(\psi_0-m_0)'C_0^{-1}(\psi_0-m_0)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\psi_t'\psi_t\right] \nonumber\\
  &\times |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right]  \times |W|^{-(\lambda_W + p + T + 2)/2} \nonumber\\
   & \exp\left[-\frac{1}{2}\left(\tr\left(\Lambda_WW^{-1}\right) + \sum_{t=1}^T(y_t - \mu_t)'(F_tWF_t')^{-1}(y_t-\mu_t)\right)\right]\label{dlmerrorjoint}
\end{align}
where we define $\mu_1 = L_V\psi_1 + F_1G_1\psi_0$ and for $t=2,3,\cdots,T$, $\mu_t =L_V\psi_t + F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{V}\psi_{t-1})$. The $|F_t|^{-1}$'s have been absorbed into the normalizing constant, but if they depended on some unknown parameter then we could not do this and as a result would have to take them into account in the Gibbs step or steps for the model parameters.

The full conditional distribution of $\psi$ is
\begin{align*}
    p(&V,W,\psi,y) \propto \exp\left[-\frac{1}{2}(\psi_0-m_0)'C_0^{-1}(\psi_0-m_0)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\psi_t'\psi_t\right] \nonumber\\
   & \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T(y_t - \mu_t)'(F_tWF_t')^{-1}(y_t-\mu_t)\right)\right]
\end{align*}
where note that $\mu_t$ depends on $\psi$. This density is Gaussian and like with $\gamma$, we can use the MCFA from Section \ref{sec:MCFA} to draw from the full conditional of $\theta$ and then transform from $\theta$ to $\psi$. However it turns out the precision matrix of $\psi$'s full conditional distribution has the necessary block tridiagonal structure, so we use the MCFA directly on $\psi$. 

The full conditional distribution of $(V,W)$ is complicated, like the case of the scaled disturbances, so we find the full conditionals of $V$ and $W$ separately instead. The full conditional of $W$ is 
\begin{align*}
 p(&W|V,\psi,y) \propto   p(V,W,\psi,y) \propto \times |W|^{-(\lambda_W + p + T + 2)/2} \exp\left[-\frac{1}{2}\left(\tr\left(\left[\Lambda_W + \sum_{t=1}^TF_t^{-1}(y_t - \mu_t)(y_t-\mu_t)'(F_t^{-1})'\right]W^{-1}\right)\right)\right],
\end{align*}
in other words
\begin{align*}
  W|V,\psi,y &\sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right) 
\end{align*}
where $w_t = F_t^{-1}(y_t - \mu_t) = \theta_t - G_t\theta_{t-1}$.

The full conditional distribution of $V$ is more complicated:
\begin{align*}
 p(V&|W,\psi,y) \propto p(V,W,\psi,y) \propto |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1} + \sum_{t=1}^T(y_t - \mu_t)'(F_tWF_t')^{-1}(y_t-\mu_t)\right)\right]
\end{align*}
with $\mu_t$ a function of $V$, defined above. In the local level model with an $IG(\alpha_V,\beta_V)$ prior on $V$, this density is simpler:
\begin{align*}
 p(V&|W,\psi,y) \propto V^{-alpha_V - 1}\exp\left[-\frac{\beta_V}{V} + \frac{1}{W}\sum_{t=1}^T(y_t - \mu_t)'(y_t-\mu_t)\right]
\end{align*}
where $\mu_1 = \sqrt{V}\psi_1 + \psi_0$ and for $t=2,3,\cdots,T$, $\mu_t = \sqrt{V}(\psi_t - \psi_{t-1}) + y_{t-1}$. Thus
\begin{align*}
 p(V|W,\psi,y) \propto V^{-\alpha_V - 1}\exp\left[ -a_{\psi}V + b_{\psi}\sqrt{V} -\frac{\beta_V}{V}\right] 
\end{align*}
where $a_{\psi}=\sum_{t=1}^T(\mathcal{L}\psi_t)^2/2W$ and $b_{\psi}=\sum_{t=1}^T(\mathcal{L}\psi_t\mathcal{L}y_t)/W$, and we define $\mathcal{L}y_t=y_t-y_{t-1}$ for $t=2,3,\cdots,T$, $\mathcal{L}y_1=y_1 - \psi_0$, $\mathcal{L}\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$ and $\mathcal{L}\psi_1=\psi_1-0$. In other words, the form of $p(V|W,\psi,y)$ is the same as $p(W|V,\gamma,y)$. The general form of these two densities is $p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b\sqrt{x} -c/x\right]$. In Section \ref{sec:scaledraw} we show how to efficiently sample from this distribution.

\subsection{The wrongly-scaled disturbances}\label{subsec:WSDs}

The wrongly-scaled disturbances are defined as  $\tilde{\gamma}=\tilde{\gamma}_{0:T}=(\tilde{\gamma}_0',\tilde{\gamma}_1',\cdots,\tilde{\gamma}_T')'$. The wrongly-scaled disturbances are related to the scaled disturbances by $\tilde{\gamma}_t = L_V^{-1}L_W\gamma_t$ for $t=1,2,\cdots,T$ and $\tilde{\gamma_0}=\gamma_0$. The reverse transformation is $\gamma_t = L_W^{-1}L_V\tilde{\gamma}_t$ and the Jacobian is block diagonal with a copy of the identity matrix and $T$ copies of $L_W^{-1}L_V$ along the diagonal. Thus $|J|=|L_W|^{-T}|L_V|^T=|W|^{-T/2}|V|^{T/2}$. Then from equation \eqref{dlmdistjoint} we can write the joint distribution of $(V,W,\tilde{\gamma},y)$ as
 \begin{align}
  p(&V,W,\tilde{\gamma},y) \propto \exp\left[-\frac{1}{2}(\tilde{\gamma}_0-m_0)'C_0^{-1}(\tilde{\gamma}_0-m_0)\right] |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right]  \nonumber\\
  &\times  \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)'V^{-1}\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)\right]\nonumber\\
   & \times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\gamma}_t'(L_V^{-1}W(L_V^{-1})')^{-1}\tilde{\gamma}_t\right]\label{dlmWSDjoint}
 \end{align}
where $\theta_t(\tilde{\gamma},L_V)$ denotes the transformation from $\tilde{\gamma}$ to $\theta$ defined by the wrongly-scaled disturbances. 

Now from equation \eqref{dlmWSDjoint}, we can write the full conditional density of $\tilde{\gamma}$ as 
\begin{align*}
p(\tilde{\gamma}|V,W,y) \propto & \exp\left[-\frac{1}{2}(\tilde{\gamma}_0-m_0)'C_0^{-1}(\tilde{\gamma}_0-m_0)\right]  \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\gamma}_t'(L_V^{-1}W(L_V^{-1})')^{-1}\tilde{\gamma}_t\right]\\
   &\times  \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)'V^{-1}\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)\right].
\end{align*}
This density is gaussian but difficult to draw from, so we use the MCFA to draw $\theta|V,W,y$ instead, then transform from $\theta$ to $\tilde{\gamma}$.

Then full conditional density of $(V,W)$ is complicated, but their separate full conditionals are easier to work with. The full conditional density of $W$ is
\begin{align*}
  p(W|V,\tilde{\gamma},y)\propto & |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\left[\Lambda_W + \sum_{t=1}^TL_V\tilde{\gamma}_t\tilde{\gamma}_t'L_V'\right]W^{-1} \right)\right],
\end{align*}
i.e. 
\begin{align*}
W|V,\tilde{\gamma},y \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t', \lambda_W + T\right)
\end{align*}
where $w_t = L_V\tilde{\gamma}_t = \theta_t - G_t\theta_{t-1}$. The full conditional density of $V$ is more complicated, from equation \eqref{dlmWSDjoint}:
\begin{align*}
  p(V|W,\tilde{\gamma},y) \propto &  |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\gamma}_t'(L_V^{-1}W(L_V^{-1})')^{-1}\tilde{\gamma}_t\right]\\
  &\times  \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)'V^{-1}\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)\right].
 \end{align*}
In the local level model with an $IG(\alpha_V, \beta_V)$ prior on $V$, this density becomes simpler. Since in that case $\theta_t = \sqrt{V}\sum_{s=1}^t\tilde{\gamma}_s + \tilde{\gamma}_0$, we have
\begin{align*}
p(V|W,\tilde{\gamma},y)\propto V^{-\alpha_V-1}\exp\left[ -a_{\tilde{\gamma}}V + b_{\tilde{\gamma}}/\sqrt{V} -c_{\tilde{\gamma}}/V\right]
\end{align*} 
where $a_{\tilde{\gamma}} = \frac{1}{2W}\sum_{t=1}^T\tilde{\gamma}_t^2$, $b_{\tilde{\gamma}} = \sum_{t=1}^T(y_t - \tilde{\gamma}_0)\sum_{s=1}^t\tilde{\gamma}_s$, and $c_{\tilde{\gamma}} = \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\gamma}_0)^2$. We show in Section \ref{sec:wscale} how to efficiently obtain a random draw from this density.

\subsection{The wrongly-scaled errors}\label{subsec:WSEs}

The wrongly-scaled errors are denoted by $\tilde{\psi}=\tilde{\psi}_{0:T}=(\tilde{\psi}_0',\tilde{\psi}_1',\cdots,\tilde{\psi}_T')'$. They  are related to the scaled errors by $\tilde{\psi}_t=L_W^{-1}L_V\psi_t$ for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\psi_0$. Then $\psi_t = L_V^{-1}L_W\tilde{\psi}_t$ and the Jacobian is block diagonal with a copy of the identical matrix and $T$ copies of $L_V^{-1}L_W$ along the diagonal. So $|J|=|V|^{-T/2}|W|^{T/2}$ and from equation \eqref{dlmerrorjoint} we can write the joint distribution of $(V, W, \tilde{\psi}, y)$ as
\begin{align}
    p(&V,W,\tilde{\psi},y) \propto \exp\left[-\frac{1}{2}(\tilde{\psi}_0-m_0)'C_0^{-1}(\tilde{\psi}_0-m_0)\right] \nonumber\\
   &\times |V|^{-(\lambda_V + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\psi}_t'(L_W^{-1}V(L_W^{-1})')^{-1}\tilde{\psi}_t\right] \nonumber\\
    & \times |W|^{-(\lambda_W + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\mu}_t)'(F_tWF_t')^{-1}(y_t-\tilde{\mu}_t)\right]\label{dlmWSEjoint}
 \end{align}
where we define $\tilde{\mu}_1 = L_W\tilde{\psi}_1 - F_1G_1\tilde{\psi_0}$ and for $t=2,3,\cdots,T$ $\tilde{\mu}_t =L_W\tilde{\psi}_t - F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{W}\tilde{\psi}_{t-1})$.

From equation \eqref{dlmWSEjoint} the full conditional distribution of $\tilde{\psi}$ is
\begin{align*}
    p(\tilde{\psi}|V,W,y) \propto & \exp\left[-\frac{1}{2}(\tilde{\psi}_0-m_0)'C_0^{-1}(\tilde{\psi}_0-m_0)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\psi}_t'(L_W^{-1}V(L_W^{-1})')^{-1}\tilde{\psi}_t\right]\\
&\times  \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\mu}_t)'(F_tWF_t')^{-1}(y_t-\tilde{\mu}_t)\right].
\end{align*}
This density is again Gaussian and it can be shown that the precision matrix is tridiagonal, so the MCFA can be directly applied. The full conditional density of $V$ is the familiar inverse Wishart:
\begin{align*}
    p(V|W,\tilde{\psi},y) \propto& |V|^{-(\lambda_V + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\psi}_t'(L_W^{-1}V(L_W^{-1})')^{-1}\tilde{\psi}_t\right]. 
 \end{align*}
So $V|W,\tilde{\psi},y \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t', \lambda_V + T\right)$ where $v_t = L_W\tilde{\psi}_t = y_t - F_t\theta_t$.

The full conditional density of $W$ is more complicated, but has the same form as the full conditional density of $V$ given $\tilde{\gamma}$:
\begin{align*}
  p(W|V,\tilde{\psi},y) \propto& |W|^{-(\lambda_W + p + 2)/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\psi}_t'(L_W^{-1}V(L_W^{-1})')^{-1}\tilde{\psi}_t\right] \\
  & \times \exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\mu}_t)'(F_tWF_t')^{-1}(y_t-\tilde{\mu}_t)\right].
\end{align*}
This density is a bit complicated, but in the case of the local level model with a $IG(\alpha_W,\beta_W)$ prior on $W$, it simplifies to
\begin{align*}
p(W|V,\tilde{\psi},y) \propto& W^{-\alpha_W-1}\exp\left[ -a_{\tilde{\psi}}W + b_{\tilde{\psi}}/\sqrt{W} -c_{\tilde{\psi}}/W\right]
\end{align*}
where $a_{\tilde{\psi}} = \frac{1}{2V}\sum_{t=1}^T\tilde{\psi}_t^2$,  $b_{\tilde{\psi}} = \sum_{t=1}^T\mathcal{L}\tilde{y}_t\mathcal{L}\tilde{\psi}_t$, and $c_{\tilde{\psi}} = \beta_W + \frac{1}{2}\sum_{t=1}^T\mathcal{L}\tilde{y}_t^2$. Here we define $\mathcal{L}y_t = y_t - y_{t-1}$ for $t=2,3,\cdots,T$ while $\mathcal{L}y_1 = y_1 - \tilde{\psi}_0$, and $\mathcal{L}\tilde{\psi}_t = \tilde{\psi}_t - \tilde{\psi}_{t-1}$ for $t=2,3,\cdots,T$ while $\mathcal{L}\tilde{\psi}_1 = \tilde{\psi}_1 - 0$. This is the same family of densities as $p(V|W,\tilde{\gamma},y)$, and in Section \ref{sec:wscale} we show how to efficiently obtain random draws.

\section{Mixed Cholesky Factorization Algorithm (MCFA) for Simulation Smoothing}\label{sec:MCFA}

Traditionally in DLMs, the Kalman filter is used in order to draw from the latent states $\theta_{0:T}$ through forward filtering, backward sampling (FFBS). This requires running the Kalman filter in order to determine the marginal distribution of $\theta_T$, then drawing $\theta_t|\theta_{t+1:T}$ for $t=T-1,T-2,\cdots,1$ \cite{carter1994gibbs,fruhwirth1994data}. The mixed Cholesky factorization algorithm (MCFA) determines the joint distribution of $\theta_{0:T}$ and draws from it using a backward sampling step as in FFBS. The idea comes from \citet{rue2001fast}, which introduces a Cholesky factorization algorithm (CFA) for drawing from a Gaussian Markov random field and notes that the conditional distribution of $\theta_{0:T}$ given $y_{1:T}$ in a Gaussian linear statespace model is a special case. The algorithm exploits the fact that the full conditional distribution of $\theta_{0:T}$ is Gaussian with a block tridiagonal precision matrix in order to quickly compute its Cholesky decomposition. \citet{mccausland2011simulation} improves the idea by implicitly computing this Cholesky decomposition through a backward sampling strategy, starting with sampling from the marginal distribution of $\theta_T$. 

Suppose our model is as follows:
\begin{align*}
  y_t &= F_t\theta_t + v_t\\
  \theta_t & = G_t\theta_{t-1} + w_t
\end{align*}
with $v_t\stackrel{ind}{\sim} N(0,V_t)$ independent of $w_t\stackrel{ind}{\sim}N(0,W_t)$ for $t=1,2,\cdots,T$ and $\theta_0\sim N(m_0,C_0)$. This is the usual DLM except now we allow for time dependent variances for illustrative pruposes. Then $(y_{1:T},\theta_{0:T})$ is joint Gaussian conditional on $(V_{1:T},W_{1:T})$ (in this section, everything is conditonal on $V_{1:T}$ and $W_{1:T}$, so we will not make this conditioning explicit). So we can write $p(\theta_{0:T}|y_{1:T})$ as
\begin{align*}
  \log p(\theta_{0:T}|y_{1:T}) = -\frac{1}{2}g(\theta_{0:T},y_{1:T}) + K
\end{align*}
where $K$ is some constant with respect to $\theta_{0:T}$ and
\begin{align*}
  g(\theta_{0:T},y_{1:T}) = \theta_{0:T}'\Omega\theta_{0:T} - 2a'\theta_{0:T}.
\end{align*}
However, we also have
\begin{align*}
  \log p(\theta_{0:T}|y_{1:T}) = \log p(\theta_{0:T},y_{1:T}) - \log p(y_{1:T}).
\end{align*}
This means that
\begin{align*}
  g(\theta_{0:T}&,y_{1:T}) = (\theta_0 - m_0)C_0^{-1}(\theta_0 - m_0) + K'\\
  & + \sum_{t=1}^T(y_t - F_t\theta_t)'V_t^{-1}(y_t - F_t\theta_t) \\
  & + \sum_{t=1}^T(\theta_t - G_t\theta_{t-1})'W_t^{-1}(\theta_t - G_t\theta_{t-1}).
\end{align*}
where $K'$ is another constant that doesn't depend on $\theta_{0:T}$.

So now we can identify blocks of $\Omega$ with the cross product terms of the $\theta_t$'s and blocks of $a$ with the single product terms. Specifically, $\Omega$ is a banded diagonal matrix with
\begin{align*}
  \Omega = \begin{bmatrix} \Omega_{00} & \Omega_{01} & 0 &\ddots & 0 & 0\\
    \Omega_{10} & \Omega_{11} & \Omega_{12} & \ddots  & 0            & 0\\
    0          & \Omega_{21} & \Omega_{22} & \ddots  & 0            & 0\\
    \ddots     & \ddots     & \ddots     & \ddots  & \ddots       & \ddots \\
    0          & 0          & 0          & \ddots  & \Omega_{T-1,T-1} & \Omega_{T-1,T}\\
    0          & 0          & 0          & \ddots  & \Omega_{T,T-1} & \Omega_{TT}\end{bmatrix}
\end{align*}
and $a = (a_0', a_1', \cdots, a_T')$ where the $\Omega_{st}$'s and $a_{t}$'s defined below:
\begin{align*}
  \Omega_{00} & = C_0^{-1} + G_1'W_1^{-1}G_1 && \\
  \Omega_{tt} & = F_t'V_t^{-1}F_t + W_t^{-1} + G_{t+1}'W_{t+1}^{-1}G_{t+1} &&  \mathrm{ for }\ \  t=1,2,\cdots T-1\\
  \Omega_{TT} & = F_T'V_T^{-1}F_T + W_T^{-1} && \\
  \Omega_{t,t-1} & = - W_t^{-1}G_t &&  \mathrm{ for }\ \  t=1,2,\cdots T\\
  \Omega_{t-1,t} & = - G_t'W_t^{-1} = \Omega_{t,t-1}' && \mathrm{ for }\ \  t=1,2,\cdots T\\
  a_0 & = C_0^{-1}m &&\\
  a_t &= F_t'V_t^{-1}y_t &&  \mathrm{ for }\ \  t=1,2,\cdots T.
\end{align*}
Together, $\Omega$ and $a$ determine the Gaussian distribution from which $\theta_{0:T}$ should be drawn. \citet{rue2001fast} shows how to take advantage of the sparsity of $\Omega$ in order to quickly compute its Cholesky factorization and in order to find the mean vector from $a$ and this factorization. \citet{mccausland2011simulation} shows that instead of computing these quantities directly, you can draw $\theta_T$ and $\theta_t|\theta_{t+1:T}$ iteratively, which ultimately reduces the number of linear algebra operations which must be performed and typically speeds up the computation despite taking advantage of essentially the same mathematical technology.

The resulting algorithm requires a couple more intermediate quantities. Let $\Sigma_0 = \Omega_{00}^{-1}$, $\Sigma_t = (\Omega_{tt} - \Omega_{t,t-1}\Sigma_{t-1}\Omega_{t-1,t})^{-1}$ for $t=1,2,\cdots,T$, $m_0 = \Sigma_0a_0$, and $m_t = \Sigma_t(a_t - \Omega_{t,t-1}m_{t-1})$ for $t=1,2,\cdots,T$. Then
\begin{align*}
  \theta_T \sim & N(m_T, \Sigma_T) &&\\
  \theta_{t|t+1:T} \sim & N(m_t - \Sigma_t\Omega_{t,t+1}\theta_{t+1}, \Sigma_t) && \mathrm{for}\ \ t=T-1,T-2,\cdots,0.
\end{align*}
\citet{mccausland2011simulation} shows how to quickly compute the required linear algebra operations and finds that this method is often faster than simply doing the Cholesky factorization. This algorithm can also be applied to drawing the scaled errors, $\psi_{0:T}$, and the wrongly-scaled errors, $\tilde{\psi}_{0:T}$.

\section{Further augmentation for non-invertible $F_t$}\label{sec:F}

Throughout the paper we assumed that $F_t$ is square and invertible for all $t$ which made the construction of the SE sampler and other samplers that use the scaled errors easier. However, most DLMs do not have $F_t$'s which are square, let alone invertible. The samplers we constructed can still be used in this case with one tweak: an additional DA is required in order to ensure that $F_t$ is square and invertible for all $t$. The basic strategy is to add elements to $y_t$ or $\theta_t$ or both until $F_t$ is invertible, then add an additional step to the sampler in order to draw the new augmentation. A second issue is that often $G_t$ or $F_t$ or both depend on some unknown parameter which must also be sampled from in the various MCMC samplers. The second case is easily dealt with simply by adding another sampling step for the unknown parameters in $F_t$ and $G_t$. The following example illustrates how to deal with the first case.

Consider the dynamic regression model
\begin{align*}
y_t & = \alpha_t + x_t\beta_t + v_t\\
\alpha_t & = \alpha_{t-1} + w_{1,t}\\
\beta_t & = \beta_{t-1} + w_{2,t}\\
\end{align*}
for $t=1,2,\cdots,T$ with $v_{1:T}$ independent of $w_{1:T}=(w_1',w_2',\cdots,w_T')'$ where $w_t=(w_{1,t},w_{2,t})'$, $v_t\stackrel{iid}{\sim} N(0,V)$ and $w_t \stackrel{iid}{\sim}N_2(0,W)$.  Here the latent state in period $t$ is $\theta_t=(\alpha_t,\beta_t)'$. The problem is that $F_t=[1,x_t]$ is neither square nor invertible. But notice that the matrix
\[
F^*_t = \begin{bmatrix} 1 & x_t \\ 0 & 1 \end{bmatrix}
\]
is invertible. Now we add an additional DA $z_t$ to $y_t$ to construct $y_t^* = (y_t, z_t)'$ so that now the model is
\begin{align*}
y_t^* &= F_t^*\theta_t + v_t^*\\
\theta_t& = \theta_{t-1} + w_t
\end{align*}
where $v_t^* = (v_t, u_t)$ where $u_{1:T}$ is independent of $(v_{1:T}, w_{1:T})$ and $u_t\stackrel{iid}{\sim} N(0,1)$. By construction $v_t^*\stackrel{iid}{\sim} N_2(0,V^*)$ where $V^*$ is a diagonal matrix with the vector $(V,1)$ along the diagonal and the full conditional distribution of $z_t$ is $N(\beta_t,1)$. Then we define the scaled errors as $\psi_0 = \theta_0$ and $\psi_t=L_{V^*}^{-1}(y_t^* - F_t^*\theta_t)$. Let $z=z_{1:T}$ and $y^*=y^*_{1:T}$ for brevity. 

In terms of $\theta$, the likelihood is
\begin{align*}
p(y,z,\theta|V,W) \propto& |V^*|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t^* - F_t^*\theta_t)'(V^*)^{-1}(y_t^* - F_t^*\theta_t)\right]\\
 &\times |W|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t - \theta_{t-1})'W^{-1}(\theta_t - \theta_{t-1})\right]\\
\propto & V^{-T/2}\exp\left[-\frac{1}{2V}\sum_{t=1}^T(y_t - \alpha_t - x_t\beta_t)^2\right]\exp\left[-\frac{1}{2}\sum_{t=1}^t(z_t - \beta_t)^2\right]\\
 &\times |W|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t - \theta_{t-1})'W^{-1}(\theta_t - \theta_{t-1})\right]
\end{align*}
Then by transforming to $\psi$, the back transformation is $\theta_t = (F_t^*)^{-1}(y_t^* - L_{V^*}\psi_t)$ so the Jacobian is block diagonal with $T$ copies of $(F_t^*)^{-1}L_{V^*}$ along with a single copy of the identity matrix along the diagonal. So the deterimant of the Jacobian is $|J| = |V^*|^{T/2}$ and the likelihood can be written in terms of $\psi$ as
\begin{align}
p(y,z,\theta|V,W) \propto& \exp\left[-\frac{1}{2}\sum_{t=1}^T\psi_t'\psi_t\right] |W|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t^* - \mu_t)'(F_t^*W (F_t^*)')^{-1}(y^*_t - \mu_t)\right] \label{Fpsilike}.
\end{align}
where we define $\mu_1 = L_{V^*}\psi_1 + F^*_1\psi_0$ and for $t=2,3,\cdots,T$, $\mu_t =L_{V^*}\psi_t + F^*_t(F^*_{t-1})^{-1}(y^*_{t-1} - L_{V^*}\psi_{t-1})$. 

Now in order to construct a sampler that uses $\psi$, we simply add a new step to sampler to draw $z$ from its full conditional just before transforming to $\psi$. In the GIS and alternating algorithms, we now have to draw an updated $z$ every time we change the DA. When using the states, $z_t|V,W,\theta,y \stackrel{iid}{\sim}N(\beta_t,1)$, so it is easiest to transform to $\theta$ before drawing $z$. So for example in the SD-SE GIS sampler with $V$, $W$, $\alpha_0$, and $\beta_0$ independent in the prior, an $IG(\alpha_V,\beta_V)$ prior on $V$, and an $IW(\Lambda_W,\lambda_W)$ prior on $W$, the algorithm becomes
\begin{alg*}[SD-SE GIS for dynamic regression]Scaled Disturbance-Scaled Error GIS Sampler for the dynamic regression model
\begin{enumerate}
\item Use the MCFA to sample $\theta \sim p(\theta|V,W,y)$.
\item Sample $V \sim IG\left(\alpha_V + T/2, \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \alpha_t - \beta_t)^2\right)$.
\item Transform $\theta$ to $\gamma$.
\item Sample $W \sim p(W|V,\gamma,y)$.
\item Transform $\gamma$ to $\theta$.
\item Sample $z_t\stackrel{iid}{\sim}N(\beta_t,1)$ and form $y^*$.
\item Transform $\theta$ to $\psi$.
\item Sample $V \sim p(V|W,\psi,y^*)$.
\item Sample $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
\end{enumerate}
\end{alg*}\noindent
Step 8 is particularly tricky since $V$ is a component of $V^*$, and $V^*$ has the same density $p(V|W,\psi,y)$ that shows up in the usual case of the scaled disturbances, except now the lower right diagonal element is set to one. So while we can write down the various algorithms in the noninvertible $F$ case, the density $p(V|W,\psi,y^*)$ is tricky to work with. In step 8 $V$ is drawn conditional on $y^*$, but another option is to draw $V$ conditional on $y$ but not on $z$. This would require integrating $z$ out of the likelihood, equation \eqref{Fpsilike}. It is not clear which of these is easier, or faster though it is likely that the changing the prior for $V$ and $W$ will have an impact.

\section{Efficiently drawing from $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$ in the LLM}\label{sec:scaledraw}
Both of these two densities are of the form
\begin{align*}
\log p(x) =  & - (\alpha + 1)\log x -ax + b\sqrt{x}  -c/x + C 
\end{align*}
for $x>0$ where $C$ is some constant, $\alpha>0$ and $c>0$ are the hyperparameters for $x$, and $a>0$ and $b\in \Re$ are parameters that depend on the data, $y$, the relevant data augmentation ($\psi$ or $\gamma$), and the other variable ($W$ or $V$). This density is not a known form and is difficult to sample from. We provide two different rejection sampling strategies below that work well under different circumstances, and combine them into a single strategy.

\subsection{Adaptive rejection sampling}
One nice strategy is to use adaptive rejection sampling, e.g. \citet{gilks1992adaptive}. This requires $\log p(x)$ to be concave, which is easy enough to check. The second derivative of $\log p(x)$ is:
\begin{align*}
\frac{\partial^2 \log p(x)}{\partial x^2} &= -\frac{1}{4}bx^{-3/2} +(\alpha + 1)x^{-2} -2 c x^{-3}.
\end{align*}
Then we have
\begin{align*}
  &\frac{\partial^2 \log p(x)}{\partial x^2} < 0 && \iff &&-\frac{b}{4}x^{3/2} + (\alpha + 1)x - 2c < 0
\end{align*}
which would imply that $\log p(x)$ is concave. We can maximize the left hand side of the last equation very easily. When $b\leq 0$ the max occurs at $x=\infty$ such that $LHS > 0$, but when $b > 0$:
\begin{align*}
  \frac{\partial LHS}{\partial x} &= -\frac{3}{8}bx^{1/2} + \alpha + 1 = 0 && \implies && x^{max} = \frac{(\alpha + 1)^2}{b^2}\frac{64}{9}.
\end{align*}
Then we have
\begin{align*}
  LHS \leq LHS|_{x=x^{max}} = \frac{(\alpha + 1)^3}{b^2}\frac{64}{27} - 2c
\end{align*}
so that
\begin{align*}
  LHS|_{x=x^{max}} < 0 &\iff  \frac{(\alpha + 1)^3}{b^2}\frac{64}{27} < 2c &&\iff&& b > \left(\frac{(\alpha + 1)^3}{c}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}.
\end{align*}
This last condition is necessary and sufficient for $\log p(x)$ to be globally (for $x>0$) concave since $b < 0$ forces $LHS > 0$ for some $x$. When the condition is satisfied, we can use adaptive rejection sampling --- which is already implemented in the \verb0R0 package \verb0ars0 \cite{PerezARS}. We input the initial evaluations of $\log p(x)$ at the mode $x^{mode}$ and at $2x^{mode}$ and $0.5x^{mode}$ in order to get the algorithm going.

\subsection{Rejection sampling on the log scale}

When $b \leq \left(\frac{(\alpha + 1)^3}{c}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}$, which happens often --- especially for small $T$ --- we need to rely on a different method to sample from $p(x)$. A naive approach would be to construct a normal or $t$ approximation to $p(x)$ and use that as a proposal in a rejection sampler. It turns out that this is often very inefficient, but for $z=\log(x)$ the approach works well. Note that
\begin{align*}
  p_z(z) = p_x(e^z)e^z
\end{align*}
so that we can write the log density of $z$ as (dropping the subscripts):
\begin{align*}
  \log p(z) = -ae^z + be^{z/2} - \alpha z - c e^{-z}.
\end{align*}
The mode of this density $z^{mode}$ can be easily found numerically, and the second derivative is:
\begin{align*}
  \frac{\partial^2 \log p(z)}{\partial z^2} = -ae^z + \frac{b}{4}e^{z/2} - c e^{-z}.
\end{align*}
The $t$ approximation then uses the proposal distribution 
p\begin{align*}
  t_{v}\left(z^{mode}, \left[-\left.\frac{\partial^2 \log p(z)}{\partial z^2}\right|_{z=z^{mode}}\right]^{-1}\right).
\end{align*}
In practice choosing degrees of freedom $v=1$ works very well over the region of the parameter space where adaptive rejection sampling cannot be used. We can easily use this method when adaptive rejection sampling does not work, then transform $z$ back to $x$. It remains to check that the tails of $t$ distribution dominate the tails of our target distribution. Let $\log q(z)$ denote the log density of the proposal distribution. Then we need
\begin{align*}
  \log p(z) - \log q(z) \leq M
\end{align*}
for some constant M, i.e.
\begin{align*}
  -ae^z + be^{z/2} - \alpha z - c e^{-z} -\left(\frac{v+1}{2}\right)\log\left[1 + \frac{1}{v}\left(\frac{z-\mu}{\sigma}\right)^2\right]\leq M
\end{align*}
where $a>0$, $c>0$, $\alpha>0$, $v>0$, $\sigma>0$, and $b,\mu\in \Re$. We can rewrite the LHS as
\begin{align*}
    e^{z/2}(b-ae^{z/2}) - \alpha z - c e^{-z} -\left(\frac{v+1}{2}\right)\log\left[1 + \frac{1}{v}\left(\frac{z-\mu}{\sigma}\right)^2\right].
\end{align*}
So as $z\to\infty$ this quantity goes to $-\infty$ since the first term will eventually become negative no matter the value of $b$, and all other terms are always negative. Similarly as $z\to -\infty$ this quantity goes to $-\infty$. Now pick any interval $(z_1,z_2)$ such that outside of the interval, $LHS<\epsilon$. Since treated as a function of $z$ the LHS is clearly continuous, it attains a maximum on this interval, and thus is bounded.

\subsection{Intelligently choosing a rejection sampler}
In practice, adaptive rejection sampling is relatively efficient for $p_x(x)$ but inefficient for $p_z(z)$ --- so much so that rejection sampling with the $t$ approximation for $p_z(z)$ is more efficient. To minimize computation time, it is best to use adaptive rejection sampling for $p_x(x)$ when the concavity condition is satisfied. When it is not, the $t$ approximation works well enough.

\section{Efficiently drawing from $p(W|V,\tilde{\gamma},y)$ and $p(V|W,\tilde{\psi},y)$ in the LLM}\label{sec:wscale}

Both the density of $\log(W)|V,\tilde{\gamma},y$ and the density of $\log(V)|W,\tilde{\psi},y$ have the following form:
\begin{align*}
  p(z)\propto \exp\left[-\alpha z - ae^{-z} + be^{-z/2} - ce^z\right].
\end{align*}
where $\alpha>0$, $a>0$, $c>0$, and $b\in \Re$. The log density is:
\begin{align*}
  \log p(z) = -\alpha z - ae^{-z} + be^{-z/2} - ce^z + C
\end{align*}
where $C$ is some constant. We only provide one strategy for rejection sampling from this density: the $t$ approximation. Similar reasoning to the previous subsection above shows that we can use a $t$ distribution as a proposal in a rejection sampler for this density. Now we choose the location parameter by maximizing $\log p(z)$ in $z$ numerically to find the mode, $z^{mode}$. Next the second derivative of $\log p(z)$ is given by
\begin{align*}
  \frac{\partial^2 \log p(z)}{\partial z^2} = -ae^{-z} + \frac{b}{4}e^{-z/2}-ce^z.
\end{align*}
We then set the scale parameter to be
\begin{align*}
  -\left[\left.\frac{\partial^2 \log p(z)}{\partial z^2}\right|_{z=z^{mode}}\right]^{-1}
\end{align*}
as in the normal approximation, and the degrees of freedom parameter to $v=1$. This rejection sampler is tolerably efficient for our purposes, but there is much room for improvement.

\section{Using posterior correlations to understand patterns of ESP}\label{sec:PostCorr}

%\setcounter{figure}{0}    

Most of the patterns in Figures \ref{ESplot10}, \ref{ESplot100}, and \ref{ESplot1000} in the next section can be explained by Figure \ref{corplot}, which contains the estimated posterior correlations between various functions of parameters estimated using the simulations from the Triple-Alt sampler for a time series with $T=100$. We omit a similar analysis for $T=10$ and $T=1000$. The state sampler consists of two steps --- a draw of $\theta$ given $V$ and $W$, and a draw of $(V,W)$ given $\theta$. From Section \ref{subsec:states} we have that conditional on $\theta$, $V$ and $W$ are independent in the posterior and each has an inverse gamma distribution that depends on the states only through the second parameter:
\begin{align*}
  b_V &\equiv \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2 &
  b_W &\equiv \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2.
\end{align*}
So we can view $(b_V,b_W)$ as the data augmentation instead of $\theta$ and thus the state sampler is
\begin{align*}
  [b_V, b_W|V^{(k)},W^{(k)}] \to [V^{(k+1)},W^{(k+1)}|b_V,b_W].
\end{align*}
Thus the dependence between $(V,W)$ and $(b_V,b_W)$ in the posterior will determine how much the state sampler moves in a given iteration and, in particular, it is possible that $V$ and $W$ have very different serial dependence from each other since we are drawing them jointly. When the dependence between $V$ and $b_V$ is high, the $(V,W)$ step will hardly move $V$ even if it drastically moves $W$ since $V$ and $W$ are independent. However, the $(b_V,b_W)$ step may move both elements a moderate amount since they both depend on $(V,W)$.

In Figure \ref{corplot} we see that the posterior correlation between $V$ and $b_V$ is high in magnitude and positive when $R^*>1$ while the posterior correlation between $V$ and $b_W$ is moderate to low and negative. When $R^*$ is large enough though, the posterior correlation between $V$ and $b_W$ evaporates. Similarly when $R^*<1$ the posterior correlation between $W$ and $b_W$ is high and positive and the posterior correlation between $W$ and $b_V$ is high and negative. Again as $R^*$ becomes large enough the correlation between $W$ and $b_V$ goes to zero. So when $R^*>1$, the draw of $(b_V, b_W)$ is unlikely to move $b_V$ much since $b_V$ is so highly correlated with $V$ and essentially uncorrelated with $b_W$, but $b_W$ is essentially uncorrelated with $W$ and negatively correlated with $V$ so $b_W$ is likely to move a fair amount. Furthermore the draw of $V$ is highly correlated with $b_V$ while the draw of $W$ is essentially independent of $b_W$ (and the draws of $V$ and $W$ are independent conditional on $b_V$ and $b_W$). Thus when $R^*>1$ we should expect high serial dependence for $V$ and low serial dependence for $W$, and so low ESP for $V$ and high ESP for $W$, which is exactly what we see in Figure \ref{ESplot100}. By similar reasoning when $R^*<1$, we should expect low serial dependence for $V$ and high serial dependece for $W$ and thus high ESP for $V$ and low ESP for $W$, which can also be seen in Figure \ref{ESplot100}.

For the SD sampler, things are a bit more complicated. The draw of $V|W,\gamma$ still depends on $b_V$ since it is the same inverse gamma draw as in the state sampler, but the draw of $W|V,\gamma$ now depends on $a_\gamma$ and $b_\gamma$ defined in Section \ref{subsec:SDs} as
\begin{align*}
  a_\gamma &\equiv \frac{1}{2V}\sum_{t=1}^T\left(\sum_{j=1}^t\gamma_j\right)^2&
  b_\gamma &\equiv \frac{1}{V}\sum_{t=1}^T(y_t-\gamma_0)\left(\sum_{j=1}^t\gamma_j\right).
\end{align*}
So the dependence between $V$ and $b_V$ determines how much the chain moves in the $V$ step, and the dependence between $W$ and $(a_\gamma , b_\gamma)$ determines how much it moves in the $W$ step. The dependence between $(V,W)$ and $\gamma$ determines how much the chain moves in the DA step, but we can view this step instead as a draw of $b_V$ in which case the dependence between $W$ and $b_V$ determines how much the chain moves in that step. So if any one of these steps has high dependence, we should expect every element of the chain, and $(V,W)$ in particular, to have high serial dependence in the chain. The SE sampler is analogous to the SD sampler except with $b_W$, $a_\psi$ and $b_\psi$ where
\begin{align*}
  a_\psi&=\frac{1}{2W}\sum_{t=1}^T(\mathcal{L}\psi_t)^2&
  b_\psi&=\frac{1}{W}\sum_{t=1}^T(\mathcal{L}\psi_t\mathcal{L}y_t).
\end{align*}

\begin{figure}[!h]
\centering
\includegraphics[width=0.24\textwidth]{corplot1}
\includegraphics[width=0.24\textwidth]{corplot2}
\includegraphics[width=0.24\textwidth]{corplot3}
\includegraphics[width=0.24\textwidth]{corplot4}
\includegraphics[width=0.24\textwidth]{corplot5}
\includegraphics[width=0.24\textwidth]{corplot6}
\includegraphics[width=0.24\textwidth]{corplot7}
\includegraphics[width=0.24\textwidth]{corplot8}
\caption{Posterior correlation between $V$ or $W$ and $b_V$, $b_W$, $a_\gamma$, $b_\gamma$, $a_\psi$ or $b_\psi$. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data with $T=100$.}
\label{corplot}
\end{figure}

In order to analyze the SD sampler, first suppose $R^*>1$. Then from Figure \ref{corplot} $b_V$ has high correlation with $V$ and low correlation with $W$, so the draw of $b_V$ should not move the chain much. Next, the draw of $V$ should again not move the chain much because of the high correlation between $V$ and $b_V$. Finally the draw of $W$ has a fair chance to move the chain because it has low correlation with both $a_\gamma$ and $b_\gamma$. But this has little impact on $b_V$ and thus the entire chain since $b_V$ is so highly correlated with $V$ but hardly correlated with $W$. So when $R^*>1$, we should expect high serial dependence and low ESP for $V$. We should also expect similar behavior for $W$ since the entire chain is hardly moving so $W$'s hyperparameters are hardly moving. This is roughly what we see in Figure \ref{ESplot100}, though this reasoning does not allow us to predict which of $V$ and $W$ will have lower ESP. When $R^*<1$ the posterior correlation in each of the steps is broken, though in the $W$ step the correlation between $W$ and both $a_\gamma$ and $b_\gamma$ becomes negative and somewhat high in magnitude. Here we should not expect less serial dependence in $V$ or $W$, but we should perhaps expect higher ESP's since negatively correlated draws decrease Monte Carlo standard error. Indeed, we see ESP's near one for both variances in Figure \ref{ESplot100}. The SE sampler is analogous to the SD sampler and a similar analysis applies --- the posterior correlations between $V$ or $W$ and $b_W$, $a_\psi$ or $b_\psi$ in Figure \ref{corplot} roughly predict the ESP of the SE sampler in Figure \ref{ESplot100}. When one or more of the correlations are high, ESPs for $V$ and $W$ are low while when all of the correlations are low, both ESPs are high. We omit a similar analysis of the wrongly-scaled samplers for brevity, but note that their behavior will allows us to predict the behavior of the CIS sampler.


\section{Plots for all values of $T$}\label{sec:plots}

\begin{figure}[!h]
\centering
\begin{subfigure}[b]{0.59\textwidth}%
\includegraphics[width=\textwidth]{basecisESplot10}
\caption{}
\label{fig:ESPa10}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altintESplotV10}
\caption{}
\label{fig:ESPb10}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altintESplotW10}
\caption{}
\label{fig:ESPc10}
\end{subfigure}
\caption{Effective sample proportion in the posterior sampler for a time series of length $T=10$, for $V$ and $W$ in the each sampler. Figure \ref{fig:ESPa10} contains ESP for $V$ and $W$ for the base samplers, Figure \ref{fig:ESPb10} contains ESP in the GIS and CIS samplers, and Figure \ref{fig:ESPc10} contains ESP in the Alt samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high.}
\label{ESplot10}
\end{figure}

\begin{figure}[!h]
\centering
\begin{subfigure}[b]{0.59\textwidth}%
\includegraphics[width=\textwidth]{basecisESplot100}
\caption{}
\label{fig:ESPa100}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altintESplotV100}
\caption{}
\label{fig:ESPb100}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altintESplotW100}
\caption{}
\label{fig:ESPc100}
\end{subfigure}
\caption{Effective sample proportion in the posterior sampler for a time series of length $T=100$, for $V$ and $W$ in the each sampler. Figure \ref{fig:ESPa100} contains ESP for $V$ and $W$ for the base samplers, Figure \ref{fig:ESPb100} contains ESP in the GIS and CIS samplers, and Figure \ref{fig:ESPc100} contains ESP in the Alt samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high.}
\label{ESplot100}
\end{figure}

\begin{figure}[!h]
\centering
\begin{subfigure}[b]{0.59\textwidth}%
\includegraphics[width=\textwidth]{basecisESplot1000}
\caption{}
\label{fig:ESPa1000}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altintESplotV1000}
\caption{}
\label{fig:ESPb1000}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altintESplotW1000}
\caption{}
\label{fig:ESPc1000}
\end{subfigure}
\caption{Effective sample proportion in the posterior sampler for a time series of length $T=1000$, for $V$ and $W$ in the each sampler. Figure \ref{fig:ESPa1000} contains ESP for $V$ and $W$ for the base samplers, Figure \ref{fig:ESPb1000} contains ESP in the GIS and CIS samplers, and Figure \ref{fig:ESPc1000} contains ESP in the Alt samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high.}
\label{ESplot1000}
\end{figure}

\begin{figure}[!h]
\centering
\begin{subfigure}[b]{0.59\textwidth}
\includegraphics[width=\textwidth]{basecistimeplot10}
\caption{}
\label{fig:timea10}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altgisVtimeplot10}
\caption{}
\label{fig:timeb10}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altgisWtimeplot10}
\caption{}
\label{fig:timec10}
\end{subfigure}
\caption{Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=10$ in each sampler. Figure \ref{fig:timea10} contains the base samplers, Figure \ref{fig:timeb10} contains the GIS and CIS samplers, while Figure \ref{fig:timec10} contains the Alt samplers. Log times larger than three $\log \mathrm{min}$ are rounded down to three for plotting purposes.}
\label{baseinttimeplot10}
\end{figure}

\begin{figure}[!h]
\centering
\begin{subfigure}[b]{0.59\textwidth}
\includegraphics[width=\textwidth]{basecistimeplot100}
\caption{}
\label{fig:timea100}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altgisVtimeplot100}
\caption{}
\label{fig:timeb100}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altgisWtimeplot100}
\caption{}
\label{fig:timec100}
\end{subfigure}
\caption{Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=100$ in each sampler. Figure \ref{fig:timea100} contains the base samplers, Figure \ref{fig:timeb100} contains the GIS and CIS samplers, while Figure \ref{fig:timec100} contains the Alt samplers. Log times larger than three $\log \mathrm{min}$ are rounded down to three for plotting purposes.}
\label{baseinttimeplot100}
\end{figure}

\begin{figure}[!h]
\centering
\begin{subfigure}[b]{0.59\textwidth}
\includegraphics[width=\textwidth]{basecistimeplot1000}
\caption{}
\label{fig:timea1000}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altgisVtimeplot1000}
\caption{}
\label{fig:timeb1000}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altgisWtimeplot1000}
\caption{}
\label{fig:timec1000}
\end{subfigure}
\caption{Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=1000$ in each sampler. Figure \ref{fig:timea1000} contains the base samplers, Figure \ref{fig:timeb1000} contains the GIS and CIS samplers, while Figure \ref{fig:timec1000} contains the Alt samplers. Log times larger than three $\log \mathrm{min}$ are rounded down to three for plotting purposes.}
\label{baseinttimeplot1000}
\end{figure}

\clearpage
\bibliographystyle{MWS_jasa}  % proper bibliography style for ASA
\bibliography{dlmasis}
\end{document}
