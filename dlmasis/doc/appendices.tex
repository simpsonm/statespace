\documentclass{article}
\usepackage{JASA_manu} %formats document like ASA wants
\usepackage{jasa_harvard} %formats citations like ASA wants
\usepackage{amssymb, amsmath, amsthm, graphics, graphicx, color, fullpage}
\usepackage{thmtools} %to format the Algorithm environment correctly
\usepackage{nameref, hyperref, cleveref} %for named references
\hypersetup{
  colorlinks   = true,    % Colours links instead of ugly boxes
  urlcolor     = blue,    % Colour for external hyperlinks
  linkcolor    = blue,    % Colour of internal links
  citecolor    = red      % Colour of citations
}
\usepackage{subcaption} % for subfigures

% define algorithm environment
\declaretheoremstyle[
notefont=\bfseries, notebraces={}{},
bodyfont=\normalfont\itshape,
headformat=\NAME:\NOTE
]{nopar}
\declaretheorem[style=nopar, name=Algorithm, 
refname={Algorithm,Algorithms},
Refname={Algorithm,Algoritms},
numbered=no]{alg*}

\newtheorem{thm}{Theorem}[subsection]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\vect}{vec}

\graphicspath{{plots/}}

% \newcommand{\blind}{0} % uncomment for compiling separately

\begin{document}

\setcounter{figure}{0}    

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\if0\blind
{
  \title{\bf Appendices for Interweaving Markov Chain Monte Carlo Strategies for Efficient
    Estimation of Dynamic Linear Models}
  \author{Matthew Simpson\\
    Department of Statistics, University of Missouri--Columbia\\~\\
    Jarad Niemi and Vivekananda Roy\\
    Department of Statistics, Iowa State University}
  \date{\today}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Appendices for Interweaving Markov Chain Monte Carlo Strategies for Efficient
    Estimation of Dynamic Linear Models}
\end{center}
  \medskip
} \fi

\spacingset{2}

\appendix
\renewcommand\thefigure{\thesection.\arabic{figure}}   

\noindent The following appendices are cited in the main article and included here:
\begin{description}
\item[A.] Derivation of marginal model for the DLM
\item[B.] Proof of lemma 1
\item[C.] Construction of the wrongly-scaled DA algorithms 
\item[D.] Derivations of relevant joint and full conditional distributions
\item[E.] MCFA for simulation smoothing 
\item[F.] Further augmentation for non-invertible $F_t$ 
\item[G.] Efficiently drawing from $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$ in the LLM 
\item[H.] Efficiently drawing from $p(W|V,\tilde{\gamma},y)$ and $p(V|W,\tilde{\psi},y)$ in the LLM 
\item[I.] Equivalence of CIS and SD-SE GIS in the DLM 
\item[J.] Partial CIS Algorithms in the DLM 
\item[K.] Using posterior correlations to understand patterns of ESP 
\item[L.] Computational time for each algorithm 
\item[M.] Additional plots for other values of $T$ 
\item[N.] Comparing GIS and Alt in very long time series. 
\end{description}

\section{Marginal model of the DLM}

The class of DLMs we consider is
\begin{align}
  y_t|\theta,V,W \stackrel{ind}{\sim} & N_k(F_t\theta_t,V) &
  \theta_t|\theta_{0:t-1},V,W  \sim & N_p(G_t\theta_{t-1},W) \label{dlmbotheqs}
\end{align}
for $t=1,2,\cdots T$ where $V$ and $W$ are unknown covariance matrices. Define $v_t=y_t - F_t\theta_t$ and $w_t = \theta_t - G_t\theta_{t-1}$. Then we can rewrite the model by recursive substitution:
\begin{align*}
  y_t &= v_t + F_t\left(w_t + G_tw_{t-1} + G_tG_{t-1}w_{t-2} + ... + G_tG_{t-1}\cdots G_{2}w_1 + G_tG_{t-1}\cdots G_1\theta_0\right).
\end{align*}
Then conditional on $\phi=(V,W)$ each $y_t$ is a linear combination of normal random variables. After marginalizing out $\theta$, $y=(y_1',y_2',\dots,y_T')$ has a normal distribution such that $\mathrm{E}[y_t|\phi] =  F_tH_tm_0$,
\begin{align*}
  \mathrm{Var}[y_t|\phi] =  V + F_t(K_tWK_t' + H_tC_0H_t')F_t' ,\quad \mathrm{and} \quad
  \mathrm{Cov}[y_s,y_t|\phi] = F_s(K_sWK_t' + H_sC_0H_t')F_t',
\end{align*}
where $H_t = G_tG_{t-1}\cdots G_1$ and $K_t = I_p + G_t + G_tG_{t-1} + \cdots + G_tG_{t-1}\cdots G_2$. Next define $D_t = F_tG_tG_{t-1}\cdots G_1$. Then let $\tilde{V}=I_T\otimes V$ and $D$ be block diagonal with elements $D_1,\ldots,D_T$, 
%\begin{align*}
%\tilde{V}_{Tk\times Tk} & = \begin{bmatrix}
%V & 0 & \cdots & 0\\
%0 & V & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots\\
%0 & 0 & \cdots & V
%\end{bmatrix}, &
%D_{Tp\times Tk} &= \begin{bmatrix}
%D_1 & 0 & \cdots & 0\\
%0 & D_2 & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots\\
%0 & 0 & \cdots & D_T
%\end{bmatrix},
%\end{align*}
\begin{align*}
\tilde{W}_{Tk\times Tk} &= \begin{bmatrix} K_1'F_1' & K_2'F_2' & \cdots K_T'F_T' \end{bmatrix}' W \begin{bmatrix} K_1'F_1' & K_2'F_2' & \cdots K_T'F_T' \end{bmatrix}, &\\
\tilde{C}_{Tk\times Tk} &= \begin{bmatrix} H_1'F_1' & H_2'F_2' & \cdots H_T'F_T' \end{bmatrix}' C_0 \begin{bmatrix} H_1'F_1' & H_2'F_2' & \cdots H_T'F_T' \end{bmatrix},&
\end{align*}
and $\tilde{m}_{Tp\times 1} = (m_0', m_0', \cdots m_0')'$. Now we have the data model for $y$ without any data augmentation:
\begin{align}
  y|V,W \stackrel{ind}{\sim} N_{Tk}(D\tilde{m}, \tilde{V} + \tilde{W} + \tilde{C}). \label{margmodel}
\end{align}


 
\section{Proof of lemma 1}

First the normality assumption implies
\begin{align*}
  y|\eta,\phi &\sim N(D\tilde{m} + \Omega_{y,\eta}'\Omega_\eta^{-1}(\eta - \alpha_\eta), \tilde{V} + \tilde{W} + \tilde{C}- \Omega_{y,\eta}'\Omega_{\eta}^{-1}\Omega_{y,\eta})\\
  \eta|\phi &\sim N(\alpha_\eta, \Omega_\eta).
\end{align*}
Now for $\eta$ to be a sufficient augmentation we need $D\tilde{m} + \Omega_{y,\eta}'\Omega_\eta^{-1}(\eta - \alpha_\eta)$ and $\tilde{V} + \tilde{W} + \tilde{C} - \Omega_{y,\eta}'\Omega_{\eta}^{-1}\Omega_{y,\eta}$ to be functionally independent of $\phi$. This requires that
\begin{align*}
  D\tilde{m} - \Omega_{y,\eta}'\Omega_\eta^{-1}\alpha_\eta + \Omega_{y,\eta}'\Omega_\eta^{-1}\eta  = b + A\eta
\end{align*}
where $A=\Omega_{y,\eta}'\Omega_\eta^{-1}$ and $b=D\tilde{m} - A\alpha_\eta$ must both be free of $\phi$. As a result $A\alpha_\eta$ is also free of $\phi$ and thus so is $\alpha_{\eta}$.

Then using the second equation, we now require $\Sigma$ free of $\phi$ where $\Sigma = \tilde{V} + \tilde{W} + \tilde{C} - A\Omega_{\eta}A'$. This ensures that $\Omega_{\eta,y}$ is not the zero matrix since $\tilde{V} + \tilde{W} + \tilde{C}$ is not free of $\phi$. Rearranging we have $A\Omega_{\eta}A' = \tilde{V} + \tilde{W} + \tilde{C} - \Sigma$. Consider $\tilde{\eta}=A\eta$, which is also a sufficient augmentation since it is just a linear transformation by a constant matrix. Then we have
\begin{align*}
y|\tilde{\eta},\phi & \sim N(b + A\eta, \Sigma)\\
\tilde{\eta}|\phi & \sim N(A\alpha_\eta, A\Omega_\eta A')
\intertext{in other words}
y|\tilde{\eta},\phi & \sim N(b + \tilde{\eta}, \Sigma)\\
\tilde{\eta}|\phi & \sim N(A\alpha_{\eta}, \tilde{V} + \tilde{W} + \tilde{C} - \Sigma).
\end{align*}
Thus the posterior density of $\phi$ given $\tilde{\eta}$ can be written as
\begin{align*}
  p(\phi|\tilde{\eta}, y) &\propto p(y|\tilde{\eta},\phi)p(\tilde{\eta}|\phi)p(\phi) \propto p(\tilde{\eta}|\phi)p(\phi) \\
&\propto p(\phi)|\tilde{V} + \tilde{W} + \tilde{C} - \Sigma|^{-1/2}\exp\left[-\frac{1}{2}(\tilde{\eta} - A\alpha_{\eta})'(\tilde{V} + \tilde{W} + \tilde{C} - \Sigma)^{-1}(\tilde{\eta} - A\alpha_{\eta})\right].
\end{align*}
Now given that $A'A$ is invertible and the properties of multivariate normal distributions, the density of $p(\phi|\eta,y)$ follows from $\eta=(A'A)^{-1}A'\tilde{\eta}$.

\section{Construction of the wrongly-scaled DA algorithms}\label{sec:W-DA}

The wrongly-scaled DA algorithms are close analogues to their correctly scaled cousins. Starting with the {\it wrongly-scaled disturbance sampler} (Algorithm \nameref{alg:DLMwdist}), the simulation smoothing step to draw from $p(\tilde{\gamma}|V,W,y)$ is similar to that of the scaled disturbance sampler --- the density is Gaussian, but the precision matrix is not tridiagonal, so we draw $\theta$ using the MCFA and transform to obtain a draw of $\tilde{\gamma}$. The density of $V,W|\tilde{\gamma},y$ is too complicated to draw from directly, as was the case when we used the scaled disturbances. In this case, the full conditional distribution of $W$ is the same as its distribution when we condition on the states while the density of $V|\tilde{\gamma},y$ is once again difficult to draw from. The density of $V|W,\tilde{\gamma},y$ is easier to work with, at least in the local level model example in Section 6. 
\begin{alg*}[WSD]Wrongly-Scaled Disturbance Sampler\label{alg:DLMwdist}
\begin{enumerate}
\item Use MCFA to draw $\theta \sim p(\theta|V,W,y)$.
\item Transform $\theta$ to $\tilde{\gamma}$.
\item Draw $V \sim p(V|W,\tilde{\gamma},y)$.
\item Draw $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
\end{enumerate}
\end{alg*}\noindent
Now the third step is difficult and we demonstrate how to accomplish it in the local level model in Appendix F. We could switch the order in which $V$ and $W$ are drawn in this algorithm so that we can draw $W$ before transforming $\theta$ to $\tilde{\gamma}$. This would make each iteration slightly cheaper and probably would not affect the mixing and convergence properties of the algorithm, however we are more interested in comparing the mixing and convergence properties of the various samplers, so we always sample $V$ before $W$ when we cannot sample them jointly.

The {\it wrongly-scaled error sampler} (Algorithm \nameref{alg:DLMwerror}) is closely related to both the wrongly-scaled disturbance sampler and the scaled error sampler. The density of $\tilde{\psi}|V,W,y$ is Gaussian with a tridiagonal precision matrix, so the simulation smoothing step can be accomplished using the MCFA. The density $p(V,W|\tilde{\psi},y)$ is from the same class as $p(W,V|\tilde{\gamma},y)$ so that $V$ and $W$ essentially switch places when we condition on $\tilde{\psi}$ instead of $\tilde{\gamma}$. In particular, $V|W,\tilde{\psi},y$ has an inverse Wishart density and the density of $W|V,\tilde{\psi},y$ is from the same class as that of $V|W,\tilde{\gamma},y$.
\begin{alg*}[WSE]Wrongly-Scaled Error Sampler\label{alg:DLMwerror}
\begin{enumerate}
\item Use MCFA to draw $\tilde{\psi} \sim p(\theta|V,W,y)$.
\item Draw $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$.
\item Draw $W \sim p(W|V,\tilde{\psi},y)$
\end{enumerate}
\end{alg*}


The constructions of Algorithms \nameref{alg:DLMwdist} and \nameref{alg:DLMwerror} in the local level model example from Section 6 require $p(W|V,\tilde{\psi},y)$ and $p(V|W,\tilde{\gamma},y)$ respectively. Both densities have the form $p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b/\sqrt{x} -c/x\right]$, which is closely related to the difficult density from the correctly scaled samplers.  For $p(V|W,\tilde{\gamma},y)$ we show in Appendix C that $\alpha=\alpha_V$, $a = a_{\tilde{\gamma}}\equiv\frac{1}{2W}\sum_{t=1}^T\tilde{\gamma}_t^2$, $b =b_{\tilde{\gamma}}\equiv \sum_{t=1}^T(y_t - \tilde{\gamma}_0)\sum_{s=1}^t\tilde{\gamma}_s$, and $c =c_{\tilde{\gamma}}\equiv \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\gamma}_0)^2$ while for $p(W|V,\tilde{\psi},y)$ we show that $\alpha=\alpha_W$,   $a =a_{\tilde{\psi}}\equiv \frac{1}{2V}\sum_{t=1}^T\tilde{\psi}_t^2$,  $b =b_{\tilde{\psi}}\equiv \sum_{t=1}^T\mathcal{L}\tilde{y}_t\mathcal{L}\tilde{\psi}_t$, and $c =c_{\tilde{\psi}}\equiv \beta_W + \frac{1}{2}\sum_{t=1}^T\mathcal{L}\tilde{y}_t^2$. This density is harder to sample from because adaptive rejection sampling does not work very well, so we construct a rejection sampler on the log scale using a $t$ approximation in Appendix G.


\section{Full conditional distributions in the general DLM for various DAs}\label{sec:DLMfullcond}

The class of DLMs we consider is defined as follows:
\begin{align}
y_t &= F_t\theta_t + v_t && v_t \stackrel{ind}{\sim} N_k(0,V) && (\mbox{observation equation}) \label{dlmtdobseq}\\
 \theta_t &= G_t\theta_{t-1} + w_t && w_t \stackrel{ind}{\sim} N_p(0,W) && (\mbox{system equation}) \label{dlmtdsyseq}
\end{align}
for $t=1,2,\cdots T$ with the priors $\theta_0 \sim N_p(m_0, C_0)$, $V \sim IW(\Lambda_V, \lambda_V)$ and $W \sim IW(\Lambda_W, \lambda_W)$ with $(\theta_0,V,W)$ mutually independent. Then the full joint distribution of $(V,W,\theta,y)$ is
\begin{align}
  p(&V,W,\theta,y) \propto \exp\left[-\frac{1}{2}(\theta_0-m_0)'C_0^{-1}(\theta_0-m_0)\right] \nonumber\\
  &\times   |V|^{-(\lambda_V + k + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - F_t\theta_t)'V^{-1}(y_t - F_t\theta_t)\right] \nonumber\\
   & \times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t-G_t\theta_{t-1})'W^{-1}(\theta_t-G_t\theta_{t-1})\right]\label{dlmjoint}
 \end{align}
where $\tr(.)$ is the matrix trace operator.

In the following subsections, we provide derivations of the full conditional distributions for when using states, scaled disturbances or scaled errors as the data augmentation. 

\subsection{States}\label{subsec:states}

With the usual DA, the full conditional distributions can be derived from equation \eqref{dlmjoint}. First, the full conditional distribution of $\theta$ is as follows:
\begin{align*}
p(\theta&|V,W,y) \propto p(V,W,\theta,y) \propto \exp\left[-\frac{1}{2}(\theta_0-m_0)'C_0^{-1}(\theta_0-m_0)\right] \\
  &\times \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - F_t\theta_t)'V^{-1}(y_t - F_t\theta_t)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t-G_t\theta_{t-1})'W^{-1}(\theta_t-G_t\theta_{t-1})\right].
\end{align*}
It turns out that this density is Gaussian. In Section \ref{sec:MCFA}, we show how to use the mixed Cholesky factorization algorithm (MCFA) in order to efficiently determine and draw from this distribution.

The full conditional of $(V,W)$ is:
\begin{align*}
  p(V,W&|\theta,y) \propto p(V,W,\theta,y) \propto  |V|^{-(\lambda_V + k + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - F_t\theta_t)'V^{-1}(y_t - F_t\theta_t)\right]\\
   & \times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t-G_t\theta_{t-1})'W^{-1}(\theta_t-G_t\theta_{t-1})\right]\\
&\propto |V|^{-(\lambda_V + k + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\left(\Lambda_V +  \sum_{t=1}^T(y_t - F_t\theta_t)(y_t - F_t\theta_t)'\right) V^{-1}\right)\right]\\
 &\times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\left(\Lambda_W  + \sum_{t=1}^T(\theta_t-G_t\theta_{t-1})(\theta_t-G_t\theta_{t-1})'\right)W^{-1}\right)\right].
 \end{align*}
In other words, $V$ and $W$ are conditionally independent given $y$ and $\theta$ with
\begin{align*}
  V|\theta,y &\sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right), &
  W|\theta,y &\sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right) 
\end{align*}
where $v_t = y_t - F_t\theta_t$ and $w_t = \theta_t - G_t\theta_{t-1}$.

In the local level model, the priors on $V$ and $W$ become $V\sim IG(\alpha_V,\beta_V)$ and $W\sim IG(\alpha_W,\beta_W)$. The full conditionals then become
\begin{align*}
V|\theta,y & \sim IG\left(\alpha_V + T/2, \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2\right), &
W|\theta,y & \sim IG\left(\alpha_W + T/2, \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2\right).
\end{align*}

\subsection{Scaled disturbances}\label{subsec:SDs}

Let $L_W$ denote the Cholesky decomposition of $W$, i.e. the lower triangle matrix $L_W$ such that $L_WL_W' =W$. Then the scaled disturbances are $\gamma=\gamma_{0:T}=(\gamma_0',\gamma_1',\cdots,\gamma_T')'$ defined by $\gamma_0=\theta_0$ and $\gamma_t = L_W^{-1}(\theta_t-G_t\theta_{t-1})$ for $t=1,2,\cdots,T$. The reverse transformation is defined recursively by $\theta_0=\gamma_0$ and $\theta_t=L_W\gamma_t + G_t\theta_{t-1}$ for $t=1,2,\cdots,T$. Then the Jacobian is block lower triangular with the identity matrix and $T$ copies of $L_W$ along the diagonal blocks, so $|J| = |L_W|^T=|W|^{T/2}$. From equation \eqref{dlmjoint} we can write the full joint distribution of $(V,W,\gamma,y)$ as
 \begin{align}
  p(&V,W,\gamma,y) \propto \exp\left[-\frac{1}{2}(\gamma_0-m_0)'C_0^{-1}(\gamma_0-m_0)\right] \exp\left[-\frac{1}{2}\gamma_t'\gamma_t\right] \nonumber\\
  &\times |W|^{-(\lambda_W + p + 2)/2} |V|^{-(\lambda_V + k + T + 2)/2} \exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]  \nonumber\\
  &\times \exp\left[-\frac{1}{2}\left(\tr\left(\Lambda_VV^{-1}\right) + \sum_{t=1}^T\left[y_t-F_t\theta_t(\gamma,W)\right]'V^{-1}\left[y_t-F_t\theta_t(\gamma,W)\right]\right)\right]\label{dlmdistjoint}. 
 \end{align}
where $\theta_t(\gamma,W)$ denotes the recursive back transformation defined by the scaled disturbances. The full conditional distribution of $\gamma$ is then
\begin{align*}
  p(\gamma&|V,W,y) \propto p(V,W,\gamma,y) \propto \exp\left[-\frac{1}{2}(\gamma_0-m_0)'C_0^{-1}(\gamma_0-m_0)\right] \exp\left[-\frac{1}{2}\gamma_t'\gamma_t\right]\\
&\times \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\left[y_t-F_t\theta_t(\gamma,W)\right]'V^{-1}\left[y_t-F_t\theta_t(\gamma,W)\right]\right)\right]. 
\end{align*}
This density is Gaussian, but difficult to draw from. We use the MCFA to draw from $\theta|V,W,y$ instead, then transform from $\theta$ to $\gamma$ using the definition of $\gamma$.

Under this parameterization, the full conditional distribution of $(V,W)$ is
 \begin{align*}
  p(&V,W,|\gamma,y) \propto  p(V,W,\gamma,y) |W|^{-(\lambda_W + p + 2)/2} |V|^{-(\lambda_V + k + T + 2)/2} \exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]  \nonumber\\
  &\times \exp\left[-\frac{1}{2}\left(\tr\left(\Lambda_VV^{-1}\right) + \sum_{t=1}^T\left[y_t-F_t\theta_t(\gamma,W)\right]'V^{-1}\left[y_t-F_t\theta_t(\gamma,W)\right]\right)\right]. 
 \end{align*}
The back transformation from $\theta$ to $\gamma$ sets $\theta_0=\gamma_0$ and for $t=1,2,\cdots,T$
\begin{align*}
\theta_t &= L_W\gamma_t + G_t\theta_{t-1}\\
&= L_W\gamma_t + \sum_{s=0}^{t-2}G_tG_{t-1}\hdots G_{t-s}L_W\gamma_{t-s-1} + G_tG_{t-1}\hdots G_1\gamma_0\\
&= \sum_{s=0}^{t-1}\tilde{G}_{s,t}L_W\gamma_{t-s} + \tilde{G}_{t,t}\gamma_0
\end{align*}
where $\tilde{G}_{s,t} = G_tG_{t-1}\cdots G_{t-s + 1}$ for $s >0$ and $\tilde{G}_{0,t}=I_p$, the $p\times p$ identity matrix.. Then we can rewrite the conditional distribution of $(V,W)$ as
 \begin{align*}
  p(&V,W,|\gamma,y) \propto  p(V,W,\gamma,y) \propto |W|^{-(\lambda_W + p + 2)/2} |V|^{-(\lambda_V + k + T + 2)/2} \exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right] \exp\left[-\frac{1}{2}\left(\tr\left(\Lambda_VV^{-1}\right)\right)\right]  \nonumber\\
  &\times  \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\left[y_t-F_t\sum_{s=0}^{t}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]'V^{-1}\left[y_t-F_t\sum_{s=0}^{t-1}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]\right)\right]. 
 \end{align*}
This density is fairly complicated, so we resort to the full conditionals of $V$ and $W$ separately. The full conditional of $V$ is familiar:
 \begin{align*}
  p(&V|W,\gamma,y) \propto  p(V,W|\gamma,y) \propto |V|^{-(\lambda_V + k + T + 2)/2} \times \exp\left[-\frac{1}{2}\left(\tr
\left[\Lambda_V + \sum_{t=1}^Tv_tv_t'\right]V^{-1}\right)\right]
 \end{align*}
where $v_t = y_t - F_t\sum_{s=0}^{t}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0 = y_t - F_t\theta_t$. This implies that
\begin{align*}
  V|W,\gamma,y &\sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)
\end{align*}
which is the same distribution as for $V|\theta,y$. In the local level model this reduces to
\begin{align*}
  V|W,\gamma,y \sim IG\left(\alpha_V + T/2, \beta_V + \sum_{t=1}^T(y_t - \theta_t(\gamma))^2/2\right)
\end{align*}
which is again the same density if we conditioned on $\theta$. 

The full conditional density of $W$ is more complicated:
 \begin{align*}
  p(W&|V,\gamma,y) \propto  p(V,W,\gamma,y) \propto |W|^{-(\lambda_W + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\\
&\times  \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\left[y_t-F_t\sum_{s=0}^{t}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]'V^{-1}\left[y_t-F_t\sum_{s=0}^{t-1}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]\right)\right]. 
 \end{align*}
In the local level model, the density is even simpler: 
\begin{align*}
  p(W&|V,\gamma,y) \propto W^{-\alpha_W   - 1}\exp\left[-\frac{1}{W}\beta_W  \right]  \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\left[y_t-\sum_{s=0}^{t}\gamma_{t-s}\sqrt{W}\right]'V^{-1}\left[y_t-\sum_{s=0}^{t-1}\gamma_{t-s}\sqrt{W}\right]\right)\right]\\
&\propto W^{-\alpha_W - 1}\exp\left[-a_\gamma W + b_\gamma \sqrt{W} -\frac{\beta_W}{W}\right]. 
\end{align*}
where $a_\gamma =\sum_{t=1}^T(\sum_{s=1}^t\gamma_j)^2/2V$ and $b_\gamma =\sum_{t=1}^T(y_t-\gamma_0)(\sum_{s=1}^t\gamma_j)/V$. In Section \ref{sec:scaledraw} we show how to efficiently obtain a random draw from this density.

\subsection{Scaled errors}\label{subsec:SEs}
Let $L_V$ denote the Cholesky decomposition of $V$, that is $L_VL_V'=V$, then we can define the scaled errors as $\psi_t = L_V^{-1}(y_t - F_t\theta_t)$ for $t=1,2,\cdots,T$ and $\psi_0 = \theta_0$. Here we assume that $k=p$ and that $F_t$ is invertible for all $t$. Then the back transformation is $\theta_t = F_t^{-1}(y_t - L_V\psi_t)$ for $t=1,2,\cdots,T$ and $\theta_0=\psi_0$. The Jacobian of this transformation is block diagonal with a single copy of the identity matrix along with the $F_t^{-1}L_V$'s along the diagonal, so $|J|=(\prod_{t=1}^T|F_t|^{-1})|V|^{T/2}$. Then from equation \eqref{dlmjoint} we can write the joint distribution of $(V, W, \psi, y)$ as
\begin{align}
    p(&V,W,\psi,y) \propto \exp\left[-\frac{1}{2}(\psi_0-m_0)'C_0^{-1}(\psi_0-m_0)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\psi_t'\psi_t\right] \nonumber\\
  &\times |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right]  \times |W|^{-(\lambda_W + p + T + 2)/2} \nonumber\\
   & \exp\left[-\frac{1}{2}\left(\tr\left(\Lambda_WW^{-1}\right) + \sum_{t=1}^T(y_t - \mu_t)'(F_tWF_t')^{-1}(y_t-\mu_t)\right)\right]\label{dlmerrorjoint}
\end{align}
where we define $\mu_1 = L_V\psi_1 + F_1G_1\psi_0$ and for $t=2,3,\cdots,T$, $\mu_t =L_V\psi_t + F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{V}\psi_{t-1})$. The $|F_t|^{-1}$'s have been absorbed into the normalizing constant, but if they depended on some unknown parameter then we could not do this and as a result would have to take them into account in the Gibbs step or steps for the model parameters.

The full conditional distribution of $\psi$ is
\begin{align*}
    p(&V,W,\psi,y) \propto \exp\left[-\frac{1}{2}(\psi_0-m_0)'C_0^{-1}(\psi_0-m_0)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\psi_t'\psi_t\right] \nonumber\\
   & \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T(y_t - \mu_t)'(F_tWF_t')^{-1}(y_t-\mu_t)\right)\right]
\end{align*}
where note that $\mu_t$ depends on $\psi$. This density is Gaussian and like with $\gamma$, we can use the MCFA from Section \ref{sec:MCFA} to draw from the full conditional of $\theta$ and then transform from $\theta$ to $\psi$. However it turns out the precision matrix of $\psi$'s full conditional distribution has the necessary block tridiagonal structure, so we use the MCFA directly on $\psi$. 

The full conditional distribution of $(V,W)$ is complicated, like the case of the scaled disturbances, so we find the full conditionals of $V$ and $W$ separately instead. The full conditional of $W$ is 
\begin{align*}
 p(&W|V,\psi,y) \propto |W|^{-(\lambda_W + p + T + 2)/2} \exp\left[-\frac{1}{2}\left(\tr\left(\left[\Lambda_W + \sum_{t=1}^TF_t^{-1}(y_t - \mu_t)(y_t-\mu_t)'(F_t^{-1})'\right]W^{-1}\right)\right)\right],
\end{align*}
in other words
\begin{align*}
  W|V,\psi,y &\sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right) 
\end{align*}
where $w_t = F_t^{-1}(y_t - \mu_t) = \theta_t - G_t\theta_{t-1}$. In the local level model, this becomes
\begin{align*}
W|V,\psi,y &\sim IG\left(\alpha_W + T/2, \beta_W + \sum_{t=1}^T(\theta_t(\psi) - \theta_{t-1}(\psi))^2/2\right).
\end{align*}

The full conditional distribution of $V$ is more complicated:
\begin{align*}
 p(V&|W,\psi,y) \propto p(V,W,\psi,y) \propto |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1} + \sum_{t=1}^T(y_t - \mu_t)'(F_tWF_t')^{-1}(y_t-\mu_t)\right)\right]
\end{align*}
with $\mu_t$ a function of $V$, defined above. In the local level model with an $IG(\alpha_V,\beta_V)$ prior on $V$, this density is simpler:
\begin{align*}
 p(V&|W,\psi,y) \propto V^{-\alpha_V - 1}\exp\left[-\frac{\beta_V}{V} + \frac{1}{W}\sum_{t=1}^T(y_t - \mu_t)'(y_t-\mu_t)\right]
\end{align*}
where $\mu_1 = \sqrt{V}\psi_1 + \psi_0$ and for $t=2,3,\cdots,T$, $\mu_t = \sqrt{V}(\psi_t - \psi_{t-1}) + y_{t-1}$. Thus
\begin{align*}
 p(V|W,\psi,y) \propto V^{-\alpha_V - 1}\exp\left[ -a_{\psi}V + b_{\psi}\sqrt{V} -\frac{\beta_V}{V}\right] 
\end{align*}
where $a_{\psi}=\sum_{t=1}^T(\mathcal{L}\psi_t)^2/2W$ and $b_{\psi}=\sum_{t=1}^T(\mathcal{L}\psi_t\mathcal{L}y_t)/W$, and we define $\mathcal{L}y_t=y_t-y_{t-1}$ for $t=2,3,\cdots,T$, $\mathcal{L}y_1=y_1 - \psi_0$, $\mathcal{L}\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$ and $\mathcal{L}\psi_1=\psi_1-0$. In other words, the form of $p(V|W,\psi,y)$ is the same as $p(W|V,\gamma,y)$. The general form of these two densities is $p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b\sqrt{x} -c/x\right]$. In Section \ref{sec:scaledraw} we show how to efficiently sample from this distribution.

\subsection{The wrongly-scaled disturbances}\label{subsec:WSDs}

The wrongly-scaled disturbances are defined as  $\tilde{\gamma}=\tilde{\gamma}_{0:T}=(\tilde{\gamma}_0',\tilde{\gamma}_1',\cdots,\tilde{\gamma}_T')'$. The wrongly-scaled disturbances are related to the scaled disturbances by $\tilde{\gamma}_t = L_V^{-1}L_W\gamma_t$ for $t=1,2,\cdots,T$ and $\tilde{\gamma_0}=\gamma_0$. The reverse transformation is $\gamma_t = L_W^{-1}L_V\tilde{\gamma}_t$ and the Jacobian is block diagonal with a copy of the identity matrix and $T$ copies of $L_W^{-1}L_V$ along the diagonal. Thus $|J|=|L_W|^{-T}|L_V|^T=|W|^{-T/2}|V|^{T/2}$. Then from equation \eqref{dlmdistjoint} we can write the joint distribution of $(V,W,\tilde{\gamma},y)$ as
 \begin{align}
  p(&V,W,\tilde{\gamma},y) \propto \exp\left[-\frac{1}{2}(\tilde{\gamma}_0-m_0)'C_0^{-1}(\tilde{\gamma}_0-m_0)\right] |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right]  \nonumber\\
  &\times  \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)'V^{-1}\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)\right]\nonumber\\
   & \times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\gamma}_t'(L_V^{-1}W(L_V^{-1})')^{-1}\tilde{\gamma}_t\right]\label{dlmWSDjoint}
 \end{align}
where $\theta_t(\tilde{\gamma},L_V)$ denotes the transformation from $\tilde{\gamma}$ to $\theta$ defined by the wrongly-scaled disturbances. 

Now from equation \eqref{dlmWSDjoint}, we can write the full conditional density of $\tilde{\gamma}$ as 
\begin{align*}
p(\tilde{\gamma}|V,W,y) \propto & \exp\left[-\frac{1}{2}(\tilde{\gamma}_0-m_0)'C_0^{-1}(\tilde{\gamma}_0-m_0)\right]  \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\gamma}_t'(L_V^{-1}W(L_V^{-1})')^{-1}\tilde{\gamma}_t\right]\\
   &\times  \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)'V^{-1}\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)\right].
\end{align*}
This density is Gaussian but difficult to draw from, so we use the MCFA to draw $\theta|V,W,y$ instead, then transform from $\theta$ to $\tilde{\gamma}$.

Then full conditional density of $(V,W)$ is complicated, but their separate full conditionals are easier to work with. The full conditional density of $W$ is
\begin{align*}
  p(W|V,\tilde{\gamma},y)\propto & |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\left[\Lambda_W + \sum_{t=1}^TL_V\tilde{\gamma}_t\tilde{\gamma}_t'L_V'\right]W^{-1} \right)\right],
\end{align*}
i.e. 
\begin{align*}
W|V,\tilde{\gamma},y \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t', \lambda_W + T\right)
\end{align*}
where $w_t = L_V\tilde{\gamma}_t = \theta_t - G_t\theta_{t-1}$. In the local level model, this density becomes 
\begin{align*}
W|V,\tilde{\gamma},y \sim IG\left(\alpha_W + T/2, \beta_W + \sum_{t=1}^T(\theta_t(\tilde{\gamma}) - \theta_{t-1}(\tilde{\gamma}))^2/2\right).
\end{align*}
The full conditional density of $V$ is more complicated, from equation \eqref{dlmWSDjoint}:
\begin{align*}
  p(V|W,\tilde{\gamma},y) \propto &  |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\gamma}_t'(L_V^{-1}W(L_V^{-1})')^{-1}\tilde{\gamma}_t\right]\\
  &\times  \exp\left[-\frac{1}{2}\sum_{t=1}^T\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)'V^{-1}\left(y_t - F_t\theta_t(\tilde{\gamma},L_V)\right)\right].
 \end{align*}
In the local level model with an $IG(\alpha_V, \beta_V)$ prior on $V$, this density becomes simpler. Since in that case $\theta_t = \sqrt{V}\sum_{s=1}^t\tilde{\gamma}_s + \tilde{\gamma}_0$, we have
\begin{align*}
p(V|W,\tilde{\gamma},y)\propto V^{-\alpha_V-1}\exp\left[ -a_{\tilde{\gamma}}V + b_{\tilde{\gamma}}/\sqrt{V} -c_{\tilde{\gamma}}/V\right]
\end{align*} 
where $a_{\tilde{\gamma}} = \frac{1}{2W}\sum_{t=1}^T\tilde{\gamma}_t^2$, $b_{\tilde{\gamma}} = \sum_{t=1}^T(y_t - \tilde{\gamma}_0)\sum_{s=1}^t\tilde{\gamma}_s$, and $c_{\tilde{\gamma}} = \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\gamma}_0)^2$. We show in Section \ref{sec:wscale} how to efficiently obtain a random draw from this density.

\subsection{The wrongly-scaled errors}\label{subsec:WSEs}

The wrongly-scaled errors are denoted by $\tilde{\psi}=\tilde{\psi}_{0:T}=(\tilde{\psi}_0',\tilde{\psi}_1',\cdots,\tilde{\psi}_T')'$. They  are related to the scaled errors by $\tilde{\psi}_t=L_W^{-1}L_V\psi_t$ for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\psi_0$. Then $\psi_t = L_V^{-1}L_W\tilde{\psi}_t$ and the Jacobian is block diagonal with a copy of the identical matrix and $T$ copies of $L_V^{-1}L_W$ along the diagonal. So $|J|=|V|^{-T/2}|W|^{T/2}$ and from equation \eqref{dlmerrorjoint} we can write the joint distribution of $(V, W, \tilde{\psi}, y)$ as
\begin{align}
    p(&V,W,\tilde{\psi},y) \propto \exp\left[-\frac{1}{2}(\tilde{\psi}_0-m_0)'C_0^{-1}(\tilde{\psi}_0-m_0)\right] \nonumber\\
   &\times |V|^{-(\lambda_V + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\psi}_t'(L_W^{-1}V(L_W^{-1})')^{-1}\tilde{\psi}_t\right] \nonumber\\
    & \times |W|^{-(\lambda_W + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\mu}_t)'(F_tWF_t')^{-1}(y_t-\tilde{\mu}_t)\right]\label{dlmWSEjoint}
 \end{align}
where we define $\tilde{\mu}_1 = L_W\tilde{\psi}_1 - F_1G_1\tilde{\psi_0}$ and for $t=2,3,\cdots,T$ $\tilde{\mu}_t =L_W\tilde{\psi}_t - F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{W}\tilde{\psi}_{t-1})$.

From equation \eqref{dlmWSEjoint} the full conditional distribution of $\tilde{\psi}$ is
\begin{align*}
    p(\tilde{\psi}|V,W,y) \propto & \exp\left[-\frac{1}{2}(\tilde{\psi}_0-m_0)'C_0^{-1}(\tilde{\psi}_0-m_0)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\psi}_t'(L_W^{-1}V(L_W^{-1})')^{-1}\tilde{\psi}_t\right]\\
&\times  \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\mu}_t)'(F_tWF_t')^{-1}(y_t-\tilde{\mu}_t)\right].
\end{align*}
This density is again Gaussian and it can be shown that the precision matrix is tridiagonal, so the MCFA can be directly applied. The full conditional density of $V$ is the familiar inverse Wishart:
\begin{align*}
    p(V|W,\tilde{\psi},y) \propto& |V|^{-(\lambda_V + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\psi}_t'(L_W^{-1}V(L_W^{-1})')^{-1}\tilde{\psi}_t\right]. 
 \end{align*}
So $V|W,\tilde{\psi},y \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t', \lambda_V + T\right)$ where $v_t = L_W\tilde{\psi}_t = y_t - F_t\theta_t$. In the local level model, this becomes
\begin{align*}
V|W,\tilde{\psi},y &\sim IG\left(\alpha_V + T/2, \beta_V + \sum_{t=1}^T(y_t - \theta_t(\tilde{\psi}))^2/2\right).
\end{align*}

The full conditional density of $W$ is more complicated, but has the same form as the full conditional density of $V$ given $\tilde{\gamma}$:
\begin{align*}
  p(W|V,\tilde{\psi},y) \propto& |W|^{-(\lambda_W + p + 2)/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\psi}_t'(L_W^{-1}V(L_W^{-1})')^{-1}\tilde{\psi}_t\right] \\
  & \times \exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\mu}_t)'(F_tWF_t')^{-1}(y_t-\tilde{\mu}_t)\right].
\end{align*}
In the case of the local level model with a $IG(\alpha_W,\beta_W)$ prior on $W$, this density simplifies to
\begin{align*}
p(W|V,\tilde{\psi},y) \propto& W^{-\alpha_W-1}\exp\left[ -a_{\tilde{\psi}}W + b_{\tilde{\psi}}/\sqrt{W} -c_{\tilde{\psi}}/W\right]
\end{align*}
where $a_{\tilde{\psi}} = \frac{1}{2V}\sum_{t=1}^T\tilde{\psi}_t^2$,  $b_{\tilde{\psi}} = \sum_{t=1}^T\mathcal{L}\tilde{y}_t\mathcal{L}\tilde{\psi}_t$, and $c_{\tilde{\psi}} = \beta_W + \frac{1}{2}\sum_{t=1}^T\mathcal{L}\tilde{y}_t^2$. Here we define $\mathcal{L}y_t = y_t - y_{t-1}$ for $t=2,3,\cdots,T$ while $\mathcal{L}y_1 = y_1 - \tilde{\psi}_0$, and $\mathcal{L}\tilde{\psi}_t = \tilde{\psi}_t - \tilde{\psi}_{t-1}$ for $t=2,3,\cdots,T$ while $\mathcal{L}\tilde{\psi}_1 = \tilde{\psi}_1 - 0$. This is the same family of densities as $p(V|W,\tilde{\gamma},y)$, and in Section \ref{sec:wscale} we show how to efficiently obtain random draws.

\section{Mixed Cholesky Factorization Algorithm (MCFA) for simulation smoothing}\label{sec:MCFA}

Traditionally in DLMs, forward filtering, backward sampling (FFBS) is used in order to draw from the latent states $\theta_{0:T}$. This requires running the Kalman filter in order to determine the marginal distribution of $\theta_T$, then drawing $\theta_t|\theta_{t+1:T}$ for $t=T-1,T-2,\cdots,1$ \cite{carter1994gibbs,fruhwirth1994data}. The mixed Cholesky factorization algorithm (MCFA) determines the joint distribution of $\theta_{0:T}$ and draws from it using a backward sampling step as in FFBS. The idea comes from \citeasnoun{rue2001fast}, which introduces a Cholesky factorization algorithm (CFA) for drawing from a Gaussian Markov random field and notes that the conditional distribution of $\theta_{0:T}$ given $y_{1:T}$ in a Gaussian linear statespace model is a special case, called the AWOL smoother in \citeasnoun{kastner2013ancillarity}. The algorithm exploits the fact that the full conditional distribution of $\theta_{0:T}$ is Gaussian with a block tridiagonal precision matrix in order to quickly compute its Cholesky decomposition. \citeasnoun{mccausland2011simulation} improves the idea by implicitly computing this Cholesky decomposition through a backward sampling strategy by mixing the substeps of the factorization and sampling steps -- hence the name -- starting with sampling from the marginal distribution of $\theta_T$. 

Suppose our model is as follows:
\begin{align*}
  y_t &= F_t\theta_t + v_t\\
  \theta_t & = G_t\theta_{t-1} + w_t
\end{align*}
with $v_t\stackrel{ind}{\sim} N(0,V_t)$ independent of $w_t\stackrel{ind}{\sim}N(0,W_t)$ for $t=1,2,\cdots,T$ and $\theta_0\sim N(m_0,C_0)$. This is the usual DLM except now we allow for time dependent variances for illustrative purposes. Then $(y_{1:T},\theta_{0:T})$ is joint Gaussian conditional on $(V_{1:T},W_{1:T})$ (in this section, everything is conditional on $V_{1:T}$ and $W_{1:T}$, so we will not make this conditioning explicit). So we can write $p(\theta_{0:T}|y_{1:T})$ as
\begin{align*}
  \log p(\theta_{0:T}|y_{1:T}) = -\frac{1}{2}g(\theta_{0:T},y_{1:T}) + K
\end{align*}
where $K$ is some constant with respect to $\theta_{0:T}$ and
\begin{align*}
  g(\theta_{0:T},y_{1:T}) = \theta_{0:T}'\Omega\theta_{0:T} - 2\omega'\theta_{0:T}.
\end{align*}
However, we also have
\begin{align*}
  \log p(\theta_{0:T}|y_{1:T}) = \log p(\theta_{0:T},y_{1:T}) - \log p(y_{1:T}).
\end{align*}
This means that
\begin{align*}
  g(\theta_{0:T}&,y_{1:T}) = (\theta_0 - m_0)C_0^{-1}(\theta_0 - m_0) + K'\\
  & + \sum_{t=1}^T(y_t - F_t\theta_t)'V_t^{-1}(y_t - F_t\theta_t) \\
  & + \sum_{t=1}^T(\theta_t - G_t\theta_{t-1})'W_t^{-1}(\theta_t - G_t\theta_{t-1}).
\end{align*}
where $K'$ is another constant that does not depend on $\theta_{0:T}$.

So now we can identify blocks of $\Omega$ with the cross product terms of the $\theta_t$'s and blocks of $\omega$ with the single product terms. Specifically, $\Omega$ is a banded diagonal matrix with
\begin{align*}
  \Omega = \begin{bmatrix} \Omega_{00} & \Omega_{01} & 0 &\ddots & 0 & 0\\
    \Omega_{10} & \Omega_{11} & \Omega_{12} & \ddots  & 0            & 0\\
    0          & \Omega_{21} & \Omega_{22} & \ddots  & 0            & 0\\
    \ddots     & \ddots     & \ddots     & \ddots  & \ddots       & \ddots \\
    0          & 0          & 0          & \ddots  & \Omega_{T-1,T-1} & \Omega_{T-1,T}\\
    0          & 0          & 0          & \ddots  & \Omega_{T,T-1} & \Omega_{TT}\end{bmatrix}
\end{align*}
and $\omega = (\omega_0', \omega_1', \cdots, \omega_T')'$ where the $\Omega_{st}$'s and $\omega_{t}$'s defined below:
\begin{align*}
  \Omega_{00} & = C_0^{-1} + G_1'W_1^{-1}G_1 && \\
  \Omega_{tt} & = F_t'V_t^{-1}F_t + W_t^{-1} + G_{t+1}'W_{t+1}^{-1}G_{t+1} &&  \mathrm{ for }\ \  t=1,2,\cdots T-1\\
  \Omega_{TT} & = F_T'V_T^{-1}F_T + W_T^{-1} && \\
  \Omega_{t,t-1} & = - W_t^{-1}G_t &&  \mathrm{ for }\ \  t=1,2,\cdots T\\
  \Omega_{t-1,t} & = - G_t'W_t^{-1} = \Omega_{t,t-1}' && \mathrm{ for }\ \  t=1,2,\cdots T\\
  \omega_0 & = C_0^{-1}m_0 &&\\
  \omega_t &= F_t'V_t^{-1}y_t &&  \mathrm{ for }\ \  t=1,2,\cdots T.
\end{align*}
Together, $\Omega$ and $a$ determine the Gaussian distribution from which $\theta_{0:T}$ should be drawn. \citeasnoun{rue2001fast} shows how to take advantage of the sparsity of $\Omega$ in order to quickly compute its Cholesky factorization and in order to find the mean vector from $\omega$ and this factorization. \citeasnoun{mccausland2011simulation} shows that instead of computing these quantities directly, you can draw $\theta_T$ and $\theta_t|\theta_{t+1:T}$ iteratively, which ultimately reduces the number of linear algebra operations which must be performed and typically speeds up the computation despite taking advantage of essentially the same mathematical technology.

The resulting algorithm requires a couple more intermediate quantities. Let $\Sigma_0 = \Omega_{00}^{-1}$, $\Sigma_t = (\Omega_{tt} - \Omega_{t,t-1}\Sigma_{t-1}\Omega_{t-1,t})^{-1}$ for $t=1,2,\cdots,T$, $h_0 = \Sigma_0\omega_0$, and $h_t = \Sigma_t(\omega_t - \Omega_{t,t-1}h_{t-1})$ for $t=1,2,\cdots,T$. Then
\begin{align*}
  \theta_T \sim & N(h_T, \Sigma_T) &&\\
  \theta_{t|t+1:T} \sim & N(h_t - \Sigma_t\Omega_{t,t+1}\theta_{t+1}, \Sigma_t) && \mathrm{for}\ \ t=T-1,T-2,\cdots,0.
\end{align*}
\citeasnoun{mccausland2011simulation} shows how to quickly compute the required linear algebra operations and finds that this method is often faster than simply doing the Cholesky factorization. This algorithm can also be applied to drawing the scaled errors, $\psi_{0:T}$, and the wrongly-scaled errors, $\tilde{\psi}_{0:T}$.

\section{Further augmentation for non-invertible $F_t$}\label{sec:F}

Throughout the paper we assumed that $F_t$ is square and invertible for all $t$ which made the construction of the SE sampler and other samplers that use the scaled errors easier. However, most DLMs do not have $F_t$'s which are square, let alone invertible. The samplers we constructed can often still be used in this case with one tweak: an additional DA is required in order to ensure that $F_t$ is square and invertible for all $t$. The basic strategy is to add elements to $y_t$ until $F_t$ is invertible, then add an additional step to the sampler in order to draw the new augmentation. A second issue is that often $G_t$ or $F_t$ or both depend on some unknown parameter which must also be sampled from in the various MCMC samplers. The second case is easily dealt with simply by adding another sampling step for the unknown parameters in $F_t$ and $G_t$. The following example illustrates how to deal with the first case. See \citeasnoun{simpson2014app} for another example.

Consider the dynamic regression model
\begin{align*}
y_t & = \alpha_t + x_t\beta_t + v_t\\
\alpha_t & = \alpha_{t-1} + w_{1,t}\\
\beta_t & = \beta_{t-1} + w_{2,t}\\
\end{align*}
for $t=1,2,\cdots,T$ with $v_{1:T}$ independent of $w_{1:T}=(w_1',w_2',\cdots,w_T')'$ where $w_t=(w_{1,t},w_{2,t})'$, $v_t\stackrel{iid}{\sim} N(0,V)$ and $w_t \stackrel{iid}{\sim}N_2(0,W)$.  Here the latent state in period $t$ is $\theta_t=(\alpha_t,\beta_t)'$. The problem is that $F_t=[1,x_t]$ is neither square nor invertible. But notice that the matrix
\[
F^*_t = \begin{bmatrix} 1 & x_t \\ 0 & 1 \end{bmatrix}
\]
is invertible. Now we add an additional DA $z_t$ to $y_t$ to construct $y_t^* = (y_t, z_t)'$ so that now the model is
\begin{align*}
y_t^* &= F_t^*\theta_t + v_t^*\\
\theta_t& = \theta_{t-1} + w_t
\end{align*}
where $v_t^* = (v_t, u_t)$ where $u_{1:T}$ is independent of $(v_{1:T}, w_{1:T})$ and $u_t\stackrel{iid}{\sim} N(0,1)$. By construction $v_t^*\stackrel{iid}{\sim} N_2(0,V^*)$ where $V^*$ is a diagonal matrix with the vector $(V,1)$ along the diagonal and the full conditional distribution of $z_t$ is $N(\beta_t,1)$. Then we define the scaled errors as $\psi_0 = \theta_0$ and $\psi_t=L_{V^*}^{-1}(y_t^* - F_t^*\theta_t)$. Let $z=z_{1:T}$ and $y^*=y^*_{1:T}$ for brevity. 

In terms of $\theta$, the likelihood is
\begin{align*}
p(y,z,\theta|V,W) \propto& |V^*|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t^* - F_t^*\theta_t)'(V^*)^{-1}(y_t^* - F_t^*\theta_t)\right]\\
 &\times |W|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t - \theta_{t-1})'W^{-1}(\theta_t - \theta_{t-1})\right]\\
\propto & V^{-T/2}\exp\left[-\frac{1}{2V}\sum_{t=1}^T(y_t - \alpha_t - x_t\beta_t)^2\right]\exp\left[-\frac{1}{2}\sum_{t=1}^t(z_t - \beta_t)^2\right]\\
 &\times |W|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t - \theta_{t-1})'W^{-1}(\theta_t - \theta_{t-1})\right]
\end{align*}
Then by transforming to $\psi$, the back transformation is $\theta_t = (F_t^*)^{-1}(y_t^* - L_{V^*}\psi_t)$ so the Jacobian is block diagonal with $T$ copies of $(F_t^*)^{-1}L_{V^*}$ along with a single copy of the identity matrix along the diagonal. So the determinant of the Jacobian is $|J| = |V^*|^{T/2}$ and the likelihood can be written in terms of $\psi$ as
\begin{align}
p(y,z,\theta|V,W) \propto& \exp\left[-\frac{1}{2}\sum_{t=1}^T\psi_t'\psi_t\right] |W|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t^* - \mu_t)'(F_t^*W (F_t^*)')^{-1}(y^*_t - \mu_t)\right] \label{Fpsilike}.
\end{align}
where we define $\mu_1 = L_{V^*}\psi_1 + F^*_1\psi_0$ and for $t=2,3,\cdots,T$, $\mu_t =L_{V^*}\psi_t + F^*_t(F^*_{t-1})^{-1}(y^*_{t-1} - L_{V^*}\psi_{t-1})$. 

Now in order to construct a sampler that uses $\psi$, we simply add a new step to sampler to draw $z$ from its full conditional just before transforming to $\psi$. In the GIS and alternating algorithms, we now have to draw an updated $z$ every time we change the DA. When using the states, $z_t|V,W,\theta,y \stackrel{iid}{\sim}N(\beta_t,1)$, so it is easiest to transform to $\theta$ before drawing $z$. So for example in the SD-SE GIS sampler with $V$, $W$, $\alpha_0$, and $\beta_0$ independent in the prior, an $IG(\alpha_V,\beta_V)$ prior on $V$, and an $IW(\Lambda_W,\lambda_W)$ prior on $W$, the algorithm becomes
\begin{alg*}[SD-SE GIS for dynamic regression]Scaled Disturbance-Scaled Error GIS Sampler for the dynamic regression model
\begin{enumerate}
\item Use the MCFA to sample $\theta \sim p(\theta|V,W,y)$.
\item Sample $V \sim IG\left(\alpha_V + T/2, \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \alpha_t - \beta_t)^2\right)$.
\item Transform $\theta$ to $\gamma$.
\item Sample $W \sim p(W|V,\gamma,y)$.
\item Transform $\gamma$ to $\theta$.
\item Sample $z_t\stackrel{iid}{\sim}N(\beta_t,1)$ and form $y^*$.
\item Transform $\theta$ to $\psi$.
\item Sample $V \sim p(V|W,\psi,y^*)$.
\item Sample $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
\end{enumerate}
\end{alg*}\noindent
Step 8 is particularly tricky since $V$ is a component of $V^*$, and $V^*$ has the same density $p(V|W,\psi,y)$ that shows up in the usual case of the scaled disturbances, except now the lower right diagonal element is set to one. So while we can write down the various algorithms in the non-invertible $F$ case, the density $p(V|W,\psi,y^*)$ is tricky to work with. In step 8 $V$ is drawn conditional on $y^*$, but another option is to draw $V$ conditional on $y$ but not on $z$. This would require integrating $z$ out of the likelihood, equation \eqref{Fpsilike}. It is not clear which of these is easier or faster, though it is likely that the changing the prior for $V$ and $W$ will have an impact.

\section{Efficiently drawing from $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$ in the LLM}\label{sec:scaledraw}
From Appendix \ref{subsec:SDs}, the full conditional distribution of $W$ given $\gamma$ is
 \begin{align*}
  p(W&|V,\gamma,y) \propto  p(V,W,\gamma,y) \propto |W|^{-(\lambda_W + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\\
&\times  \exp\left[-\frac{1}{2}\left(\sum_{t=1}^T\left[y_t-F_t\sum_{s=0}^{t}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]'V^{-1}\left[y_t-F_t\sum_{s=0}^{t-1}\tilde{G}_{s,t}L_W\gamma_{t-s} - F_t\tilde{G}_{t,t}\gamma_0\right]\right)\right]
 \end{align*}
where $L_W$ is the Cholesky factor of $W$ defined so that $L_WL_W'=W$. We can write this density as
\begin{align*}
p(W|V,\gamma,y)\propto& |W|^{-(\lambda_W + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\\
&\times  \exp\left[-\frac{1}{2}\left(\vect(L_W)'A_W\vect(L_W) - 2B_W\vect(L_W)\frac{}{}\right)\right]
\end{align*}
where
\begin{align*}
A_W &= \sum_{t=1}^T\sum_{s=0}^t\left(\gamma_{t-s}\gamma_{t-s}'\otimes \tilde{G}_{s,t}'F_t'V^{-1}F_t\tilde{G}_{s,t}\right)\\
\intertext{and}
B_W &= \sum_{t=1}^T\sum_{s=0}^t\left(\gamma_{t-s}'\otimes (y_t - F_t\tilde{G}_{t,t}\gamma_0)'V^{-1}F_t\tilde{G}_{s,t}\right)
\end{align*}
can be found using the properties of the $\vect$ and $\tr$ operators.

Similarly from Appendix \ref{subsec:SEs}, the full conditional distribution of $V$ given $\psi$ is
\begin{align*}
 p(V&|W,\psi,y) \propto p(V,W,\psi,y) \propto |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\left(\tr\left(\Lambda_VV^{-1}\right) + \sum_{t=1}^T(y_t - \mu_t)'(F_tWF_t')^{-1}(y_t - \mu_t)\right)\right]
\end{align*}
where $\mu_1 = L_V\psi_1 + F_1G_1\psi_0$ and for $t=2,3,\cdots,T$, $\mu_t =L_V\psi_t + F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{V}\psi_{t-1})$. This density can be written in a familiar form:
\begin{align*}
 p(V&|W,\psi,y) \propto p(V,W,\psi,y) \propto |V|^{-(\lambda_V + p + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right]\\
&\times\exp\left[-\frac{1}{2}\left(\vect(L_V)'A_V\vect(L_V) - 2B_V\vect(L_V)\frac{}{}\right)\right]
\end{align*}
where
\begin{align*}
A_V =& \sum_{t=1}^T\psi_t\psi_t'\otimes (F_tWF_t')^{-1} + \sum_{t=2}^T\psi_{t-1}\psi_{t-1}' \otimes (G_tF_{t-1}^{-1})'W^{-1}G_tF_{t-1}^{-1}\\
& - \sum_{t=2}^T\psi_t\psi_{t-1}'\otimes (WF_t')^{-1}G_tF_{t-1}^{-1} - \sum_{t=2}^T\psi_{t-1}\psi_{t}'\otimes (G_tF_{t-1}^{-1})'(F_tW)^{-1}\\
\intertext{and}
B_V =& \psi_1'\otimes (y_1 + F_1G_1\psi_0)'(F_1WF_1')^{-1} + \sum_{t=2}^T \psi_t'\otimes (y_t-F_tG_tF_{t-1}^{-1}y_{t-1})'(F_tWF_t')^{-1}\\
& - \sum_{t=2}^T\psi_{t-1}'\otimes (y_t-F_tG_tF_{t-1}^{-1}y_{t-1})'(WF_t')^{-1}G_tF_{t-1}^{-1}
\end{align*}
can again be found using the properties of the $\vect$ and $\tr$ operators. Both of these densities are of the form
\begin{align*}
p(X) \propto |X|^{-(\lambda + p + 2)/2}\exp\left[-\frac{1}{2}\left(\tr(\Lambda X^{-1}) + \vect(L_X)'A\vect(L_X) - 2B\vect(L_X)\frac{}{}\right)\right]
\end{align*}
where $X$ is a $p\times p$ symmetric and positive definite random matrix, $L_X$ is the Cholesky factor of $X$ so that $L_XL_X'=X$, $\lambda>0$, $\Lambda$ is a $p\times p$ symmetric and positive definite matrix, $A$ is a $p^2\times p^2$ matrix, and $B$ is a $1\times p^2$ matrix. 

The complexity of this density is caused by the interaction between the inverse Wishart prior and the augmented data likelihood in terms of the scaled disturbances for $W$ or for the scaled errors for $V$. In the local level model, the density still is not a known form and is difficult to sample from, but sampling from it is possible. In this case the log density is
\begin{align*}
\log p(x) =  & - (\alpha + 1)\log x -ax + b\sqrt{x}  -c/x + C 
\end{align*}
for $x>0$ where $C$ is some constant, $\alpha>0$ and $c>0$ are the hyperparameters for $x$, and $a>0$ and $b\in \Re$ are parameters that depend on the data, $y$, the relevant data augmentation ($\psi$ or $\gamma$), and the other variable ($W$ or $V$). We provide two different rejection sampling strategies below that work well under different circumstances, and combine them into a single strategy.

\subsection{Adaptive rejection sampling}
One nice strategy is to use adaptive rejection sampling, e.g. \citeasnoun{gilks1992adaptive}. This requires $\log p(x)$ to be concave, which is easy enough to check. The second derivative of $\log p(x)$ is:
\begin{align*}
\frac{\partial^2 \log p(x)}{\partial x^2} &= -\frac{1}{4}bx^{-3/2} +(\alpha + 1)x^{-2} -2 c x^{-3}.
\end{align*}
Then we have
\begin{align*}
  &\frac{\partial^2 \log p(x)}{\partial x^2} < 0 && \iff &&-\frac{b}{4}x^{3/2} + (\alpha + 1)x - 2c < 0
\end{align*}
which would imply that $\log p(x)$ is concave. We can maximize the left hand side of the last equation very easily. When $b\leq 0$ the max occurs at $x=\infty$ such that $LHS > 0$, but when $b > 0$:
\begin{align*}
  \frac{\partial LHS}{\partial x} &= -\frac{3}{8}bx^{1/2} + \alpha + 1 = 0 && \implies && x^{max} = \frac{(\alpha + 1)^2}{b^2}\frac{64}{9}.
\end{align*}
Then we have
\begin{align*}
  LHS \leq LHS|_{x=x^{max}} = \frac{(\alpha + 1)^3}{b^2}\frac{64}{27} - 2c
\end{align*}
so that
\begin{align*}
  LHS|_{x=x^{max}} < 0 &\iff  \frac{(\alpha + 1)^3}{b^2}\frac{64}{27} < 2c &&\iff&& b > \left(\frac{(\alpha + 1)^3}{c}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}.
\end{align*}
This last condition is necessary and sufficient for $\log p(x)$ to be globally (for $x>0$) concave since $b < 0$ forces $LHS > 0$ for some $x$. When the condition is satisfied, we can use adaptive rejection sampling --- which is already implemented in the \verb0R0 package \verb0ars0 \cite{PerezARS}. We input the initial evaluations of $\log p(x)$ at the mode $x^{mode}$ and at $2x^{mode}$ and $0.5x^{mode}$ in order to get the algorithm going.

\subsection{Rejection sampling on the log scale}

When $b \leq \left(\frac{(\alpha + 1)^3}{c}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}$, which happens often --- especially for small $T$ --- we need to rely on a different method to sample from $p(x)$. A naive approach would be to construct a normal or $t$ approximation to $p(x)$ and use that as a proposal in a rejection sampler. It turns out that this is often very inefficient, but for $z=\log(x)$ the approach works well. Note that
\begin{align*}
  p_z(z) = p_x(e^z)e^z
\end{align*}
so that we can write the log density of $z$ as (dropping the subscripts):
\begin{align*}
  \log p(z) = -ae^z + be^{z/2} - \alpha z - c e^{-z}.
\end{align*}
The mode of this density $z^{mode}$ can be easily found numerically, and the second derivative is:
\begin{align*}
  \frac{\partial^2 \log p(z)}{\partial z^2} = -ae^z + \frac{b}{4}e^{z/2} - c e^{-z}.
\end{align*}
The $t$ approximation then uses the proposal distribution 
p\begin{align*}
  t_{v}\left(z^{mode}, \left[-\left.\frac{\partial^2 \log p(z)}{\partial z^2}\right|_{z=z^{mode}}\right]^{-1}\right).
\end{align*}
In practice choosing degrees of freedom $v=1$ works very well over the region of the parameter space where adaptive rejection sampling cannot be used. We can easily use this method when adaptive rejection sampling does not work, then transform $z$ back to $x$. It remains to check that the tails of $t$ distribution dominate the tails of our target distribution. Let $\log q(z)$ denote the log density of the proposal distribution. Then we need
\begin{align*}
  \log p(z) - \log q(z) \leq M
\end{align*}
for some constant M, i.e.
\begin{align*}
  -ae^z + be^{z/2} - \alpha z - c e^{-z} -\left(\frac{v+1}{2}\right)\log\left[1 + \frac{1}{v}\left(\frac{z-\mu}{\sigma}\right)^2\right]\leq M
\end{align*}
where $a>0$, $c>0$, $\alpha>0$, $v>0$, $\sigma>0$, and $b,\mu\in \Re$. We can rewrite the LHS as
\begin{align*}
    e^{z/2}(b-ae^{z/2}) - \alpha z - c e^{-z} -\left(\frac{v+1}{2}\right)\log\left[1 + \frac{1}{v}\left(\frac{z-\mu}{\sigma}\right)^2\right].
\end{align*}
So as $z\to\infty$ this quantity goes to $-\infty$ since the first term will eventually become negative no matter the value of $b$, and all other terms are always negative. Similarly as $z\to -\infty$ this quantity goes to $-\infty$. Now pick any interval $(z_1,z_2)$ such that outside of the interval, $LHS<\epsilon$. Since treated as a function of $z$ the LHS is clearly continuous, it attains a maximum on this interval, and thus is bounded.

\subsection{Intelligently choosing a rejection sampler}
In practice, adaptive rejection sampling is relatively efficient for $p_x(x)$ but inefficient for $p_z(z)$ --- so much so that rejection sampling with the $t$ approximation for $p_z(z)$ is more efficient. To minimize computation time, it is best to use adaptive rejection sampling for $p_x(x)$ when the concavity condition is satisfied. When it is not, the $t$ approximation works well enough.

\section{Efficiently drawing from $p(W|V,\tilde{\gamma},y)$ and $p(V|W,\tilde{\psi},y)$ in the LLM}\label{sec:wscale}

Both the density of $\log(W)|V,\tilde{\gamma},y$ and the density of $\log(V)|W,\tilde{\psi},y$ have the following form:
\begin{align*}
  p(z)\propto \exp\left[-\alpha z - ae^{-z} + be^{-z/2} - ce^z\right].
\end{align*}
where $\alpha>0$, $a>0$, $c>0$, and $b\in \Re$. The log density is:
\begin{align*}
  \log p(z) = -\alpha z - ae^{-z} + be^{-z/2} - ce^z + C
\end{align*}
where $C$ is some constant. We only provide one strategy for rejection sampling from this density: the $t$ approximation. Similar reasoning to the previous subsection above shows that we can use a $t$ distribution as a proposal in a rejection sampler for this density. Now we choose the location parameter by maximizing $\log p(z)$ in $z$ numerically to find the mode, $z^{mode}$. Next the second derivative of $\log p(z)$ is given by
\begin{align*}
  \frac{\partial^2 \log p(z)}{\partial z^2} = -ae^{-z} + \frac{b}{4}e^{-z/2}-ce^z.
\end{align*}
We then set the scale parameter to be
\begin{align*}
  -\left[\left.\frac{\partial^2 \log p(z)}{\partial z^2}\right|_{z=z^{mode}}\right]^{-1}
\end{align*}
as in the normal approximation, and the degrees of freedom parameter to $v=1$. This rejection sampler is tolerably efficient for our purposes, but there is much room for improvement.

\section{Equivalence of CIS and GIS in the DLM}\label{sec:CISGIS}
The CIS algorithm consists of the following steps:
\begin{align*}
&[\psi|V^{(k)},W^{(k)}] \to [V^{(k+0.5)}|W^{(k)},\psi] \to [\tilde{\psi}|V^{(k+0.5)},W^{(k)},\psi] \to [V^{(k+1)}|W^{(k)},\tilde{\psi}]\to\\
&[\tilde{\gamma}|V^{(k+1)},W^{(k)},\tilde{\psi}] \to [W^{(k+0.5)}|V^{(k+1)},\tilde{\gamma}] \to [\gamma|V^{(k+1)},W^{(k+0.5)},\tilde{\gamma}]\to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
In the fourth step of line one and the second step of line two, each of those densities would be unchanged if we conditioned on $\theta$ instead of $\tilde{\psi}$ on the first line or $\tilde{\gamma}$ on the second line. So the CIS algorithm above is equivalent to the following:
\begin{align*}
&[\psi|V^{(k)},W^{(k)}] \to [V^{(k+0.5)}|W^{(k)},\psi] \to [\theta|V^{(k+0.5)},W^{(k)},\psi] \to [V^{(k+1)}|W^{(k)},\theta]\to\\
&[W^{(k+0.5)}|V^{(k+1)},\theta] \to [\gamma|V^{(k+1)},W^{(k+0.5)},\theta]\to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
Now since $V$ and $W$ are conditionally independent given $\theta$ and $y$, the last step of line one and the first step of line 2 can be switched: 
\begin{align*}
&[\psi|V^{(k)},W^{(k)}] \to [V^{(k+0.5)}|W^{(k)},\psi] \to [\theta|V^{(k+0.5)},W^{(k)},\psi] \to [W^{(k+0.5)}|V^{(k+0.5)},\theta] \to\\
&[V^{(k+1)}|W^{(k+0.5)},\theta] \to [\gamma|V^{(k+1)},W^{(k+0.5)},\theta]\to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
Next $V$'s conditional density is the same whether we condition on $\theta$ or $\gamma$, so we can do the $V$ step between the $\gamma$ step and the $W$ step in line two. Similarly we can move the $W$ step to between the $V$ step and the $\theta$ step in line one. This yields:
\begin{align*}
&[\psi|V^{(k)},W^{(k)}] \to [V^{(k+0.5)}|W^{(k)},\psi] \to [W^{(k+0.5)}|V^{(k+0.5)},\psi] \to \\
&[\gamma|V^{(k+0.5)},W^{(k+0.5)},\psi] \to  [V^{(k+1)}|W^{(k+0.5)},\gamma] \to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
This is actually a SE-SD GIS algorithm, so the CIS sampler we started with is equivalent to SE-SD GIS. Since we do not expect the order in which the DAs appear in a GIS algorithm to matter, CIS should have the same mixing and convergence properties as the SD-SE GIS algorithm we constructed.

\section{Partial CIS algorithms in the DLM}

In addition to the GIS and CIS algorithms discussed in the main body of the article, \citeasnoun{yu2011center} also introduce {\it partial CIS} algorithms. While a CIS algorithm interweaves in separate Gibbs steps for each sub-vector of the parameter, a partial CIS algorithm has a usual Gibbs step for at least one of the parameter vectors. For example, suppose that the model parameter is $\phi=(\phi_1,\phi_2)$, and $\gamma_1$, $\gamma_2$, and $\theta$ are available DAs. Then a partial CIS algorithm using these DAs is
\begin{alg*}[partial CIS]Partial Componentwise Interweaving Strategy\label{alg:pCIS}
\begin{center}
\begin{tabular}{llllllll}
$[\gamma_1|\phi_1^{(k)},\phi_2^{(k)}]$ &$\to$& $[\phi_1^{(k+0.5)}|\phi_2^{(k)},\gamma_1]$ & $\to$ & $[\gamma_2|\phi_1^{(k+0.5)},\phi_2^{(k)},\gamma_1]$ & $\to$ & $[\phi_1^{(k+1)}|\phi_2^{(k)},\gamma_2]$ &$\to$\\
$[\theta|\phi_1^{(k+1)},\phi_2^{(k)},\gamma_2]$ & $\to$ & $[\phi_2^{(k+1)}|\phi_1^{(k+1)},\theta]$. &&&&&
\end{tabular}
\end{center}
\end{alg*}\noindent
The first line is an interweaving step for $\phi_1$ while the second line is a standard Gibbs step for $\phi_2$. Partial CIS algorithms are easier to construct than full CIS algorithms at the cost of slower convergence \cite{yu2011center}.

In the DLM we can construct two partial CIS algorithms using the wrongly-scaled DAs in much the same way they were used to construct the full CIS algorithm. The first algorithm interweaves for $W$ using the scaled disturbances, $\gamma$, and the wrongly-scaled disturbances, $\tilde{\gamma}$:
\begin{align*}
&[\theta|V^{(k)},W^{(k)}] \to [V^{(k+1)}|W^{(k)},\theta] \to\\
&[W^{(k+0.5)}|V^{(k+1)},\theta]\to [\gamma|V^{(k+1)},W^{(k+0.5)},\theta] \to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
As in the construction of the full CIS algorithm, we use $\theta$ instead of $\tilde{\gamma}$ in the second line since $p(W|V,\tilde{\gamma}) = p(W|V,\theta)$. Using an argument similar to that used in Appendix \ref{sec:CISGIS}, we can show that this partial CIS algorithm is equivalent to the SD-State GIS algorithm.

Analogously, we can use the scaled errors, $\psi$, and the wrongly-scaled errors, $\tilde{\psi}$, to construct a partial CIS algorithm that interweaves for $V$:
\begin{align*}
&[\psi|V^{(k)},W^{(k)}] \to [V^{(k+0.5)}|W^{(k)},\psi] \to [\theta|V^{(k+0.5)},W^{(k)},\psi] \to [V^{(k+1)}|W^{(k)},\theta]\to\\
&[W^{(k+1)}|V^{(k+1)},\theta].
\end{align*}
This algorithm is equivalent to the SE-State GIS algorithm.

\section{Using posterior correlations to understand patterns of ESP}

\setcounter{figure}{0}    

Most of the patterns in Figures \ref{ESplot10}, \ref{ESplot100}, and \ref{ESplot1000} in the next section can be explained by Figure \ref{corplot}, which contains the estimated posterior correlations between various functions of parameters estimated using the simulations from the Triple-Alt sampler for a time series with $T=100$. We omit a similar analysis for $T=10$ and $T=1000$. The state sampler consists of two steps --- a draw of $\theta$ given $V$ and $W$, and a draw of $(V,W)$ given $\theta$. From Section \ref{subsec:states} we have that conditional on $\theta$, $V$ and $W$ are independent in the posterior and each has an inverse gamma distribution that depends on the states only through the second parameter:
\begin{align*}
  b_V &\equiv \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2 &
  b_W &\equiv \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2.
\end{align*}
So we can view $(b_V,b_W)$ as the data augmentation instead of $\theta$ and thus the state sampler is
\begin{align*}
  [b_V, b_W|V^{(k)},W^{(k)}] \to [V^{(k+1)},W^{(k+1)}|b_V,b_W].
\end{align*}
Thus the dependence between $(V,W)$ and $(b_V,b_W)$ in the posterior will determine how much the state sampler moves in a given iteration and, in particular, it is possible that $V$ and $W$ have very different serial dependence from each other since we are drawing them jointly. When the dependence between $V$ and $b_V$ is high, the $(V,W)$ step will hardly move $V$ even if it drastically moves $W$ since $V$ and $W$ are independent. However, the $(b_V,b_W)$ step may move both elements a moderate amount since they both depend on $(V,W)$.

In Figure \ref{corplot} we see that the posterior correlation between $V$ and $b_V$ is high in magnitude and positive when $R^*>1$ while the posterior correlation between $V$ and $b_W$ is moderate to low and negative. When $R^*$ is large enough though, the posterior correlation between $V$ and $b_W$ evaporates. Similarly when $R^*<1$ the posterior correlation between $W$ and $b_W$ is high and positive and the posterior correlation between $W$ and $b_V$ is high and negative. Again as $R^*$ becomes large enough the correlation between $W$ and $b_V$ goes to zero. So when $R^*>1$, the draw of $(b_V, b_W)$ is unlikely to move $b_V$ much since $b_V$ is so highly correlated with $V$ and essentially uncorrelated with $b_W$, but $b_W$ is essentially uncorrelated with $W$ and negatively correlated with $V$ so $b_W$ is likely to move a fair amount. Furthermore the draw of $V$ is highly correlated with $b_V$ while the draw of $W$ is essentially independent of $b_W$ (and the draws of $V$ and $W$ are independent conditional on $b_V$ and $b_W$). Thus when $R^*>1$ we should expect high serial dependence for $V$ and low serial dependence for $W$, and so low ESP for $V$ and high ESP for $W$, which is exactly what we see in Figure \ref{ESplot100}. By similar reasoning when $R^*<1$, we should expect low serial dependence for $V$ and high serial dependence for $W$ and thus high ESP for $V$ and low ESP for $W$, which can also be seen in Figure \ref{ESplot100}.

For the SD sampler, things are a bit more complicated. The draw of $V|W,\gamma$ still depends on $b_V$ since it is the same inverse gamma draw as in the state sampler, but the draw of $W|V,\gamma$ now depends on $a_\gamma$ and $b_\gamma$ defined in Section \ref{subsec:SDs} as
\begin{align*}
  a_\gamma &\equiv \frac{1}{2V}\sum_{t=1}^T\left(\sum_{j=1}^t\gamma_j\right)^2&
  b_\gamma &\equiv \frac{1}{V}\sum_{t=1}^T(y_t-\gamma_0)\left(\sum_{j=1}^t\gamma_j\right).
\end{align*}
So the dependence between $V$ and $b_V$ determines how much the chain moves in the $V$ step, and the dependence between $W$ and $(a_\gamma , b_\gamma)$ determines how much it moves in the $W$ step. The dependence between $(V,W)$ and $\gamma$ determines how much the chain moves in the DA step, but we can view this step instead as a draw of $b_V$ in which case the dependence between $W$ and $b_V$ determines how much the chain moves in that step. So if any one of these steps has high dependence, we should expect every element of the chain, and $(V,W)$ in particular, to have high serial dependence in the chain. The SE sampler is analogous to the SD sampler except with $b_W$, $a_\psi$ and $b_\psi$ where
\begin{align*}
  a_\psi&=\frac{1}{2W}\sum_{t=1}^T(\mathcal{L}\psi_t)^2&
  b_\psi&=\frac{1}{W}\sum_{t=1}^T(\mathcal{L}\psi_t\mathcal{L}y_t).
\end{align*}

\begin{figure}[!h]
\centering
\includegraphics[width=0.24\textwidth]{corplot1}
\includegraphics[width=0.24\textwidth]{corplot2}
\includegraphics[width=0.24\textwidth]{corplot3}
\includegraphics[width=0.24\textwidth]{corplot4}
\includegraphics[width=0.24\textwidth]{corplot5}
\includegraphics[width=0.24\textwidth]{corplot6}
\includegraphics[width=0.24\textwidth]{corplot7}
\includegraphics[width=0.24\textwidth]{corplot8}
\caption{Posterior correlation between $V$ or $W$ and $b_V$, $b_W$, $a_\gamma$, $b_\gamma$, $a_\psi$ or $b_\psi$. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data with $T=100$.}
\label{corplot}
\end{figure}

In order to analyze the SD sampler, first suppose $R^*>1$. Then from Figure \ref{corplot} $b_V$ has high correlation with $V$ and low correlation with $W$, so the draw of $b_V$ should not move the chain much. Next, the draw of $V$ should again not move the chain much because of the high correlation between $V$ and $b_V$. Finally the draw of $W$ has a fair chance to move the chain because it has low correlation with both $a_\gamma$ and $b_\gamma$. But this has little impact on $b_V$ and thus the entire chain since $b_V$ is so highly correlated with $V$ but hardly correlated with $W$. So when $R^*>1$, we should expect high serial dependence and low ESP for $V$. We should also expect similar behavior for $W$ since the entire chain is hardly moving so $W$'s hyperparameters are hardly moving. This is roughly what we see in Figure \ref{ESplot100}, though this reasoning does not allow us to predict which of $V$ and $W$ will have lower ESP. When $R^*<1$ the posterior correlation in each of the steps is broken, though in the $W$ step the correlation between $W$ and both $a_\gamma$ and $b_\gamma$ becomes negative and somewhat high in magnitude. Here we should not expect less serial dependence in $V$ or $W$, but we should perhaps expect higher ESP's since negatively correlated draws decrease Monte Carlo standard error. Indeed, we see ESP's near one for both variances in Figure \ref{ESplot100}. The SE sampler is analogous to the SD sampler and a similar analysis applies --- the posterior correlations between $V$ or $W$ and $b_W$, $a_\psi$ or $b_\psi$ in Figure \ref{corplot} roughly predict the ESP of the SE sampler in Figure \ref{ESplot100}. When one or more of the correlations are high, ESPs for $V$ and $W$ are low while when all of the correlations are low, both ESPs are high. We omit a similar analysis of the wrongly-scaled samplers for brevity, but note that their behavior will allows us to predict the behavior of the CIS sampler.

\section{Computational time}\label{sec:time}

From a practical standpoint a more important question than how well the chain mixes is the full computational time required to adequately characterize the target posterior distribution. In order to investigate this, we compute the natural log of the average time in minutes required for each sampler to achieve an effective sample size of 1000 --- in other words the log minutes per 1000 effective draws. All simulations were performed on a server with Intel Xeon X5675 3.07 GHz processors. While different systems will yield different absolute times, the relative times should be similar. Figure \ref{baseinttimeplot} contains plots of the log minutes per 1000 effective draws for both $V$ and $W$ and for each of the samplers.

\begin{figure}[!h]
\centering
\begin{subfigure}[b]{0.59\textwidth}
\includegraphics[width=\textwidth]{basecistimeplot100}
\caption{}
\label{fig:timea}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altgisVtimeplot100}
\caption{}
\label{fig:timeb}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altgisWtimeplot100}
\caption{}
\label{fig:timec}
\end{subfigure}
\caption{Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=100$ in each sampler. Figure \ref{fig:timea} contains the base samplers, Figure \ref{fig:timeb} contains the GIS and CIS samplers, while Figure \ref{fig:timec} contains the Alt samplers. Log times larger than three $\log \mathrm{min}$ are rounded down to three for plotting purposes.}
\label{baseinttimeplot}
\end{figure}

For $T=100$ the pattern we saw for ESP also appears for log minutes per 1000 effective draws. The State sampler becomes slow to reach 1000 effective draws for $V$ when $R^*>1$ and for $W$ when $R^*<1$. The SD and SE samplers behave as expected --- the SD sampler is slow for both $V$ and $W$ when $R^*>1$ while the SD sampler is slow for both $V$ and $W$ when $R^*<1$. The SD-SE GIS, Triple GIS and CIS algorithms appear to be the big winners here and are almost indistinguishable. All three algorithms are slightly slower for both $V$ and $W$ when $R^*$ is near one, though for larger $T$,  when $R^*$ is near or below one all three are slow for $W$ (plots available in Appendix \ref{sec:plots}). Compared to the state sampler, all three offer large gains over most of the parameter space. There appears to be no difference between a GIS algorithm and the corresponding alternating algorithm in terms of log time per 1000 effective draws, so the SD-SE Alt and Triple Alt algorithms are both just as efficient as the best interweaving algorithms. This may not always be the case though --- the GIS version of an algorithm is computationally cheaper than the Alt version since it consists of three of the four same steps, and in the fourth step the Alt algorithm has to obtain a random draw while the GIS algorithm typically only has to make a transformation. The more expensive that draw is relative to the transformation, the faster GIS will be relative to Alt. For example, in a longer the time series the transformation will be significantly cheaper relative to the random draw. We find that With very long time series, e.g. $T>100,000$, GIS is cheaper than Alt even in the local level model (Appendix \ref{sec:GISALTtime}). 


\clearpage

\section{Plots for all values of $T$}\label{sec:plots}

\begin{figure}[!thb]
\centering
\begin{subfigure}[b]{0.59\textwidth}%
\includegraphics[width=\textwidth]{basecisESplot10}
\caption{}
\label{fig:ESPa10}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altintESplotV10}
\caption{}
\label{fig:ESPb10}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altintESplotW10}
\caption{}
\label{fig:ESPc10}
\end{subfigure}
\caption{Effective sample proportion in the posterior sampler for a time series of length $T=10$, for $V$ and $W$ in the each sampler. Figure \ref{fig:ESPa10} contains ESP for $V$ and $W$ for the base samplers, Figure \ref{fig:ESPb10} contains ESP in the GIS and CIS samplers, and Figure \ref{fig:ESPc10} contains ESP in the Alt samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high.}
\label{ESplot10}
\end{figure}

\begin{figure}[!thb]
\centering
\begin{subfigure}[b]{0.59\textwidth}%
\includegraphics[width=\textwidth]{basecisESplot100}
\caption{}
\label{fig:ESPa100}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altintESplotV100}
\caption{}
\label{fig:ESPb100}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altintESplotW100}
\caption{}
\label{fig:ESPc100}
\end{subfigure}
\caption{Effective sample proportion in the posterior sampler for a time series of length $T=100$, for $V$ and $W$ in the each sampler. Figure \ref{fig:ESPa100} contains ESP for $V$ and $W$ for the base samplers, Figure \ref{fig:ESPb100} contains ESP in the GIS and CIS samplers, and Figure \ref{fig:ESPc100} contains ESP in the Alt samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high.}
\label{ESplot100}
\end{figure}

\begin{figure}[!thb]
\centering
\begin{subfigure}[b]{0.59\textwidth}%
\includegraphics[width=\textwidth]{basecisESplot1000}
\caption{}
\label{fig:ESPa1000}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altintESplotV1000}
\caption{}
\label{fig:ESPb1000}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altintESplotW1000}
\caption{}
\label{fig:ESPc1000}
\end{subfigure}
\caption{Effective sample proportion in the posterior sampler for a time series of length $T=1000$, for $V$ and $W$ in the each sampler. Figure \ref{fig:ESPa1000} contains ESP for $V$ and $W$ for the base samplers, Figure \ref{fig:ESPb1000} contains ESP in the GIS and CIS samplers, and Figure \ref{fig:ESPc1000} contains ESP in the Alt samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high.}
\label{ESplot1000}
\end{figure}

\begin{figure}[!thb]
\centering
\begin{subfigure}[b]{0.59\textwidth}
\includegraphics[width=\textwidth]{basecistimeplot10}
\caption{}
\label{fig:timea10}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altgisVtimeplot10}
\caption{}
\label{fig:timeb10}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altgisWtimeplot10}
\caption{}
\label{fig:timec10}
\end{subfigure}
\caption{Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=10$ in each sampler. Figure \ref{fig:timea10} contains the base samplers, Figure \ref{fig:timeb10} contains the GIS and CIS samplers, while Figure \ref{fig:timec10} contains the Alt samplers. Log times larger than three $\log \mathrm{min}$ are rounded down to three for plotting purposes.}
\label{baseinttimeplot10}
\end{figure}

\begin{figure}[!thb]
\centering
\begin{subfigure}[b]{0.59\textwidth}
\includegraphics[width=\textwidth]{basecistimeplot100}
\caption{}
\label{fig:timea100}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altgisVtimeplot100}
\caption{}
\label{fig:timeb100}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altgisWtimeplot100}
\caption{}
\label{fig:timec100}
\end{subfigure}
\caption{Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=100$ in each sampler. Figure \ref{fig:timea100} contains the base samplers, Figure \ref{fig:timeb100} contains the GIS and CIS samplers, while Figure \ref{fig:timec100} contains the Alt samplers. Log times larger than three $\log \mathrm{min}$ are rounded down to three for plotting purposes.}
\label{baseinttimeplot100}
\end{figure}

\begin{figure}[!thb]
\centering
\begin{subfigure}[b]{0.59\textwidth}
\includegraphics[width=\textwidth]{basecistimeplot1000}
\caption{}
\label{fig:timea1000}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=\textwidth]{altgisVtimeplot1000}
\caption{}
\label{fig:timeb1000}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{altgisWtimeplot1000}
\caption{}
\label{fig:timec1000}
\end{subfigure}
\caption{Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=1000$ in each sampler. Figure \ref{fig:timea1000} contains the base samplers, Figure \ref{fig:timeb1000} contains the GIS and CIS samplers, while Figure \ref{fig:timec1000} contains the Alt samplers. Log times larger than three $\log \mathrm{min}$ are rounded down to three for plotting purposes.}
\label{baseinttimeplot1000}
\end{figure}

\clearpage
\section{GIS vs. Alt in very long time series}\label{sec:GISALTtime}

While our results indicate that GIS and the corresponding Alternating algorithms seem to perform equally well in terms of both mixing and computational time, this is not always the case. GIS is computationally cheaper per iteration and in long enough time series this different is significant. To illustrate this we again simulated data from the local level model with $V=1$, $W=1$, for various lengths of the time series starting at $T=1000$ and increasing to $T=500,000$. Then we fit each model using the same priors as before using the SD-SE GIS and SD-SE Alt algorithms. Figure \ref{gisaltlongt} contains plots of the time in minutes per 1000 effective draws for each of $V$ and $W$ in each sampler. 

For both samplers the relationship between the length of the time series and the time per 1000 effective draws appears linear. However, for the alternating sampler the time required increases at a fast rate as the $T$ increases. For example with a time series of 300,000 observations, the SD-SE GIS sampler requires about 1000 minutes ($16.67$ hours) to achieve an effective sample size of 1000 for $W$ and about about 500 minutes ($8.33$ hours) for $V$. On the other hand the SD-SE Alt sampler requires 2000 minutes ($33.33$ hours) to achieve an effective sample size of 1000 for $W$ and 1000 minutes for $V$. Theses differences do not add up to much when the time series is short enough -- e.g. $T=1000$ and below, but when $T$ is on the order of $100,000$ the benefit of GIS starts to become significant.

\begin{figure}[!thb]
\centering
\includegraphics[width=\textwidth]{gisaltplot}
\caption{Time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for the SD-SE GIS and SD-SE Alt samplers in very long time series.}
\label{gisaltlongt}
\end{figure}


\clearpage
\bibliographystyle{MWS_jasa}  % proper bibliography style for ASA
\bibliography{dlmasis}
\end{document}

