\documentclass{article}

\usepackage{graphicx, color, amssymb, amsmath, bm, rotating, graphics,
epsfig, multicol, amsthm, bbm}
\usepackage{multicol}
\usepackage{textcomp}
\usepackage{fullpage}
\usepackage{booktabs}
\usepackage{caption}
\usepackage[authoryear]{natbib} %numbers instead of authoryear for [1] instead of [1980]

%Indicator function: use as \indicator{X=x}
\newcommand{\indicator}[1]{\mathbbm{1}{\left\{ {#1} \right\} }}

%Independent: use as X \ind Y | Z
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newtheorem{alg}{Algorithm}
\newtheorem{thm}{Theorem}[subsection]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\vect}{vec}

\graphicspath{{plots/}}
\newcommand{\matt}[1]{{\color{red} Matt: #1}}
\newcommand{\jarad}[1]{{\color{red} Jarad: #1}}

\begin{document}


\title{Ancillarity-Sufficiency or not; Interweaving to Improve Markov Chain Monte Carlo (MCMC) Estimation of the Dynamic Linear Model (DLM)}
\author{Matt Simpson}
\maketitle

\begin{abstract}
In DLMs, MCMC sampling can often be very slow for accurately estimating the posterior density --- especially for longer time series. In particular, in some regions of the parameter space the standard data augmentation algorithm can mix very slowly. Using some of the insights from the data augmentation for multilevel models literature, we explore several alternative data augmentations for a general class of DLMs and we show that no ``practical'' centered parameterization (or sufficient augmentation in the terminology of \citet{yu2011center}) exists. In addition, we utilize these augmentations to construct several interweaving algorithms --- though we cannot construct an ancillary-sufficient interweaving algorithm (ASIS) since no sufficient augmentation exists, we find two ancillary augmentations and are able to construct a componentwise interweaving algorithm (CIS) that uses ASIS for each model parameter conditional on the rest. Using the local level DLM, we show how to construct several of these algorithms and conduct a simulation study in order to discern their properties. We find that several algorithms that outperform the usual ``state sampler'' for many values of the population parameters, though there is room for improvement.
\end{abstract}


\section{Introduction}\label{sec:Intro}

\subsection{Data augmentations}

\subsection{Dynamic linear models} \matt{probably move this to section 2} 

\section{Estimating the Model via Data Augmentation: Parameterization Issues}\label{sec:DLMest}
\matt{Why not just talk about parameterizations in this section?}







\section{Interweaving in the DLM: Global and Componentwise}\label{sec:DLMinter}
\matt{I think the previous section should just be data augmentations with minimal discussion of sampling while this section should introduce sampling starting with the base samplers.}




\section{Application: The Local Level Model}\label{sec:LLMest}

 \matt{At some point we should comment that there is really no reason to prefer these priors other than they allow for a joint Gibbs step for V and W in the state sampler. We should also make sure we discuss alternative priors for the variances in the discussion. We could even mention that it is ongoing work.}

\matt{Consider putting the details of these samplers in the Appendix. What is really relevant here? Certainly the fact that in all but the state sampler, the steps for V and W are drawn conditionally and typically only one of the two will be a Gibbs step. The one that is not a Gibbs step probably deserves a subsection on its own since it comes up repeatedly (nevermind, putting the details in the appendix is good). There will also be he question of what happens to this step when we have more complicated models.}


Here $\theta_t=E[y_t|\theta_{0:T}]$. The states are $\theta_{0:T}$, the scaled disturbances are $\gamma_{0:T}$ with $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$, and the scaled errors are $\psi_{0:T}$ with $\psi_0=\theta_0$ and $\psi_t=(y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$. 

\subsection{Base Samplers}\label{sec:LLMbase}

\matt{Need a transition. What is a ``base sampler''?}

The joint density of $(V,W,\theta_{0:T},y_{1:T})$ is:
\begin{align}
  p(&V,W,\theta_{0:T},y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)} \exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \theta_{t})^2\right)\right]\nonumber\\
  &W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(\theta_t - \theta_{t-1})^2\right) \right] \exp\left[-\frac{1}{2C_0}(\theta_0 - m_0)^2\right]\label{llmstatejoint}
\end{align}


This immediately gives the state sampler:
\begin{alg}State Sampler for LLM.\label{alg:LLMstate}\\
  \begin{center}
    \begin{tabular}{lll}
      $[\theta_{0:T}|V^{(k)},W^{(k)}]$& $\to$& $[V^{(k+1)},W^{(k+1)}|\theta_{0:T}]$
    \end{tabular}
  \end{center}
\end{alg} \noindent


The scaled disturbance sampler, i.e. the DA algorithm based on the scaled disturbances, is a bit more complicated. In this context $\gamma_0=\theta_0$ and $\gamma_t=(\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$, and thus $\theta_t = \gamma_0 + \sqrt{W}\sum_{s=1}^t\gamma_s$ for $t=1,2,\cdots,T$. Following \eqref{dlmdistjoint}, we can write the joint posterior of $(V,W,\gamma_{0:T})$ as
\begin{align}
  p(&V,W,\gamma_{0:T}|y_{1:T}) \propto V^{-(\alpha_V + 1 + T/2)}\exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}\textstyle\sum_{t=1}^T(y_t - \gamma_0 - \sqrt{W}\textstyle\sum_{s=1}^t\gamma_s)^2\right)\right] \nonumber\\
  & \times W^{-(\alpha_W + 1)}\exp\left[-\frac{\beta_W}{W}\right] \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\gamma_t^2\right]\exp\left[-\frac{1}{2C_0}(\gamma_0-m_0)^2\right]\label{llmdistpost}
\end{align}
Now $V$ and $W$ are no longer conditionally independent given $\gamma_{0:T}$ and $y_{1:T}$. Instead of attempting the usual DA algorithm, we will add an extra Gibbs step and draw $V$ and $W$ separately primarily for ease of computation. This gives us the scaled disturbance sampler:
\begin{alg}Scaled Disturbance Sampler for LLM.\label{alg:LLMdist}\\
  \begin{center}
    \begin{tabular}{lllll}
      $[\gamma_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V^{(k+1)}|W^{(k)},\gamma_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$
    \end{tabular}
  \end{center}
\end{alg} \noindent
In step 2, $V$ is drawn from the same inverse gamma distribution as in step 2 of algorithm \ref{alg:LLMstate}. In step 3, the draw of $W$ is more complicated. The density can be written as
\begin{align*}
  p(W|V,\gamma_{0:T},y_{1:T}) \propto & W^{-\alpha_W - 1}\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \gamma_0 - \sqrt{W}\sum_{s=1}^t\gamma_s\right)^2\right]\exp\left[-\frac{\beta_W}{W}\right].
\end{align*}
This density is not any known form and is difficult to sample from, though its functional form is similar to the generalized inverse Gaussian distribution. The density can be written as
\begin{align*}
p(W|V,\gamma_{0:T},y_{1:T}) \propto& W^{-\alpha_W - 1}\exp\left[-aW + b\sqrt{W} -\frac{\beta_W}{W}\right]. 
\end{align*}
where $a=\sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$ and $b=\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$. It can be shown that $b > \left(\frac{(\alpha + 1)^3}{\beta}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}$ implies that the density is log concave --- see the appendix for details. It turns out that this tends to hold over a wide region of the parameter space --- so long as $V$ is smaller or is not much larger than $W$. This allows for the use of adaptive rejection sampling in order to sample from this distribution in many cases, e.g. using \citet{gilks1992adaptive}. An alternative is to use a $t$ approximation to the conditional density as a proposal in a rejection sampler. This is much more computationally expensive when necessary, but it works OK on the log scale.

The scaled error sampler is similar to the scaled disturbance sampler and this is easy to see in the local level model. Here $\psi_0=\theta_0$ and $\psi_t = (y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$ so that $\theta_t = y_t - \sqrt{V}\psi_t$ for $t=1,2,\cdots,T$. From \eqref{dlmerrorjoint} we can write $p(V,W,\psi_{0:T}|y_{1:T})$ as
\begin{align}
    p(&V,W,\psi_{0:T},y_{1:T}) \propto W^{-(\alpha_W + 1 + T/2)}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}\textstyle\sum_{t=1}^T(Ly_t - \sqrt{V}L\psi_t)^2\right)\right]\nonumber \\
 & V^{-(\alpha_V + 1)}\exp\left[-\frac{\beta_V}{V}\right]  \exp\left[-\frac{1}{2}\textstyle\sum_{t=1}^T\psi_t^2\right] \exp\left[-\frac{1}{2C_0}(\psi_0-m_0)^2 \right] \label{llmerrorjoint}
\end{align}
where we define $Ly_t=y_t-y_{t-1}$ for $t=2,3,\cdots,T$ and $Ly_1=y_1 - \psi_0$ and $L\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$ and $L\psi_1=\psi_1-0$. Once again, $V$ and $W$ are no longer conditionally independent given $\psi_{0:T}$ and $y_{1:T}$. In fact, the density is analogous to \eqref{llmdistpost} with $V$ and $W$ switching places. The scaled error sampler obtained from drawing $V$ and $W$ separately is:
\begin{alg}Scaled Error Sampler for LLM.\label{alg:LLMerror}\\
  \begin{center}
    \begin{tabular}{lllll}
    $[\psi_{0:T}|V^{(k)},W^{(k)}]$&$\to$&$[V^{(k+1)}|W^{(k)},\psi_{0:T}]$&$\to$&$[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$
    \end{tabular}
  \end{center}
\end{alg} \noindent
In step 3, $W$ is drawn from the same inverse gamma distribution as in step 2 of algorithm \ref{alg:LLMstate}. Drawing $V$ in step 2 is exactly analogous to drawing $W$ in algorithm \ref{alg:LLMdist}. The density of $V|W,\psi_{0:T},y_{1:T}$ can be written as
\begin{align*}
 p(V|W,\psi_{0:T},y_{1:T}) \propto V^{-\alpha_V - 1}\exp\left[ -aV + b\sqrt{V} -\frac{\beta_V}{V}\right] 
\end{align*}
where $a=\sum_{t=1}^T(L\psi_t)^2/2W$ and $b=\sum_{t=1}^T(L\psi_tLy_t)/W$, i.e. the form of the density is the same as that of $W|V,\gamma_{0:T},y_{1:T}$. 

We can also construct the DA algorithms based on the ``wrongly scaled'' disturbances or errors. The wrongly scaled disturbances are defined by $\tilde{\gamma}_t = \gamma_t\frac{\sqrt{W}}{\sqrt{V}}$ for $t=1,2,\cdots,T$ and $\tilde{\gamma}_0=\gamma_0$ while the wrongly scaled errors are defined by $\tilde{\psi}_t = \psi_t\frac{\sqrt{V}}{\sqrt{W}}$ for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\psi_0$. For $\tilde{\gamma}_{0:T}$ we have
\begin{align}
  p(V,W&|\tilde{\gamma}_{0:T},y_{1:T}) \propto W^{-\alpha_W - T/2 - 1}\exp\left[-\frac{1}{2W/V}\sum_{t=1}^T\tilde{\gamma}_t^2\right]\exp\left[-\frac{\beta_W}{W}\right]\nonumber\\
  &\times V^{-\alpha_V-1}\exp\left[-\frac{\beta_V}{V}\right]\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \tilde{\gamma}_0 - \sqrt{V}\sum_{s=1}^t\tilde{\gamma}_s\right)^2\right].\label{llmwdistpost}
\end{align}
Thus the conditional posterior of $W$ given $V$ and $\tilde{\gamma}_{0:T}$ is the same as if we had conditioned on $\theta_{0:T}$ instead of $\tilde{\gamma}_{0:T}$. In other words
\begin{align*}
  p(W|V,\tilde{\gamma}_{0:T},y_{1:T}) \propto W^{-(\alpha_W + T/2) - 1}\exp\left[-\frac{1}{W}\left(\beta_W + \frac{1}{2}V\sum_{t=1}^T\tilde{\gamma}_t^2\right)\right]
\end{align*}
so that $V|W,\tilde{\gamma}_{0:T},y_{1:T}\sim IG(a_W, b_W)$ where $a_W = \alpha_W + T/2$ and 
\begin{align*}
  b_W = \beta_W + \frac{1}{2}V\sum_{t=1}^T\tilde{\gamma}_t^2 = \beta_W + \frac{1}{2}\sum_{t=1}^T(\theta_t - \theta_{t-1})^2.
\end{align*}
The conditional posterior of $V$ is more complicated. We have
\begin{align*}
  p(V|W,\tilde{\gamma}_{0:T},y_{1:T}) &\propto \exp\left[-\frac{1}{2W/V}\sum_{t=1}^T\tilde{\gamma}_t^2\right] V^{-\alpha_V-1}\exp\left[-\frac{\beta_V}{V}\right]\exp\left[-\frac{1}{2V}\sum_{t=1}^T\left(y_t - \tilde{\gamma}_0 - \sqrt{V}\sum_{s=1}^t\tilde{\gamma}_s\right)^2\right]\\
  &\propto V^{-\alpha_V - 1}\exp\left[-\frac{a}{V} + \frac{b}{\sqrt{V}} - cV\right]
\end{align*}
where 
\begin{align*}
  a & = \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\gamma}_0)^2 > 0 &
  b & = \sum_{t=1}^T(y_t - \tilde{\gamma}_0)\sum_{s=1}^t\tilde{\gamma}_s &
  c & = \frac{1}{2W}\sum_{t=1}^T\tilde{\gamma}_t^2 > 0.
\end{align*}
We will return to this density momentarily.

For the wrongly scaled errors, we have
\begin{align}
  p(V,W&|\tilde{\psi}_{0:T},y_{1:T}) \propto V^{-\alpha_V - T/2 -1}\exp\left[-\frac{1}{2V/W}\sum_{t=1}^T\tilde{\psi}_t^2\right]\exp\left[-\frac{\beta_V}{V}\right]\nonumber\\
  &\times W^{-\alpha_W-1}\exp\left[-\frac{1}{2W}\sum_{t=1}^T\left(\tilde{Ly}_t - \sqrt{W}(\tilde{L\psi}_t)\right)\right]\label{llmwerrorpost}
\end{align}
where we define $L\tilde{y}_t = y_t - y_{t-1}$ for $t=1,2,\cdots,T$ and $L\tilde{y}_1=y_1 - \tilde{\psi}_0$, and $L\tilde{\psi}_t=\tilde{\psi}_t-\tilde{\psi}_{t-1}$ for $t=1,2,\cdots,T$ with $L\tilde{\psi}_1=\tilde{\psi}_1$. Then the conditional posterior of $V$ is the same as if we had conditioned on $\theta_{0:T}$ instead of $\tilde{\psi}_{0:T}$, i.e.
\begin{align*}
  p(V|W,\tilde{\psi}_{0:T},y_{1:T}) & \propto V^{-(\alpha_V - T/2)-1}\exp\left[-\frac{1}{V}\left(\beta_V + \frac{1}{2}W\sum_{t=1}^T\tilde{\psi}_t^2\right)\right]
\end{align*}
so that $V|W,\tilde{\psi}_{0:T},y_{1:T}\sim IG(a_V, b_V)$ where $a_V = \alpha_V + T/2$ and
\begin{align*}
  b_V = \beta_V + \frac{1}{2}W\sum_{t=1}^T\tilde{\psi}_t^2 = \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \theta_t)^2.
\end{align*}
The conditional posterior of $W$ is more complicated but similar to that of $V$ when we conditioned on $\tilde{\gamma}_{0:T}$. We have
\begin{align*}
  p(W|V,\tilde{\psi}_{0:T},y_{1:T}) &\propto \exp\left[-\frac{1}{2V/W}\sum_{t=1}^T\tilde{\psi}_t^2\right] W^{-\alpha_W-1}\exp\left[-\frac{1}{2W}\sum_{t=1}^T\left(L\tilde{y} - \sqrt{W}L\tilde{\psi}\right)\right]\\
  &\propto W^{-\alpha_W - 1}\exp\left[-\frac{a}{W} + \frac{b}{\sqrt{W}} - cW\right]
\end{align*}
where now
\begin{align*}
  a & = \beta_W + \frac{1}{2}\sum_{t=1}^TL\tilde{y}_t^2 > 0 & 
  b & = \sum_{t=1}^TL\tilde{y}_tL\tilde{\psi}_t &
  c & = \frac{1}{2V}\sum_{t=1}^T\tilde{\psi}_t^2 > 0.
\end{align*}

So in the case of both wrongly scaled DAs we need to sample from a density of the form
\begin{align*}
  p(x) \propto x^{-\alpha -1}\exp\left[-\frac{a}{x} + \frac{b}{\sqrt{x}} - cx\right].
\end{align*}
The density of $y=\log(x)$ is 
\begin{align*}
  p(y) \propto \exp\left[-\alpha y - ae^{-y} + be^{-y/2} - ce^y\right].
\end{align*}
This density is easy to sample from fairly efficiently with rejection sampler using a $t$ approximation as a proposal. The details of this sampler are in the appendix \matt{reference}.

\matt{Need another transition: foreshadow the results (and reference?) saying ``these samplers will work well in different regions of the parameter space and therefore combining multiple base samplers via interweaving or alternating amongst these base samplers should improve mixing over the entirety of the parameter space.}



\subsection{Hybrid Samplers: Interweaving and Alternating}
Section \ref{sec:DLMinter} contains the details for the interweaving algorithms in the general DLM. In the local level model the only difference \matt{from what?} is that we only sample $V$ and $W$ jointly when we condition on the states. We will consider all four GIS samplers based on any two or three of the base samplers and one CIS sampler \matt{It is definitely not clear what you are referring to here.}. In the GIS samplers, the order of the parameterizations \matt{steps?} will always be the states $(\theta_{0:T})$, then the scaled disturbances $(\gamma_{0:T})$, then the scaled errors $(\psi_{0:T})$. All of the GIS algorithms and the Full CIS algorithm are below in Table \ref{GISalgorithms}. Note the distributional forms for each of these steps (in some cases a transformation) are in Section \ref{sec:LLMbase}. We omit the partial CIS algorithm, though note that in practice it behaves essentially the same as the State-Dist algorithm \matt{results not shown?}.
\begin{table}[!h]
  \centering
\begin{enumerate}
  \item State-Dist GIS algorithm:\\
    \begin{center}
      \begin{tabular}{lllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V,W|\theta_{0:T}]$& $\to$&\\ 
        $[\gamma_{0:T}|V,W,\theta_{0:T}]$& $\to$& $[V^{(k+1)}|W,\gamma_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$
        \end{tabular}
    \end{center}
  \item State-Error GIS algorithm:\\
    \begin{center}
      \begin{tabular}{lllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$&$[V,W|\theta_{0:T}]$& $\to$&\\ 
        $[\psi_{0:T}|V,W,\theta_{0:T}]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$
      \end{tabular}
    \end{center}
  \item Dist-Error GIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllll}
        $[\gamma_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V|W^{(k)}, \gamma_{0:T,}y_{1:T}]$& $\to$& $[W|V, \gamma_{0:T}]$& $\to$\\ 
        $[\psi_{0:T}|V,W,\gamma_{0:T}]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$&
      \end{tabular}
    \end{center}
  \item Triple GIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V,W|\theta_{0:T}]$& $\to$&& \\
        $[\gamma_{0:T}|V,W,\theta_{0:T}]$& $\to$& $[V|W,\gamma_{0:T}]$& $\to$& $[W|V,\gamma_{0:T}]$ & $\to$\\
        $[\psi_{0:T}|V,W,\gamma_{0:T}]$& $\to $&$[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$&
      \end{tabular}
    \end{center}
  \item Full CIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$& $\to$& $[V|W^{(k)},\theta_{0:T}]$& $\to$& $[\psi_{0:T}|V,W,\theta_{0:T}]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$\\ 
        $[\theta_{0:T}|V^{(k+1)},W]$& $\to$& $[W|V^{(k+1)},\theta_{0:T}]$& $\to$& $[\gamma_{0:T}|V^{(k+1)},W]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$&
    \end{tabular}
\end{center}
\end{enumerate}
\caption{GIS and CIS algorithms for the local level model}
\label{GISalgorithms}
\end{table}

Interweaving algorithms are conceptually very similar to alternating algorithms. For every GIS algorithm, there is a corresponding alternating algorithm  where each $[DA_2|V,W,DA_1]$ step is replaced by a $[DA_2|V,W]$ step (here $DA_i$ is a data augmentation for $i=1,2$.). Table \ref{altalgorithms} contains each alternating algorithm. Note that there are two possible ``hybrid triple'' algorithms that we do not consider here where the move from $\theta_{0:T}$ to $\gamma_{0:T}$ interweaves and while the move from $\gamma_{0:T}$ to $\psi_{0:T}$ alternates and vice-versa.
\begin{table}[!h]
  \centering
\begin{enumerate}
  \item State-Dist alternating algorithm:\\
    \begin{center}
      \begin{tabular}{lllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V,W|\theta_{0:T}]$& $\to$& \\
        $[\gamma_{0:T}|V,W]$& $\to$& $[V^{(k+1)}|W,\gamma_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$
      \end{tabular}
    \end{center}
  \item State-Error alternating GIS algorithm:\\
    \begin{center}
      \begin{tabular}{lllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V,W|\theta_{0:T}]$& $\to$& \\
        $[\psi_{0:T}|V,W]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$
      \end{tabular}
    \end{center}
  \item Dist-Error alternating GIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllll}
        $[\gamma_{0:T}|V^{(k)},W^{(k)}]$&$\to$& $[V|W, \gamma_{0:T}]$& $\to$& $[W|V, \gamma_{0:T}]$& $\to$\\
        $[\psi_{0:T}|V,W]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$&
      \end{tabular}
    \end{center}
  \item Triple alternating GIS algorithm:\\
    \begin{center}
      \begin{tabular}{llllll}
        $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$&$ [V,W|\theta_{0:T}]$&$ \to$&&\\
        $[\gamma_{0:T}|V,W]$&$ \to$&$ [V|W,\gamma_{0:T}]$&$ \to$&$ [W|V,\gamma_{0:T}]$& $\to$\\
        $[\psi_{0:T}|V,W]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$&
      \end{tabular}
    \end{center}
\end{enumerate}
\caption{Alternating algorithms for the local level model}
\label{altalgorithms}
\end{table}

Table \ref{LLMalgorithms} lists each algorithm we considered for the local level model. The basic idea here is that the alternating algorithms should serve as a sort of baseline to compare the corresponding interweaving algorithms against. The GIS version of an algorithm at least be similar to the alternating version since both take advantage of the fact that when at least one of the underlying DA algorithms works well, the hybrid algorithm should also work well. When the two DAs are (nearly) independent in the posterior, it is plausible that the GIS version of the algorithm would improve upon the results of the alternating version. Since we do not have such a pair of DAs, it seems unlikely that there will be much of a difference between the GIS and alternating versions of a given algorithm. We can make the comparisons fairly precise by considering the effective sample size (ESS) of the Markov chain -- given the same sample size for two algorithms, we can compare ESS to see which one has better mixing. \matt{I wouldn't introduce ESS here, but rather put after describing the simulation setup.}

\begin{table}[!ht]
  \centering
  \begin{tabular}{|l||l|l|l|l|}\hline
    Base & State & (wrongly) Scaled Disturbance & (wrongly) Scaled Error & \\\hline
    GIS & State-Dist & State-Error & Dist-Error & Triple \\
    Alt & State-Dist & State-Error & Dist-Error & Triple \\
    CIS & \multicolumn{3}{l|}{State-Error for $V|W$; State-Dist for $W|V$} & \\
    \hline
  \end{tabular}
  \caption{Each algorithm considered for the local level model}
  \label{LLMalgorithms}
\end{table}



\matt{Do we really want to show results for different T? I feel like we can just show results for a single T and mention that the patterns observed are less extreme for smaller T and more extreme for larger T, although perhaps showing this once would be good. Otherwise we could also put this in the appendix or supplemental material.}







Figure \ref{baseESplot} tells a different story for the scaled disturbance sampler. When $R<1$, ESPs for both $V$ and $W$ are nearly 1, i.e. the effective sample size is nearly the actual sample size of the chain. When $R>1$, ESP for both $V$ and $W$ becomes small, especially for $V$. Once again the absolute values of $V$ and $W$ do not matter for this behavior --- just their ratio. The scaled error sampler has essentially the opposite properties. When $R$ is large, it has a near 1 ESP for both $V$ and $W$. On the other hand, when $R$ is small is has a low ESP for both $V$ and $W$, especially for $V$. The lesson here seems to be that the scaled disturbances are the preferred data augmentation for low signal-to-noise ratios and the scaled errors  are the preferred data augmentation for high signal-to-noise ratios, while the states are preferred for signal-to-noise ratios near 1. The wrongly scaled disturbances ($\tilde{\gamma}_{0:T}$) and wrongly scaled errors ($\tilde{\psi}_{0:T}$), on the other hand, look like worse versions of the state sampler. The pattern of mixing for $V$ and $W$ over the range of the parameter space is essentially the same as the state sampler, except the wrongly scaled disturbance sampler has worse mixing for $V$ than the state sampler almost everywhere and similarly the wrongly scaled error sampler has worse mixing for $W$ than the state sampler almost everywhere.

The plots for $T=1000$ in Figure \ref{baseESplot} tell basically the same story, with a twist. Increasing the length of the time series seems to exacerbate all problems without changing the basic conclusions. As $T$ increases, $R$ has to be smaller and smaller for the scaled disturbance sampler to have decent mixing, and similarly $R$ has to be larger and larger for the scaled error sampler to have decent mixing. Interestingly, the scaled error sampler appears to mix well for both $V$ and $W$ over a larger region of the space $R<1$ than the scaled disturbance sampler does over $R>1$. The state sampler is stuck between a rock and a hard place, so to speak, since as $T$ increases, good mixing for $V$ requires $R$ to be smaller and smaller, but good mixing for $W$ requires $R$ to be larger and larger. The wrongly scaled samplers are again pretty similar to the state sampler for larger $T$ except the wrongly scaled sampler tends to be worse everywhere for the variance that was used to scale --- i.e. once again the wrongly scaled disturbance sampler has worse mixing for $V$ than the state sampler while the wrongly scaled error sampler has worse mixing for $W$ than the state sampler. However, the wrongly scaled samplers do appear to have slightly better mixing than the state sampler for the variance that was {\it not} used to scale. In particular, the wrongly scaled error sampler appears to have slightly better mixing for $V$ than the state sampler over part of the parameter space. 

The behavior of the wrongly scaled data augmentation algorithms is consistent with what we showed in Section \ref{sec:DLMinter} --- that the Full CIS algorithm based on the scaled errors and disturbances and the wrongly scaled errors and disturbances is equivalent to the Full CIS algorithm that replaces the wrongly scaled DAs with the usual latent states. Since the behavior of the state sampler and the wrongly scaled disturbance sampler are the same for $W$, we might expect that when drawing $W$ it does not matter whether we use the states or the wrongly scaled disturbances in the Full CIS algorithm. Similarly since the behavior of the state sampler and the wrongly scaled error sampler are the same for $V$, we might expect that it does not matter which one we use when drawing $V$. In fact this is what we found when constructing the Full CIS algorithm. Even though $(\gamma_{0:T}, \tilde{\gamma}_{0:T})$ forms an AA-SA pair for $W|V$ and $(\psi_{0:T},\tilde{\psi}_{0:T})$ forms an AA-SA pair for $V|W$ while $\theta_{0:T}$ is not an SA for $V$, replacing $\tilde{\gamma}_{0:T}$ and $\tilde{\psi}_{0:T}$ with $\theta_{0:T}$ does not actually change the CIS algorithm.

We summarize some of the above results for convenience in Table \ref{tab:stnmix}.
\begin{table}
  \centering
  \begin{tabular}{|l|ccccc|}\hline
    Parameter & State & Dist & Error & W-Dist & W-Error \\\hline
    V & $R < 1$ & $R < 1$ & $R > 1$ & $R < 1$ & $R < 1$\\
    W & $R > 1$ & $R < 1$ & $R > 1$ & $R > 1$ & $R > 1$ \\\hline
  \end{tabular}
  \caption{Rule of thumb for when each base algorithm has a high effective sample size for each variable as a function of the true signal-to-noise ratio, $R=W/V$.}
  \label{tab:stnmix}
\end{table}

Most of the patterns of Figure \ref{baseESplot} and Table \ref{tab:stnmix} can be explained by Figure \ref{corplot}, which contains the estimated posterior correlations between various functions of parameters estimated using the simulations from the Triple-Alternating sampler. First we need to understand the correlations we are looking at. \matt{$\leftarrow$ This is a throw-away sentence and needs to be eliminated.} The state sampler consists of two steps --- a draw of $\theta_{0:T}$ given $V$ and $W$, and a draw of $(V,W)$ given $\theta_{0:T}$. From Section \ref{sec:LLMbase} we have that conditional on $\theta_{0:T}$, $V$ and $W$ are independent in the posterior and each has an inverse gamma distribution that depends on the states only through the second parameter:
\begin{align*}
  b_V &= \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2 &
  b_W &= \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2.
\end{align*}
So we can view $(b_V,b_W)$ as the data augmentation instead of $\theta_{0:T}$ and thus the state sampler is
\begin{align*}
  [b_V, b_W|V^{(k)},W^{(k)}] \to [V^{(k+1)},W^{(k+1)}|b_V,b_W].
\end{align*}
Thus the dependence between $(V,W)$ and $(b_V,b_W)$ in the posterior will determine how much the state sampler moves in a given iteration and, in particular, it is possible that $V$ and $W$ have very different serial dependence from each other since we are drawing them jointly. When the dependence between $V$ and $b_V$ is high, the $(V,W)$ step will hardly move $V$ even if it drastically moves $W$ since $V$ and $W$ are independent. However, the $(b_V,b_W)$ step may move both elements a moderate amount since they both depend on $(V,W)$.

For the scaled disturbance sampler, things are a bit more complicated. The draw of $V|W,\gamma_{0:T}$ still depends on $b_V$ since it is the same inverse gamma draw as in the state sampler, but the draw of $W|V,\gamma_{0:T}$ now depends on $a_\gamma$ and $b_\gamma$ defined below.
\begin{align*}
  b_V &= \beta_V + \frac{1}{2}\sum_{t=1}^T\left(y_t - \gamma_0 - \sqrt{W}\sum_{s=1}^t\gamma_{s}\right)^2 = \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \theta_t)^2\\
  a_\gamma & = \sum_{t=1}^T\left(\sum_{j=1}^t\gamma_j\right)^2/2V\\
  b_\gamma &=\sum_{t=1}^T(y_t-\gamma_0)\left(\sum_{j=1}^t\gamma_j\right)/V.
\end{align*}
This again comes from Section \ref{sec:LLMbase}. So the dependence between $V$ and $b_V$ determines how much the chain moves in the $V$ step, and the dependence between $W$ and $(a_\gamma , b_\gamma)$ determines how much it moves in the $W$ step. The dependence between $(V,W)$ and $\gamma_{0:T}$ determines how much the chain moves in the DA step, but we can view this step instead as a draw of $b_V$ in which case the dependence between $W$ and $b_V$ determines how much the chain moves. So if any one of these steps has high dependence, we should expect every element of the chain, and $(V,W)$ in particular, to have high serial dependence in the chain. The scaled error sampler is analogous to the scaled disturbance sampler except with 
\begin{align*}
  b_W &= \beta_W + \frac{1}{2}\sum_{t=2}^T\left(Ly_t - \sqrt{V}L\psi_t\right)^2 = \beta_W + \frac{1}{2}\sum_{t=1}^T(\theta_t - \theta_{t-1})^2\\
  a_\psi&=\sum_{t=1}^T(L\psi_t)^2/2W\\
  b_\psi&=\sum_{t=1}^T(L\psi_tLy_t)/W.
\end{align*}

In Figure \ref{corplot} we see that the posterior correlation between $V$ and $b_V$ is high in magnitude and positive when $R>1$ while the posterior correlation between $V$ and $b_W$ is high and negative. When $R$ is large enough though, the posterior correlation between $V$ and $b_W$ appears to evaporate, especially for smaller $T$. Similarly when $R<1$ the posterior correlation is high and positive between $W$ and $b_W$ and the posterior correlation between $W$ and $b_V$ is high and negative. Again as $R$ becomes large enough, especially for small $T$, the correlation between $W$ and $b_V$ goes to zero. We can use these results to explain the behavior of the state sampler. When $R>1$, the draw of $(b_V, b_W)$ is unlikely to move $b_V$ much since $b_V$ is so highly correlated with $V$ and essentially uncorrelated with $b_W$, but $b_W$ is essentially uncorrelated with $W$ and negatively correlated with $V$ so $b_W$ is likely to move a fair amount. Furthermore the draw of $V$ is highly correlated with $b_V$ while the draw of $W$ is essentially independent of $b_W$ (and the draws of $V$ and $W$ are independent conditional on $b_V$ and $b_W$). Thus when $R>1$ we should expect high serial dependence for $V$ and low serial dependence for $W$, and so low ESP for $V$ and high ESP for $W$, which is exactly what we see in Figure \ref{baseESplot}. Similarly when $R<1$ the posterior correlation between $V$ and $b_V$ is low, the correlation between $W$ and $b_W$ is high, the correlation between $V$ and $b_W$ is low, and the correlation between $W$ and $b_V$ is high but negative. So we should expect the exact opposite behavior --- low serial dependence for $V$ and high serial dependence for $W$ and thus high ESP for $V$ and low ESP for $W$. Again, this is exactly what we see in Figure \ref{baseESplot}.

In order to analyze the scaled disturbance sampler, we can look at each step separately and combine them together. First, suppose $R>1$. Then from Figure \ref{corplot} $b_V$ has high correlation with $V$ and low correlation with $W$, so the draw of $b_V$ should not move the chain much. Next, the draw of $V$ should again not move the chain much because of the high correlation between $V$ and $b_V$. Finally the draw of $W$ has a fair chance to move the chain because it has low correlation with both $a_\gamma$ and $b_\gamma$. But this has little impact on $b_V$ and thus the entire chain since $b_V$ is so highly correlated with $V$ but hardly correlated with $W$. So when $R>1$, we should expect high serial dependence and low ESP for $V$. We should also expect similar behavior for $W$ since the entire chain is hardly moving so $W$'s hyperparameters are hardly moving, though it makes sense for this dependence to be a bit weaker since we see such low correlation between $W$ and both $a_\gamma$ and $b_\gamma$. This is close to what we see, expect when $R>1$ the serial dependence is worse for $W$ rather than for $V$, as evidences by the high ESP for $W$ and high but slightly lower ESP for $V$ in Figure \ref{baseESplot}. When $R<1$ the posterior correlation in each of the steps is broken though in the $W$ step the correlation between $W$ and both $a_\gamma$ and $b_\gamma$ becomes negative and somewhat high. Here we should expect much less serial dependence in both $V$ and $W$ and indeed, we see ESP's near one for both variances.

The scaled error sampler is analogous to the scaled disturbance sampler. When $R<1$ we see high posterior correlation between $W$ and $b_W$ which suggests that the $W$ step will not move the chain much. The $V$ step could move the chain a bit since $V$ is essentially uncorrelated with $a_\psi$ and $b_\psi$, but since $b_W$ is essentially uncorrelated with $V$ and highly correlated with $W$ this is unlikely to affect the entire chain much since both $b_W$ and $W$ are unlikely to move much. As a result we should see high serial dependence in the chain for $W$ and high though a bit lower serial dependence for $V$. This is close to what we see if Figure \ref{baseESplot} --- when $R<1$, the scaled error sampler has low ESP for both $V$ and $W$, an indication of high serial dependence in the chain. However, the ESP is lower and thus the serial dependence is higher for $V$ than it is for $W$, much like the case for the scaled disturbance sampler. When $R>1$, Figure \ref{corplot} shows low correlation between $W$ and $b_W$, which suggests that the $W$ step will be able to amply move the chain. Similarly, the correlation between $V$ and $b_W$ is mildly high but negative so the $b_W$ step, just before the $V$ step, should move the chain fairly well. Finally the correlation between $V$ and both $a_\psi$ and $b_\psi$ is mildly high and positive with $R>1$ --- and especially high when $T$ is large. As a result, when $R>1$ we should expect minimal serial dependence in the scaled error sampler for both $V$ and $W$, which is what we see with the high ESP's in Figure \ref{baseESplot}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{baseESplot1}
\includegraphics[width=0.7\textwidth]{baseESplot2}
\caption{Effective sample proportion in the posterior sampler for a time series of lengths $T=100$ and $T=1000$, for $V$ and $W$, and for the state, scaled disturbance, scaled error, wrongly scaled disturbance, and wrongly scaled error samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Note that for plotting purposes, effective sample proportions larger than $1$ were rounded down to $1$.}
\label{baseESplot}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.24\textwidth]{corplot1}
\includegraphics[width=0.24\textwidth]{corplot2}
\includegraphics[width=0.24\textwidth]{corplot3}
\includegraphics[width=0.24\textwidth]{corplot4}
\includegraphics[width=0.24\textwidth]{corplot5}
\includegraphics[width=0.24\textwidth]{corplot6}
\includegraphics[width=0.24\textwidth]{corplot7}
\includegraphics[width=0.24\textwidth]{corplot8}
\caption{Posterior correlation between $V$ or $W$ and $b_V$ or $b_W$. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high.}
\label{corplot}
\end{figure}

\subsection{GIS and CIS Results}

Based on the intuition in Section \ref{sec:DLMest} above, both the GIS and alternating algorithms should work best when at least one of the underlying base algorithms has a high ESP --- the basic idea is that when least one of the underlying algorithms has low serial dependence in the chain, the we should have low serial dependence in the GIS and alternating algorithms since the serial dependence in the bad DA algorithm is broken by adding a lower dependence step before the next iteration. This suggests that the Dist-Error GIS and alternating algorithms will have the best performance of the GIS and alternating algorithms using two DAs for both $V$ and $W$, especially for $R$ far away from one. When $R$ is near one it may offer no improvement however, especially for large $T$. The State-Dist algorithms should have trouble with $V$ when $R$ is high since both the state sampler and the scaled disturbance sampler have trouble with $V$ when $R$ is high. Similarly, the State-Error GIS algorithm should have trouble with $W$ when $R$ is low since both underlying samplers have trouble with $W$ when $R$ is low. Since the triple algorithms add the state sampler into the Dist-Error algorithms, it seems plausible that it might improve mixing for one of $V$ or $W$ since for $R$ different from one, the state sampler has good mixing for at least one of $V$ of $W$. 

There are a couple of ways to gain some intuition about what we expect the Full CIS algorithm to do before seeing the results. First, we mentioned in Section \ref{sec:DLMinter} that the Full CIS and the Dist-Error GIS algorithm consist of the same steps, just rearranged. This suggests that they should perform similarly so that we expect the Full CIS algorithm to have good mixing for both $V$ and $W$ when $R$ is sufficiently different from one. We can draw the same conclusion in a different way by noticing that in the Gibbs step for $V$, the CIS algorithm interweaves between the states and the scaled errors and in the Gibbs step for $W$ it interweaves between the states and the scaled disturbances. Since the state sampler has a high ESP for $V$ when $R<1$ and the scaled disturbance sampler has a high ESP for $V$ when $R>1$ we should expect the Full CIS sampler to have a high ESP for $V$ when $R$ is different from one. Similarly, since the state sampler has a high ESP for $W$ when $R>1$ and the scaled error sampler has a high ESP for $W$ when $R<1$, we should expect the Full CIS sampler to have a high ESP for $W$ when $R$ is different from one.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{intESplot1}
\includegraphics[width=0.7\textwidth]{intESplot2}
\caption{Effective sample proportion in the posterior sampler for $V$ and $W$ in for $T=100$ and $T=1000$, in all three GIS samplers based on any two of the base samplers and the full CIS sampler. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Note that for plotting purposes, effective sample proportions larger than one were rounded down to one.}
\label{intESplot}
\end{figure}



When the underlying DA algorithms form a ``beauty and the beast'' pair, the interweaving algorithm appears to mix just as well as the best mixing single DA algorithm. 

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{altESplot1}
\includegraphics[width=0.7\textwidth]{altESplot2}
\caption{Effective sample proportion in the posterior sampler for $V$ and $W$ in for $T=100$ and $T=1000$, in all four alternating samplers. Horizontal and vertical axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high. Note that for plotting purposes, effective sample proportions larger than one were rounded down to one. \matt{$\leftarrow$ No need to repeat this.}}
\label{altESplot}
\end{figure}





\section{Discussion}\label{sec:Discussion}
Our results on computational time in Section \ref{sec:LLMtime} should be taken with a grain of salt because we did not put much effort \matt{I think you did put some effort into this. Instead, just describe this as the rate limiting step.} into efficiently sampling from $p(W|V,\gamma_{0:T},y_{1:T})$ or $p(V|W,\psi_{0:T},y_{1:T})$. Both densities have the form
\[
p(x)\propto x^{-\alpha-1}\exp\left[-ax + b\sqrt{x} - \frac{\beta}{x}\right]
\]
where $\alpha,\beta,a>0$. This density is the same as the generalized inverse Gaussian distribution (see e.g. \citet{jorgensen1982statistical} for its properties; \citet{dagpunar1989easily} and \citet{devroye2012random} for generating random draws) when $b=0$, this is almost surely not the case in our application. It is possible that sampling from this density can be significantly improved in which case, the relative speed of the algorithms based on either the scaled errors or the scaled disturbances will improve significantly. On the other hand, it might be worth putting effort into drawing $V$ and $W$ jointly conditional on the scaled disturbances or the scaled errors. The conditional distribution of $V$ given $W$, $p(V|W,\gamma_{0:T},y_{1:T})$, is inverse gamma in our example and inverse Wishart in general, so it is easy to derive the marginal density $p(W|\gamma_{0:T},y_{1:T})$. In our local level model example, this density turns out to be very difficult to sample from and, in particular, is not easily approximated for rejection sampling or for a Metropolis step. 

We initially chose inverse Wishart priors for $V$ and $W$ partially because they are standard and partially for computational convenience, i.e. full Gibbs steps in the state sampler. There are well known problems with this prior \matt{Can reference my paper with Ignacio} in the hierarchical model literature, e.g. \citet{gelman2006prior}, though less is known about the time series case. Given that the prior has potential problems and is inconvenient for the scaled disturbances and the scaled errors, a better choice of prior is probably out there. \matt{$\leftarrow$ a throwaway sentence.} In particular, the conditionally conjugate prior for $\sqrt{W}$ using the scaled disturbances as the DA is a Gaussian distribution --- strictly speaking this prior is on $\pm \sqrt{W}$. If we use this prior for $\pm\sqrt{V}$ as well, the $V$ step in the Gibbs sampler becomes a draw from the generalized inverse Gaussian distribution. This prior has been used by \citet{fruhwirth2011bayesian} and \citet{fruhwirth2008bayesian} to speed up computation while using the scaled disturbances in hierarchical models and by \citet{fruhwirth2010stochastic} for time series models with a DA similar to the scaled disturbances (the latent states are scaled, but not centered). This prior seems particularly useful for variable selection problems, as evidenced by the papers which use it. We omit the results here, but using this prior does not alter our mixing results --- effective sample sizes were basically the same with either prior. There is a trade-off in computation time to consider, however. For example when using the scaled disturbances, the draw of $W|V,\gamma_{0:T},y_{1:T}$ is sped up by using the Gaussian prior on $\pm\sqrt{W}$ since it becomes a Gaussian draw while the $V|W,\gamma_{0:T},y_{1:T}$ is slower since it becomes a generalized inverse Gaussian draw instead of an inverse gamma. The gains outweigh the costs at least until a better way of sampling from $W$'s full conditional from the inverse gamma case is invented, but this only applies when $V$ is a scalar. When $V$ is a matrix, its full conditional becomes the matrix analogue of the generalized inverse Gaussian distribution and it is not clear how efficiently this can be sampled.

In our simulations with the original inverse gamma priors and the with the normal prior on the standard deviations we mentioned above, we varied the prior so that the prior mean was the true value of the parameters used to simulate the datasets. This may seem suspect at first glance, but there is a method to our madness. In the data augmentation for multilevel models literature, a key quantity is called the fraction of missing information (\citet{van2001art}, for example). When $\phi$ is the model parameter, $\theta$ is the data augmentation and $y$ is the data, the Bayesian fraction of missing information is defined as
\begin{align*}
  \mathcal{F}_B = I - [var(\phi|y)]^{-1}E[var(\phi|\theta,y)|y]
\end{align*}
while the EM fraction of missing information is defined as
\begin{align*}
  \mathcal{F}_{EM} = I - I_{obs}I_{aug}^{-1}
\end{align*}
where 
\begin{align*}
  I_{aug}=& \left.\mathrm{E} \left[-\left.\frac{\partial^2 \log p(\phi|\theta,y)}{\partial \phi \dot \partial \phi}\right| y,\phi\right]\right|_{\phi=\phi^*}\\
  \intertext{is the expected augmented Fisher information matrix and}
  I_{obs} =& -\left.\frac{\partial^2\log p(\phi|y)}{\partial\phi \dot \partial\phi}\right|_{\phi=\phi^*}
\end{align*}
is the observed Fisher information matrix while $\phi^*$ is the posterior mode. The rate of convergence of the EM algorithm is governed by $\mathcal{F}_{EM}$ while the maximum lag-1 autocorrelation in the Gibbs sampler for any linear function of the model parameters is governed by $\mathcal{F}_{B}$ --- the larger the spectral radius of $\mathcal{F}$, the high the autocorrelation. While $\mathcal{F}_{B}$ is difficult to compute, $\mathcal{F}_{EM}$ is often easier and is a decent approximation to $\mathcal{F}_{B}$ to the degree that the posterior distribution is Gaussian. We currently cannot analytically compute either of these quantities in our model, but the significance of the signal-to-noise ratio in our results is likely related. In particular, the EM fraction of missing information requires the expected and observed information matrices at the posterior mode. So the behavior of the samplers likely depends on the posterior model signal-to-noise ratio, or perhaps the ratio of the posterior model signal to the posterior mode ratio (depending on whether we take the mode of $R$, of $V$ and $W$ separately or of $V$ and $W$ together). Given the way we choose our priors, the posterior mode of $(V,W)$ is likely to be close to the true values of $V$ and $W$ used to simulate the data, especially for longer time series. If we used the same prior for each simulation, the posterior mode and the true values of $V$ and $W$ are less likely to be close for some true values of $V$ and $W$. In fact, in simulations with a constant prior (details not reported here), plots such as Figure \ref{baseESplot} look somewhat similar but with a much less stark difference in ESP over different regions of the parameter space.

Some previous work has been done in choosing data augmentations for time series models. In the AR(1) plus noise model, \citet{pitt1999analytic} find that the signal-to-noise ratio along with the AR(1) coefficient determine the convergence rate of a Gibbs sampler. In addition, they find that as the length of the time series increases, the convergence rate slows down and compute asymptotic convergence rates for an infinite time series. Both of these results are consistent with our findings and further suggest that the signal-to-noise ratio will play a key role in the fraction of missing information, Bayesian or EM, however, \citet{pitt1999analytic} assume that both of the variances are fixed for simplicity. In a continuous time model, \citet{roberts2004bayesian} find that the Gibbs sampler based on a NCP is at least as efficient as the Gibbs sampler based on the CP and sometimes it is much more efficient, depending on the true values of the parameters. It is unclear whether the time series dependence or something unique to the model is driving this, but it is interesting that the NCP is essentially always better. The dynamic regression model with a stationary AR(1) process on the regression coefficient has been studied in \citet{fruhwirth2004efficient}. They use both the states and the scaled disturbances and several other DAs motivated by some results for Gibbs samplers in the hierarchical model literature. When they examine the behavior of the resulting DA algorithms, \citet{fruhwirth2004efficient} find that the relative behavior of the scaled disturbance (in their language, the ``noncentered disturbances'') and state samplers are similar to our own results in the local level model, though now the signal-to-noise ratio is no longer the crucial quantity, but rather some function of it that also depends on the distribution of the covariate and the autocorrelation parameter in some currently unknown way. \citet{fruhwirth2004efficient} also find that none of the other DA algorithms they consider are more efficient than both the state sampler and the scaled disturbance sampler --- one of these always ends up being faster. This holds true both when they assume that the autorcorrelation parameter is known but when it is assumed unknown. This is encouraging since the extra complexity from adding a new parameter to the model (in our general DLM notation, this is in the form of $G_t$ depending on an additional unknown parameter) could drastically change the properties of the DA algorithms based on an AA (e.g. the scaled disturbances), but it appears the differences are relatively small.

\section{Appendix}\label{sec:append}

\subsection{Efficiently drawing from $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$}\label{sec:Append:scale}
Both of these two densities are of the form
\begin{align*}
\log p(x) = lp(x) & -ax + b\sqrt{x} - (\alpha + 1)\log x -\beta/x + C 
\end{align*}
for $x>0$ where $C$ is some constant, $\alpha$ and $\beta$ are the hyperparameters for $x$, and $a>0$ and $b\in \Re$ are parameters that depend on the data, $y$, the relevant data augmentation ($\psi$ or $\gamma$), and the other variable ($W$ or $V$). This density is not a known form and is difficult to sample from. We provide two different rejection sampling strategies below that work well under different circumstances, and combine them into a single strategy.

\subsubsection{Adaptive rejection sampling}
One nice strategy is to use adaptive rejection sampling, e.g. \citet{gilks1992adaptive}. This requires $lp(x)$ to be concave, which is easy enough to check. The second derivative of $lp(x)$ is:
\begin{align*}
\frac{\partial^2 lp(x)}{\partial x^2} &= -\frac{1}{4}bx^{-3/2} +(\alpha + 1)x^{-2} -2 \beta x^{-3}.
\end{align*}
Then we have
\begin{align*}
  &\frac{\partial^2 lp(x)}{\partial x^2} < 0 \iff \\
  &-\frac{b}{4}x^{3/2} + (\alpha + 1)x - 2\beta < 0
\end{align*}
which would imply that $lp(x)$ is concave. We can maximize the left hand side of the last equation very easily. When $b\leq 0$ the max occurs at $x=\infty$ such that $LHS > 0$, but when $b > 0$:
\begin{align*}
  \frac{\partial LHS}{\partial x} &= -\frac{3}{8}bx^{1/2} + \alpha + 1 = 0\\
  \implies & x^{max} = \frac{(\alpha + 1)^2}{b^2}\frac{64}{9}.
\end{align*}
Then we have
\begin{align*}
  LHS \leq LHS|_{x=x^{max}} = \frac{(\alpha + 1)^3}{b^2}\frac{64}{27} - 2\beta
\end{align*}
so that
\begin{align*}
  LHS|_{x=x^{max}} < 0 &\iff  \frac{(\alpha + 1)^3}{b^2}\frac{64}{27} < 2\beta\\
    &\iff b > \left(\frac{(\alpha + 1)^3}{\beta}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}.
\end{align*}
This last condition is necessary and sufficient for $lp(x)$ to be globally (for $x>0$) concave since $b < 0$ forces $LHS > 0$ for some $x$. When the condition is satisfied, we can use adaptive rejection sampling --- which is already implemented in the \verb0R0 package \verb0ars0. We input the initial evalutions of $lp(x)$ at the mode $x^{mode}$ and at $2x^{mode}$ and $0.5x^{mode}$ in order to get the algorithm going.

\subsubsection{Rejection sampling on the log scale}
When $b \leq \left(\frac{(\alpha + 1)^3}{\beta}\right)^{1/2}\frac{4\sqrt{2}}{3\sqrt{3}}$, which happens often --- especially for small $T$ --- we need to rely on a different method to sample from $p(x)$. A naive approach would be to construct a normal or $t$ approximation to $p(x)$ and use that as a proposal in a rejection sampler. It turns out that this is often very inefficient, but for $y=log(x)$ the approach works well. Note that
\begin{align*}
  p_y(y) = p_x(e^y)e^y
\end{align*}
so that we can write the log density of $y$ as (dropping the subscripts):
\begin{align*}
  lp(y) = -ae^y + be^{y/2} - \alpha y - \beta e^{-y}.
\end{align*}
The mode of this density $y^{mode}$ can be easily found numerically, and the second derivative is:
\begin{align*}
  \frac{\partial^2 lp(y)}{\partial y^2} = -ae^y + \frac{b}{4}e^{y/2} - \beta e^{-y}.
\end{align*}
The $t$ approximation then uses the proposal distribution 
\begin{align*}
  t_{v}\left(y^{mode}, \left[\left.\frac{\partial^2 lp(y)}{\partial y^2}\right|_{y=y^{mode}}\right]^{-1}\right).
\end{align*}
In practice choosing degrees of freedom $v=1$ works very well over the region of the parameter space where adaptive rejection sampling cannot be used. We can easily use this method when adaptive rejection sampling does not work, then transform $y$ back to $x$. It remains to check that the tails of $t$ distribution dominate the tails of our target distribution. Let $lq(y)$ denote the log density of the proposoal distribution. Then we need
\begin{align*}
  lp(y) - lq(y) \leq M\\
  \intertext{for some constant M, i.e.}
  -ae^y + be^{y/2} - \alpha y - \beta e^{-y} -\left(\frac{v+1}{2}\right)\log\left[1 + \frac{1}{v}\left(\frac{y-\mu}{\sigma}\right)\right]\leq M
\end{align*}
where $a>0$, $\alpha>0$, $\beta>0$, $v>0$, $\sigma>0$, and $b,\mu\in \Re$. We can rewrite the LHS as
\begin{align*}
    e^{y/2}(b-ae^{y/2}) - \alpha y - \beta e^{-y} -\left(\frac{v+1}{2}\right)\log\left[1 + \frac{1}{v}\left(\frac{y-\mu}{\sigma}\right)\right].
\end{align*}
So as $y\to\infty$ this quantity goes to $-\infty$ since the first term will eventually become negative no matter the value of $b$, and all other terms are always negative. Similarly as $y\to\-\infty$ this quantity goes to $-\infty$. Now pick any interval $(y_1,y_2)$ such that outside of the interval, $LHS<\epsilon$. Since treated as a function of $y$ the LHS is clearly continuous, it attains a maximum on this interval, and thus is bounded.

\subsubsection{Intelligently choosing a rejection sampler}
In practice, adaptive rejection sampling is relatively efficient for $p_x(x)$ but inefficient for $p_y(y)$ --- so much so that rejection sampling with the $t$ approximation for $p_y(y)$ is more efficient. To minimize computation time, it is best to use adaptive rejection sampling for $p_x(x)$ when the concavity condition is satisfied. When it is not, the $t$ approximation works well enough.

\subsection{Efficiently drawing from $p(W|V,\tilde{\gamma},y)$ and $p(V|W,\tilde{\psi},y)$ in the LLM}

Both the density of $\log(W)|V,\tilde{\gamma}_{0:T},y_{1:T}$ and the density of $\log(V)|W,\tilde{\psi}_{0:T},y_{1:T}$ have the following form:
\begin{align*}
  p(y)\propto \exp\left[-\alpha y - ae^{-y} + be^{-y/2} - ce^y\right].
\end{align*}
where $\alpha>0$, $a>0$, $c>0$, and $b\in \Re$. The log density is:
\begin{align*}
  lp(y) = -\alpha y - ae^{-y} + be^{-y/2} - ce^y + C
\end{align*}
where $C$ is some constant. We only provide one strategy for rejection sampling from this density: the $t$ approximation. Similar reasoning to above shows that we can use a $t$ distribution as a proposal in a rejection sampler. Now we choose the location parameter by maximizing $lp(y)$ in $y$ numerically to find the mode, $y^{mode}$. Next the second derivative of $lp(y)$ is given by
\begin{align*}
  \frac{\partial^2 lp(y)}{\partial y^2} = -ae^{-y} + \frac{b}{4}e^{-y/2}-ce^y.
\end{align*}
We then set the scale parameter to be
\begin{align*}
  -\left[\left.\frac{\partial^2 lp(y)}{\partial y^2}\right|_{y=y^{mode}}\right]^{-1}
\end{align*}
as in the normal approximation, and the degrees of freedom parameter to $v=1$. This rejection sampler is tolerably efficient for our purposes, but it is not fast.




\clearpage

\bibliographystyle{plainnat}
\bibliography{dlmasis}
\end{document}




