\documentclass[12pt]{article}
\usepackage{JASA_manu} %formats document like ASA wants
\usepackage{natbib} %formats citations like ASA wants
\usepackage{amssymb, amsmath, amsthm, graphics, graphicx, color, fullpage}
\usepackage{thmtools} %to format the Algorithm environment correctly
%\usepackage{hyperref}
%\hypersetup{
%  colorlinks   = true,    % Colours links instead of ugly boxes
%  urlcolor     = blue,    % Colour for external hyperlinks
%  linkcolor    = blue,    % Colour of internal links
%  citecolor    = red      % Colour of citations
%}
\usepackage{nameref, cleveref} %for named references

% define algorithm environment
\declaretheoremstyle[
notefont=\bfseries, notebraces={}{},
bodyfont=\normalfont\itshape,
headformat=\NAME:\NOTE
]{nopar}
\declaretheorem[style=nopar, name=Algorithm,
refname={Algorithm,Algorithms},
Refname={Algorithm,Algoritms},
numbered=no]{alg*}

\newtheorem{lem}{Lemma}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\vect}{vec}

\graphicspath{{plots/}}
\newcommand{\matt}[1]{{\color{red} Matt: #1}}
\newcommand{\jarad}[1]{{\color{red} Jarad: #1}}

\usepackage[page,header]{appendix}
\usepackage{titletoc}


\begin{document}

\title{Interweaving Markov Chain Monte Carlo Strategies for Efficient
Estimation of Dynamic Linear Models}
%short title: Interweaving MCMC Strategies for DLMs
\author{Matthew Simpson\textsuperscript{1}, Jarad Niemi\textsuperscript{2}, Vivekananda Roy\textsuperscript{2}}
\date{\today}
\maketitle

\begin{center}
 \vspace{-.5em}
 {\small \textsuperscript{1} Iowa State University, Departments of Statistics and Economics, Ames, IA 50011-1210\\[-.3em]
 {\tt simpsonm@iastate.edu}}
 
 \vspace{.5em}

{\small \textsuperscript{2} Iowa State University, Department of Statistics, Ames, IA 50011-1210\\[-.3em]
{\tt <niemi,vroy>@iastate.edu}}

\end{center}


\newpage

\mbox{}
\vspace*{2in}
\begin{center}
\textbf{Author's Footnote:}
\end{center}
Matthew Simpson is Doctoral Candidate, Departments of Economics and Statistics, Iowa State University, Ames, IA 50011-1210 (E-mail: simpsonm@iastate.edu); Jarad Niemi is Assistant Professor, Department of Statistics, Iowa State University, Ames, IA 50011-1210 (E-mail: niemi@iastate.edu); and Vivekananda Roy is Assistant Professor, Department of Statistics, Iowa State University, Ames, IA 50011-1210 (E-mail: vroy@iastate.edu). The authors thank participants of the Economics, Finance, and Business workshop at the Bayes 250 conference for helpful comments. %The authors thank Sylvia Fr{\"u}hwirth-Schnatter for helpful comments. 

\newpage
\begin{abstract}
In dynamic linear models (DLMs) with unknown fixed parameters, a standard Markov chain Monte Carlo (MCMC) sampling strategy is to alternate sampling of latent states conditional on fixed parameters and sampling of fixed parameters conditional on latent states. In some regions of the parameter space, this standard data augmentation (DA) algorithm can be inefficient. To improve efficiency, we seek to employ the interweaving strategies of \citet{yu2011center} that combine separate DAs by weaving them together. For this, we introduce a number of novel alternative DAs for a general class of DLMs: the scaled errors, wrongly-scaled errors, and wrongly-scaled disturbances. With the latent states and the less commonly used scaled disturbances, this yields five unique DAs to employ in MCMC algorithms. Each DA implies a unique MCMC sampling strategy and they can be combined into interweaving or alternating strategies that improve MCMC efficiency. We assess the strategies using the local level DLM and demonstrate that several strategies improve efficiency relative to the standard approach, the most efficient being either interweaving or alternating the scaled errors and scaled disturbances. Supplementary materials are available for this article online.

\smallskip
\noindent \textbf{Key Words:} Ancillary augmentation; Centered parameterization; Data augmentation; Non-centered parameterization; Reparameterization; Sufficient augmentation; Time series; State-space model
\end{abstract}


\newpage

\section{Introduction}

The Data Augmentation (DA) algorithm of \citet{tanner1987calculation} and the closely related EM algorithm of \citet{dempster1977maximum} have become widely used strategies for computing posterior distributions and maximum likelihood estimates. There is a long history of using ideas from the EM literature to inform the construction of DA algorithms and vice versa \citep{meng1997algorithm,van2010cross}. While DA and EM algorithms are useful, they often suffer from slow convergence in practice and a large literature has grown up around the various possible improvements to both algorithms \citep{meng1997algorithm,meng1999seeking,liu1999parameter,hobert2008theoretical,yu2011center} though much of the work on constructing improved DAs has focused on hierarchical models \citep{gelfand1995efficient,roberts1997updating,meng1998fast,van2001art,bernardo2003non,papaspiliopoulos2007general,papaspiliopoulos2008stability}. Relatively little attention has been paid to time series models despite similarities between some time series and hierarchical models, though there are some exceptions \citep{pitt1999analytic,fruhwirth2003bayesian,fruhwirth2006auxiliary}. 

We seek to improve DA schemes in dynamic linear models (DLMs), i.e. linear Gaussian state-space models. The standard DA scheme uses the latent states and alternates between drawing from the full conditional distributions of the latent states and the model parameters \citep{fruhwirth1994data,carter1994gibbs}. Much of the existing literature on improving DA schemes focuses on non-Gaussian state-space models --- in particular, the stochastic volatility model and models based on it \citep{shephard1996statistical,fruhwirth2003bayesian,roberts2004bayesian,bos2006inference,strickland2008parameterisation,fruhwirth2008heston,kastner2013ancillarity}, but a few work with the class of DLMs we consider \citep{fruhwirth2004efficient}. One recent development in the DA literature is interweaving strategy for using two separate DAs in a single algorithm by weaving them together in a particular fashion \citep{yu2011center}. This strategy draws on the strengths of both underlying DA algorithms in order to construct an MCMC algorithm which is at least as efficient as the best of the two DA algorithms and often more efficient. We implement interweaving algorithms in a general class of DLMs, and in order to do so we introduce several new DAs for this class of models. We also show that no practical sufficient augmentation (centered augmentation) exists for the DLM, which restricts the sort of interweaving algorithms we can construct. Using the local level model, we fit the model to simulated data using a variety of the MCMC strategies we discuss in order to assess the relative performance of the algorithms.

The rest of the paper is organized as follows. In Section \ref{sec:DA} we review the data augmentation literature while in Section \ref{sec:DLM} we introduce the dynamic linear model, discuss the subclass of DLMs we consider, and explore some of the key properties of the model. Section \ref{sec:DAs} explores several possible DAs for our class of DLMs and shows that any sufficient augmentation is likely to be difficult to use. Section \ref{sec:Algs} discusses the various MCMC strategies available for the DLM while Section \ref{sec:LLM} applies these algorithms to the local level model. Finally, Section \ref{sec:Discuss} discusses these results and suggests directions for further research while Section \ref{sec:Supp} contains an index to the supplementary materials.

\section{Variations of data augmentation}\label{sec:DA}

Suppose $p(\phi|y)$ is a probability density, for example the posterior distribution of some parameter $\phi$ given data $y$. %We use $p(.)$ to denote the probability density of the enclosed random variables.
Then a data augmentation algorithm adds $\theta$ with joint distribution $p(\phi,\theta|y)$ such that $\int_{\Theta}p(\phi,\theta|y)d\theta = p(\phi|y)$. The DA algorithm is similar to a Gibbs sampler, except it constructs a Markov chain for $\phi$ instead of $(\phi, \theta)$. In this DA algorithm, the $k+1$'st state of $\phi$ is obtained from the $k$'th state as follows (we implicitly condition on the data $y$ in all algorithms and only superscript the previous and new draws of the model parameters of interest):
\begin{alg*}[DA]Data Augmentation\label{alg:DA}
  \begin{align*}
  [\theta|\phi^{(k)}] \to [\phi^{(k+1)}|\theta]
\end{align*}
\end{alg*}
\noindent where $[\theta|\phi^{(k)}]$ means a draw of $\theta$ from $p(\theta|\phi^{(k)},y)$ and $[\phi^{(k+1)}|\theta]$ means a draw from $p(\phi|\theta,y)$. The DA, $\theta$, need not be interesting in any scientific sense --- it can be viewed purely as a computational construct. 

\subsection{Reparameterization and alternating DAs}

One well known method of improving mixing and convergence in MCMC samplers (and convergence in EM algorithms) is reparameterization of the model (see  \citet{papaspiliopoulos2007general} and references therein). The DA $\theta$ is called a {\it sufficient augmentation} (SA) for the model parameter $\phi$ if $p(y|\theta,\phi)=p(y|\theta)$. Similarly $\theta$ is called an {\it ancillary augmentation} (AA) for $\phi$ if $p(\theta|\phi)=p(\theta)$. An SA is sometimes called a centered augmentation or centered parameterization in the literature while an AA is sometimes called a non-centered augmentation or non-centered parameterization. Like \citet{yu2011center} we prefer the SA and AA terminology because it suggests a connection with Basu's theorem \citep{basu1955statistics}, which we will return to in Section \ref{sec:Intro:int}.

A key reason behind the emphasis on SAs and AAs is that typically when the DA algorithm based on the SA has nice mixing and convergence properties, the DA algorithm based on the AA has poor mixing and convergence properties and vice-versa. This property suggests combining the two such DA algorithms to construct an improved sampler. One intuitive approach is to alternate between the two augmentations within a Gibbs sampler \citep{papaspiliopoulos2007general}. Suppose we have a second distinct DA $\gamma$ such that $\int_\Gamma p(\phi,\gamma|y)d\gamma = p(\phi|y)$, then the alternating algorithm for sampling from $p(\phi|y)$ is as follows:
\begin{alg*}[Alt]Alternating Algorithm\label{alg:Alt}
  \begin{center}
    \begin{tabular}{lllllll}
  $[\theta|\phi^{(k)}]$& $\to$& $[\phi|\theta]$& $\to$& $[\gamma|\phi]$& $\to$& $[\phi^{(k+1)}|\gamma]$
    \end{tabular}
  \end{center}
\end{alg*}
\noindent One iteration of the alternating algorithm consists of one iteration of the DA algorithm based on $\theta$ to obtain an intermediate value of $\phi$, followed by one iteration of the DA algorithm based on $\gamma$ started at the intermediate value of $\phi$.

In Algorithm \nameref{alg:DA}, when $\phi$ and $\theta$ are highly dependent in the joint posterior, the draws from $p(\theta|\phi,y)$ and then from $p(\phi|\theta,y)$ will hardly move the chain, resulting in high autocorrelation. In an alternating algorithm, there are essentially two chances to substantially move the chain -- one for $\theta$ in the draws from $p(\theta|\phi,y)$ and from $p(\phi|\theta,y)$ and one for $\gamma$ in the draws from $p(\gamma|\phi,y)$ and from $p(\phi|\gamma,y)$.  In particular, when $\theta$ is an SA and $\gamma$ is an AA or vice versa, then it is often the case that there is low dependence either between $\phi$ and $\theta$ or between $\phi$ and $\gamma$ and thus the alternating strategy will improve mixing.

\subsection{Interweaving: an alternative to alternating}\label{sec:Intro:int}

Another option is to {\it interweave} the two DAs together \citep{yu2011center}. A general interweaving strategy (GIS) is an MCMC algorithm that obtains $\phi^{(k+1)}$ from $\phi^{(k)}$ as follows:
\begin{alg*}[GIS]General Interweaving Strategy\label{alg:GIS}
  \begin{align*}
    [\theta|\phi^{(k)}] \to [\gamma|\theta] \to [\phi^{(k+1)}|\gamma].
  \end{align*}
\end{alg*}
\noindent The GIS algorithm obtains the next iteration of the parameter $\phi$ in three steps: 1) draw $\theta$ conditional on $\phi^{(k)}$, 2) draw $\gamma$ conditional on $\theta$, and 3) draw $\phi^{(k+1)}$ conditional on $\gamma$. This looks similar to the usual DA algorithm except a second DA is ``weaved'' in between the draw of the first DA and of the parameter vector. 

The second step of the GIS algorithm is often accomplished by sampling $\phi|\theta$ and then $\gamma|\theta,\phi$. If we expand this out, then the GIS algorithm becomes:
\begin{alg*}[eGIS]Expanded GIS\label{alg:GIS2}
  \begin{center}
    \begin{tabular}{lllllll}
      $[\theta|\phi^{(k)}]$& $\to$& $[\phi|\theta]$& $\to $&$[\gamma|\theta,\phi]$& $\to$& $[\phi^{(k+1)}|\gamma]$
    \end{tabular}
  \end{center}
\end{alg*}
\noindent
In addition, $\gamma$ and $\theta$ are often, but not always, one-to-one transformations of each other conditional on $(\phi,y)$, i.e. $\gamma = M(\theta;\phi,y)$ where $M(.;\phi,y)$ is a one-to-one function, and thus $[\gamma|\theta,\phi]$ is deterministic.
The key difference between Algorithm \nameref{alg:GIS} and Algorithm \nameref{alg:Alt} can be seen in step three Algorithm \nameref{alg:GIS2}: instead of drawing from $p(\gamma|\phi,y)$, the GIS algorithm draws from $p(\gamma|\theta,\phi,y)$, connecting the two DAs together while the alternating algorithm keeps them separate.

\citet{yu2011center} call a GIS approach where one of the DAs is an SA and the other is an AA an ancillary sufficient interweaving strategy (ASIS). They show that the GIS algorithm has a geometric rate of convergence no worse than the worst of the two underlying DA algorithms and in some cases better than the the corresponding alternating algorithm. In particular, their Theorem 1 suggests that the weaker the dependence between the two DAs in the posterior, the more efficient the GIS algorithm. With \emph{a posteriori} independent data augmentations, the GIS algorithm obtains iid draws from the posterior for $\phi$. This helps motivate their focus on ASIS and the choice of terminology --- conditional on the model parameter, an SA and an AA are independent under the conditions of Basu's theorem \citep{basu1955statistics}, which suggests that the dependence between the two DAs will be limited in the posterior. In fact, when the prior on $\phi$ is nice in some sense, \citet{yu2011center} show that the ASIS algorithm is the same as the optimal parameter expanded data augmentation (PX-DA) algorithm \citep{liu1999parameter}, which is closely related to marginal and conditional augmentation \citep{meng1999seeking,hobert2008theoretical}.

In addition to the GIS, it is possible to define a componentwise interweaving strategy (CIS) that interweaves within specific steps of a Gibbs sampler as well. A CIS algorithm for $\phi=(\phi_1, \phi_2)$ essentially employs interweaving for each block of $\phi$ separately, e.g.
\begin{alg*}[CIS]Componentwise Interweaving Strategy\label{alg:CIS}
  \begin{center}
    \begin{tabular}{llllll}
      $[\theta_1|\phi_1^{(k)},\phi_2^{(k)}]$ & $\to$  & $[\gamma_1|\phi_2^{(k)},\theta_1]$ & $\to$ & $[\phi_1^{(k+1)}|\phi_2^{(k)},\gamma_1]$ &$\to$ \\
      $[\theta_2|\phi_1^{(k+1)},\phi_2^{(k)},\gamma_1]$ &$\to$ & $[\gamma_2|\phi_1^{(k+1)},\theta_2]$ & $\to$ & $[\phi_2^{(k+1)}|\phi_1^{(k+1)},\gamma_2]$ &
    \end{tabular}
  \end{center}
\end{alg*}\noindent
where $\theta_i$ and $\gamma_i$ are distinct data augmentations for $i=1,2$, but potentially $\gamma_1=\theta_2$  or $\gamma_2=\theta_1$. The first row draws $\phi_1$ conditional on $\phi_2$ using interweaving in a Gibbs step, while the second row does the same for $\phi_2$ conditional on $\phi_1$. The algorithm can easily be extended to greater than two blocks within $\phi$. The main attraction of CIS is that it is often easier to find an AA--SA pair of DAs for $\phi_1$ conditional on $\phi_2$ and another pair for $\phi_2$ conditional on $\phi_1$ than it is to find and AA--SA pair for $\phi=(\phi_1,\phi_2)$ jointly.

\section{Dynamic linear models} \label{sec:DLM}

The general dynamic linear model is well studied \citep{harrison1999bayesian,petris2009dynamic,prado2010time} and is defined as
\begin{align*}
y_t &= F_t\theta_t + v_t && v_t \stackrel{ind}{\sim} N_k(0,V_t) && (\mbox{observation equation}) \\
 \theta_t &= G_t\theta_{t-1} + w_t && w_t \stackrel{ind}{\sim} N_p(0,W_t) && (\mbox{system equation}) 
\end{align*}
where $N_d(\mu,\Sigma)$ is a $d$-dimensional multivariate normal distribution with mean $\mu$ and covariance $\Sigma$ and the observation errors, $v_{t}$ for $t=1,2,\cdots,T$, and system disturbances, $w_{t}$ for $t=1,2,\cdots,T$, are independent. The observed data is $y\equiv y_{1:T} \equiv (y_1',y_2',\cdots y_T')'$ while the latent states are $\theta \equiv \theta_{0:T} \equiv (\theta_0',\theta_1',\cdots \theta_T')'$ and is the standard DA for this model. For each $t=1,2,\cdots,T$, $F_t$ is a $k\times p$ matrix and $G_t$ is a $p\times p$ matrix. Let $\phi$ denote the vector of unknown parameters in the model. Then possibly $F_{t}$, $G_{t}$, $V_{t}$, and $W_{t}$ are all functions of $\phi$ for $t=1,2,\cdots,T$.

The subclass of DLMs we will focus on sets $V_t=V$ and $W_t=W$ and treats $F_{t}$ and $G_{t}$ as known for all $t$. Our results can be extended when $V_t$ or $W_t$ is time-varying or when $F_t$ or $G_t$ depend on unknown parameters, but we ignore those cases for simplicity. As a result $\phi=(V,W)$ is our unknown parameter vector and we can write the model as
\begin{align}
  y_t|\theta,V,W \stackrel{ind}{\sim} & N_k(F_t\theta_t,V) &
  \theta_t|\theta_{0:t-1},V,W  \sim & N_p(G_t\theta_{t-1},W) \label{dlmbotheqs}
\end{align}
for $t=1,2,\cdots T$. We use the standard conditionally conjugate priors, i.e. independently $\theta_0 \sim N_p(m_0, C_0)$, $V \sim IW(\Lambda_V, \lambda_V)$ and $W \sim IW(\Lambda_W, \lambda_W)$ where $m_0$, $C_0$, $\Lambda_V$, $\lambda_V$, $\Lambda_W$, and $\lambda_W$ are known hyperparameters and $IW(\Lambda, \lambda)$ denotes the inverse Wishart distribution with degrees of freedom $\lambda$ and positive definite scale matrix $\Lambda$.

\subsection{The implied marginal model}\label{sec:DLM:marg}

While the DLM is typically defined using the latent states as a DA, strictly speaking they are unnecessary for the model's definition. Since we view $\theta$ purely as a nuisance parameter added for computational convenience, we wish to characterize the the marginal model for $y$. From equation \eqref{dlmbotheqs}, we can rewrite the model by recursive substitution:
\begin{align*}
  y_t &= v_t + F_t\left(w_t + G_tw_{t-1} + G_tG_{t-1}w_{t-2} + ... + G_tG_{t-1}\cdots G_{2}w_1 + G_tG_{t-1}\cdots G_1\theta_0\right).
\end{align*}
Then conditional on $\phi=(V,W)$ each $y_t$ is a linear combination of normal random variables. After marginalizing out $\theta$, $y$ has a normal distribution such that $\mathrm{E}[y_t|\phi] =  F_tH_tm_0$,
\begin{align*}
  \mathrm{Var}[y_t|\phi] =  V + F_t(K_tWK_t' + H_tC_0H_t')F_t' ,\quad \mathrm{and} \quad
  \mathrm{Cov}[y_s,y_t|\phi] = F_s(K_sWK_t' + H_sC_0H_t')F_t',
\end{align*}
where $H_t = G_tG_{t-1}\cdots G_1$ and $K_t = I_p + G_t + G_tG_{t-1} + \cdots + G_tG_{t-1}\cdots G_2$. Next define $D_t = F_tG_tG_{t-1}\cdots G_1$. Then let $\tilde{V}=I_T\otimes V$ and $D$ be block diagonal with elements $D_1,\ldots,D_T$, 
%\begin{align*}
%\tilde{V}_{Tk\times Tk} & = \begin{bmatrix}
%V & 0 & \cdots & 0\\
%0 & V & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots\\
%0 & 0 & \cdots & V
%\end{bmatrix}, &
%D_{Tp\times Tk} &= \begin{bmatrix}
%D_1 & 0 & \cdots & 0\\
%0 & D_2 & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots\\
%0 & 0 & \cdots & D_T
%\end{bmatrix},
%\end{align*}
\begin{align*}
\tilde{W}_{Tk\times Tk} &= \begin{bmatrix} K_1'F_1' & K_2'F_2' & \cdots K_T'F_T' \end{bmatrix}' W \begin{bmatrix} K_1'F_1' & K_2'F_2' & \cdots K_T'F_T' \end{bmatrix}, &\\
\tilde{C}_{Tk\times Tk} &= \begin{bmatrix} H_1'F_1' & H_2'F_2' & \cdots H_T'F_T' \end{bmatrix}' C_0 \begin{bmatrix} H_1'F_1' & H_2'F_2' & \cdots H_T'F_T' \end{bmatrix},&
\end{align*}
and $\tilde{m}_{Tp\times 1} = (m_0', m_0', \cdots m_0')'$. Now we have the data model for $y$ without any data augmentation:
\begin{align}
  y|V,W \stackrel{ind}{\sim} N_{Tk}(D\tilde{m}, \tilde{V} + \tilde{W} + \tilde{C}). \label{margmodel}
\end{align}
%Given a prior $p(\phi)$, the posterior density we are interested in is $p(\phi|y)\propto p(y|\phi)p(\phi)$.

\section{Augmenting the DLM}\label{sec:DAs}

The standard definition of the DLM includes $\theta$ and unsurprisingly, $\theta$ is the standard DA used in estimation of the DLM. Any other DA would suffice to estimate $\phi$ --the only restriction is that $p(\phi|y) = \int p(\phi,\theta|y)d\theta$ for any augmentation $\theta$. We now introduce one data augmentation that is known, scaled disturbances, and three other novel augmentations: scaled errors, wrongly-scaled disturbances, and wrongly-scaled errors. The primary purpose of these augmentations is as grist for the interweaving mill, but each new DA will also implicitly define a new base DA algorithm.

\subsection{The scaled disturbances}\label{sec:DAs:dist}

A natural way to create new DAs is by reparameterizing old DAs. \citet{papaspiliopoulos2007general} note that typically the standard augmentation results in an SA for the parameter $\phi$. All that would be necessary for an ASIS algorithm, then, is to construct an AA for $\phi$. We immediately run into a problem because the standard DA for a DLM is $\theta$ but from equation \eqref{dlmbotheqs} we see that $V$ is in the observation equation so that $\theta$ is not an SA for $(V,W)$ while $W$ is in the system equation so that $\theta$ is not an AA for $(V,W)$ either. In order to find an SA we need to somehow move $V$ from the observation equation to the system equation while leaving $W$ in the system equation. Similarly, to findd an AA we need to somehow move $W$ from the system equation to the observation equation while leaving $V$ in the observation equation.

We follow \citet{papaspiliopoulos2007general}'s suggestion to construct a pivotal quantity in order to find an ancillary augmentation. In particular, in order to obtain an ancillary augmentation for a variance parameter we can center the sufficient augmentation and scale it by the square root of the variance parameter. Based on this intuition, notice from equation \eqref{dlmbotheqs} that if we hold $V$ constant then $\theta$ is an SA for $W$ conditional on $V$, i.e. for $W|V$. Similarly $\theta$ is an AA for $V|W$. This suggests that if we center and scale $\theta_{t}$ by $W$ appropriately for all $t$ we will have an ancillary augmentation for $V$ and $W$ jointly, thus creating the {\it scaled disturbances} (SDs).

In the general DLM the disturbances are vectors, so to define the scaled disturbances let $L_W$ denote the Cholesky decomposition of $W$, i.e. the lower triangle matrix $L_W$ such that $L_WL_W' =W$. Then we will define the scaled disturbances $\gamma\equiv\gamma_{0:T}\equiv(\gamma_0',\gamma_1',\cdots,\gamma_T')'$ by $\gamma_0=\theta_0$ and $\gamma_t = L_W^{-1}(\theta_t-G_t\theta_{t-1})$ for $t=1,2,\cdots,T$. There are actually $p!$ different versions of the scaled disturbances depending on how we order the elements of $\theta_t$, as \citet{meng1998fast} note for EM algorithms in a different class of models. We make no attempt to determine which ordering should be used.
%\matt{There isn't always a most natural ordering. Consider a multivariate local level model}
%Here we will use the most natural ordering.
The reverse transformation is defined recursively by $\theta_0(\gamma,L_W)=\gamma_0$ and $\theta_t(\gamma,L_W)=L_W\gamma_t + G_t\theta_{t-1}(\gamma,W)$ for $t=1,2,\cdots,T$. Ultimately under the scaled disturbance parameterization we can write the model as
\begin{equation}
  y_t|\gamma,V,W  \stackrel{ind}{\sim} N_k\left(F_t\theta_t(\gamma,L_W), V\right), \qquad
  \gamma_t  \stackrel{iid}{\sim}N_p(0,I_p) \label{dlmdistmodel}
\end{equation}
for $t=1,2,\cdots,T$ where $I_p$ is the $p\times p$ identity matrix. Neither $V$ nor $W$ are in the system equation so the scaled disturbances are an AA for $(V,W)$.

The SDs are well known --- the disturbance smoother of \citet{koopman1993disturbance} finds the conditional posterior of the scaled disturbances given the model parameters and the data and \citet{fruhwirth2004efficient} use the SDs in a dynamic regression model with stationary regression coefficients.

\subsection{The scaled errors}\label{sec:DAs:error}
The scaled disturbances immediately suggest an analogous augmentation using the {\it scaled errors} (SEs), i.e. $v_t=y_t - F_t\theta_t$ appropriately scaled by $V$. Let $L_V$ denote the Cholesky decomposition of $V$ so that $L_VL_V'=V$, then we can define a version of the scaled errors as $\psi_t = L_V^{-1}(y_t - F_t\theta_t)$ for $t=1,2,\cdots,T$ and $\psi_0 = \theta_0$. This time there are $k!$ versions of the scaled errors depending on how $y_t$ is ordered.

%It is not straightforward to write down the model in terms of $\psi=(\psi_0',\psi_1',\cdots,\psi_T)'$ instead of $\theta$ and determine $p(\psi|V,W)$ in general. When $dim(y_t)\!=\!k\!=\!p\!=\!dim(\theta_t)$, $F_t$ is $p\times p$ and is invertible for $t=1,2,\cdots,T$, $\psi$ is a one-to-one transformation of $\theta$ and the problem is easier. This restriction is not necessary --- $\theta_t$, $y_t$ or both could be augmented in order to construct an $\tilde{F}_t$ which is square and invertible using a more complicated data augmentation scheme, but we pass over this complication and assume that $F_t$ is $k\times k$ and invertible. In Appendix D of the supplementary materials, we return to this issue. 

Assuming $F_t$ is invertible for all $t$ (this restriction is relaxed in Appendix of the supplementary materials), let $\theta_t = F_t^{-1}(y_t - L_V\psi_t)$ for $t=1,2,\cdots,T$ while $\theta_0=\psi_0$. Define $\mu_1 = L_V\psi_1 + F_1G_1\psi_0$ and $\mu_t =L_V\psi_t + F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{V}\psi_{t-1})$ for $t=2,3,\cdots,T$. Then the scaled error parameterization is 
\begin{align*}
  y_t|V,W,\psi,y_{1:t-1} \sim N_p(\mu_t, F_tWF_t') \qquad \psi_t  \stackrel{iid}{\sim} N_p(0,I_k)
\end{align*}
for $t=1,2,\cdots,T$ where $I_k$ is the $k\times k$ identity matrix. Now we immediately see that the scaled errors are an AA for $(V,W)$ since neither $V$ nor $W$ are in the system equation of this model. However, both $V$ and $W$ are in the observation equation so that $\psi$ is not an SA for $(V,W)$ nor for $V|W$ or $W|V$.

\subsection{The ``wrongly-scaled'' DAs}
Two other novel augmentations can be obtained by scaling the SD and SE by the opposite variance. Define $\tilde{\gamma}_t=L_V^{-1}(\theta_t - G_t\theta_{t-1})$ and $\tilde{\psi}_t=L_W^{-1}(y_t - \theta_t)$ for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\tilde{\gamma}_0=\theta_0$. Then the {\it wrongly-scaled disturbances} are $\tilde{\gamma}\equiv\tilde{\gamma}_{0:T}\equiv(\tilde{\gamma}_0',\tilde{\gamma}_1',\cdots,\tilde{\gamma}_T')'$ and the {\it wrongly-scaled errors} are $\tilde{\psi}\equiv\tilde{\psi}_{0:T}\equiv(\tilde{\psi}_0',\tilde{\psi}_1',\cdots,\tilde{\psi}_T')'$. The ``tilde'' versions of the scaled disturbances and the scaled errors are scaled by the ``wrong'' Cholesky decomposition, hence we call them the wrongly-scaled disturbances (WSDs) and the wrongly-scaled errors (WSEs) respectively.

We can write the model in terms of $\tilde{\gamma}$ as
\begin{align*}
  y_t|\tilde{\gamma},V,W \stackrel{ind}{\sim} N_p\left(F_t\theta_t(\tilde{\gamma},L_V), V\right) \qquad  
  \tilde{\gamma}_t \stackrel{ind}{\sim}N_p(0,L_V^{-1}W(L_V^{-1})')
\end{align*}
for $t=1,2,\cdots,T$ where $\theta_t(\tilde{\gamma},L_V)$ denotes the transformation from $\tilde{\gamma}$ to $\theta$ defined by the wrongly-scaled disturbances. Since $L_V$ is the Cholesky decomposition of $V$, the observation equation does not contain $W$, so $\tilde{\gamma}$ is an SA for $W|V$. Since $W$ and $L_V$ are both in the system equation, $\tilde{\gamma}$ is not an AA for $V|W$ nor for $W|V$. %Also, notice that the transformation from $\theta$ to $\tilde{\gamma}$ does not require $F_t$ to be invertible so that the constraint dim($y_t$)=dim($\theta_t$) can be relaxed.

Similarly, we can write the model in terms of $\tilde{\psi}$ as
\begin{align*}
  y_t|V,W,\tilde{\psi},y_{1:t-1} \sim N_p(\tilde{\mu}_t, F_tWF_t') \qquad 
  \tilde{\psi}_t  \stackrel{iid}{\sim} N_p(0,L_W^{-1}V(L_W^{-1})')
\end{align*}
for $t=1,2,\cdots,T$ where we define $\tilde{\mu}_1 = L_W\tilde{\psi}_1 - F_1G_1\tilde{\psi_0}$ and for $t=2,3,\cdots,T$ $\tilde{\mu}_t =L_W\tilde{\psi}_t - F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{W}\tilde{\psi}_{t-1})$. Since $\tilde{\mu}_t$ only depends on $W$ and not on $V$, $V$ is absent from the observation equation and thus $\tilde{\psi}$ is an SA for $V|W$. Once again, since both $W$ and $V$ are in the system equation $\tilde{\psi}$ is not an AA for either $V$ or $W$.

\subsection{The elusive search for a sufficient augmentation}

Having found two ancillary augmentations for the DLM, we would like to find a sufficient augmentation in order to construct an ASIS for sampling from the posterior distribution. The following lemma shows this is no easy task and clarifies where the difficulty lies.


\begin{lem}\label{noSA}
Suppose $\eta$ is an SA for the DLM such that conditional on $\phi$, $\eta$ and $y$ are jointly normally distributed, that is
\begin{align*}
 \left. \begin{bmatrix}\eta \\ y \end{bmatrix}\right|\phi \sim N\left(\begin{bmatrix} \alpha_\eta \\ D\tilde{m} \end{bmatrix}, \begin{bmatrix}
   \Omega_\eta & \Omega_{y,\eta}' \\
   \Omega_{y,\eta} & \tilde{V} + \tilde{W} + \tilde{C} \end{bmatrix}\right).
\end{align*}
Then $\tilde{\eta}=\Omega_{y,\eta}'\Omega_{\eta}^{-1}\eta$ is also an SA and
\[
\tilde{\eta}|\phi \sim N(A\alpha_{\eta},\tilde{V} + \tilde{W} + \tilde{C} - \Sigma)
\]
where $A=\Omega_{y,\eta}'\Omega_{\eta}^{-1}$ and $\Sigma = \tilde{V} + \tilde{W} + \tilde{C} - A\Omega_{\eta}A'$. Here $A$, $\Sigma$, and $\alpha_{\eta}$ are constants with respect to $\phi$. The conditional posterior of $\phi$ given $\tilde{\eta}$ can be written as
\begin{align*}
p(\phi|\tilde{\eta},y) &\propto p(\phi) |A\Omega_{\eta}A'|^{-1/2}\exp\left[-\frac{1}{2}(\tilde{\eta}-A\alpha_{\eta})'(A\Omega_{\eta}A)^{-1}(\tilde{\eta}-A\alpha_{\eta})\right]\\
&\propto p(\phi)|\tilde{V} + \tilde{W} + \tilde{C} - \Sigma|^{-1/2}\exp\left[-\frac{1}{2}(\tilde{\eta} - A\alpha_{\eta})'(\tilde{V} + \tilde{W} + \tilde{C} - \Sigma)^{-1}(\tilde{\eta} - A\alpha_{\eta})\right].
\end{align*}
\end{lem}
\noindent Recall from Section \ref{sec:DLM:marg} that the the marginal model for the data in the DLM is $y\sim N_{Tk}(D\tilde{m}, \tilde{V} + \tilde{W} + \tilde{C})$. Then $\alpha_\eta$, $\Omega_\eta$ and $\Omega_{y,\eta}$ are additional parameters included to define the full joint distribution of $y$ and $\eta$.

The posterior density we wish to sample from comes from equation \eqref{margmodel} and is the nearly the same as $p(\phi|\tilde{\eta},y)$ except it has $\Sigma$ equal to the zero matrix and $b$ equal to the zero vector with $y$ replacing $\tilde{\eta}$. Using $\eta$ instead of $\tilde{\eta}$ will not result in a simpler density $p(\phi|\eta,y)$. This conditional posterior can be written as
\begin{align*}
p(\phi|&\eta,y) \propto p(\phi) |\Omega_{\eta}|^{-1/2}\exp\left[-\frac{1}{2}(\eta - \alpha_{\eta})'\Omega_{\eta}^{-1}(\eta - \alpha_\eta)\right].
\end{align*}
which is similar to one form of $p(\phi|\tilde{\eta},y)$ in the Lemma \ref{noSA}. It is unclear how $\alpha_\eta$ and $\Omega_{\eta}$ must depend on $\phi$. When $A'A$ is invertible,  $\eta=(A'A)^{-1}A'\tilde{\eta}$ so that $\Omega_{\eta} = (A'A)^{-1}A'(\tilde{V} + \tilde{W} + \tilde{C} - \Sigma)A(A'A)^{-1}$ and $p(\phi|\eta,y)$ is clearly more complicated than $p(\phi|\tilde{\eta},y)$. The upshot is that once we find an SA, in order to use it we must obtain draws from a density that appears just as hard to sample from as the posterior density we are already trying to approximate.

Lemma \ref{noSA} does not quite rule out the existence of a useful SA, but it does suggest that it will be difficult to find one. This result brings to mind \citet{van2001art}'s contention that there is an art to constructing data augmentation algorithms --- our goal is not only to find an MCMC algorithm that has nice convergence and mixing properties, but also one that is easy to implement, and this second criteria is much more difficult to quantify. The problem we run into is unlikely to be unique to the time series setting but rather seems driven by trying to find an SA for a pair of variances: one on the data level and the other on the latent data level.

\section{MCMC Strategies for the DLM}\label{sec:Algs}

This section discusses how to construct various MCMC algorithms for approximating the posterior distribution of the DLM. We focus on {\it what} to do, not {\it why}. Derivations of the relevant full conditional distributions are omitted here and left to the supplementary materials in Appendix B. Similarly, we occasionally come across a full conditional density that is difficult to sample from. We only mention where this occurs here and leave the details about why it happens and how to overcome it, to the supplementary materials (Appendices E and F).

\subsection{Base algorithms}\label{sec:Algs:base}
Using any of the DAs introduced in Section \ref{sec:DAs}, we can construct several DA algorithms which we call {\it base algorithms} to distinguish them from the alternating and interweaving algorithms we will construct later. A well known method to estimate the parameters in a DLM uses the DA algorithm using the latent states $\theta$ as the DA \citep{fruhwirth1994data,carter1994gibbs} which we refer to as the {\it state sampler}. In order to construct this algorithm, we need two pieces --- the conditional posterior of $V$ and $W$ given $\theta$ and the conditional posterior of $\theta$ given $V$ and $W$. The density $p(\theta|V,W,y)$ is multivariate normal and any algorithm that obtains a random draw from it is called a simulation smoother in the literature. One commonly used simulation smoother is the forward filtering, backward sampling algorithm (FFBS) \citep{fruhwirth1994data,carter1994gibbs} which uses the Kalman filter, but there are several alternatives including \citet{koopman1993disturbance} and \citet{de1995simulation}. The smoothers introduced in \citet{mccausland2011simulation} and \citet{rue2001fast}, dubbed ``all without a loop'' smoothers by \citet{kastner2013ancillarity} are particularly computationally efficient. Both methods exploit the tridiagonal structure of the precision matrix of the joint normal distribution for $\theta$ in order to speed up the computation of its Cholesky factor and are typically faster than methods based on the Kalman filter and other methods \citep{mccausland2011simulation}. The method of \citet{rue2001fast} is called the Cholesky factor algorithm (CFA). We use the method of \citet{mccausland2011simulation} and call it the mixed Cholesky factor algorithm (MCFA) because it is similar to the CFA except instead of computing the Cholesky factor of the precision matrix, then obtaining a random draw of the latent states, it mixes up the substeps required in those two steps. We omit the details of this algorithm in the context of the DLM, but they can be found in the supplementary materials in Appendix C.

In order to complete the state sampler, we need to obtain a draw from $p(V,W|\theta,y)$. It is easy to show $V$ and $W$ have independent inverse Wishart distributions conditional on $\theta$ and $y$. In particular,
\begin{align*}
  V|\theta,y &\sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right), &
  W|\theta,y &\sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right), %\label{eq:VW|theta}
\end{align*}
where $v_t = y_t - F_t\theta_t$, and $w_t = \theta_t - G_t\theta_{t-1}$. Putting the pieces together, the state sampler is the following DA algorithm:
\begin{alg*}[State]State Sampler
\label{alg:DLMstate}
\begin{enumerate}
\item Use the MCFA to sample $\theta \sim p(\theta|V,W,y)$.
\item Sample $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$ and $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$ independently.
\end{enumerate}
\end{alg*}
\noindent As we will show in Section \ref{sec:LLM}, the Markov chain constructed using the state sampler can mix poorly in some regions of the parameter space. For example, in a dynamic regression through the origin with stationary regression coefficient, it is known that if the variance of the latent states is too small relative to the variance of the data, mixing will be poor for $W$ \citep{fruhwirth2004efficient}.

Next, we can use the scaled disturbances, $\gamma$, in order to construct a second DA algorithm called the {\it scaled disturbance sampler}. In the smoothing step, we need to obtain a draw from $p(\gamma|V,W,y)$. This density is also Gaussian but does not have a tridiagonal precision matrix. As a result, in order to obtain a draw from the conditional posterior of $\gamma$, we use the MCFA to draw from $p(\theta|V,W,y)$, then use the definition of the scaled disturbances in order to transform from $\theta$ to $\gamma$. The density $p(V,W|\gamma,y)$ is rather complicated and does not appear easy to draw from, but it is easy to show that $V|W,\gamma,y \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$ where again $v_t = y_t - F_t\theta_t$ and $\theta_t$ is a function of $\gamma$ and $W$. However, it is not easy to draw from $p(W|\gamma,y)$ so we abandon drawing $V$ and $W$ jointly. The density $p(W|V,\gamma,y)$ is simpler and, at least in the local level model, can be efficiently sampled from. As a result, the scaled disturbance sampler, Algorithm \nameref{alg:DLMdist}, has three steps instead of the usual two. 
\begin{alg*}[SD]Scaled Disturbance Sampler\label{alg:DLMdist}
\begin{enumerate}
\item Use the MCFA to sample $\theta \sim p(\theta|V,W,y)$.
\item Draw $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$.
\item Transform $\theta$ to $\gamma$.
\item Draw $W \sim p(W|V,\gamma,y)$.
\end{enumerate}
\end{alg*}\noindent
We draw $V$ before $W$, but there is no intrinsic reason for this order. In order to draw $\gamma$, we first draw $\theta$ and transform. We can do this because the draw of $V$ is the same whether we condition on $\gamma$ or $\theta$, and it reduces the computational cost of the algorithm. The last step is the difficult one but we demonstrate how to accomplish it when $W$ is a scalar in the local level model in the supplementary materials (Appendix E).

The DA algorithm based on the scaled errors is called the {\it scaled error sampler} (Algorithm \nameref{alg:DLMerror}) and is similar to the scaled disturbance sampler with a couple of key differences. First, the simulation smoothing step in the scaled error sampler can be accomplished directly with the MCFA because the precision matrix of the conditional posterior of $\psi$ retains the necessary tridiagonal structure. Second, the full conditional distribution of $W$ is the familiar inverse Wishart density and the full conditional of $V$ is the complicated density. The density of $V|W,\psi,y$ is in the same class as that of $W|V,\gamma,y$. In fact there is a strong symmetry here --- the joint conditional posterior of $(V,W)$ given $\gamma$ is from the same family of densities as that of $(W,V)$ given $\psi$ so that $V$ and $W$ essentially switch places when we condition on the scaled errors instead of the scaled disturbances.
\begin{alg*}[SE]Scaled Error Sampler\label{alg:DLMerror}
\begin{enumerate}
\item Use the MCFA to sample $\psi \sim  p(\psi|V,W,y)$
\item Draw $V \sim p(V|W,\psi,y)$.
\item Draw $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
\end{enumerate}
\end{alg*}

The wrongly-scaled DA algorithms are close analogues to their correctly scaled cousins. Starting with the {\it wrongly-scaled disturbance sampler} (Algorithm \nameref{alg:DLMwdist}), the simulation smoothing step to draw from $p(\tilde{\gamma}|V,W,y)$ is similar to that of the scaled disturbance sampler --- the density is Gaussian, but the precision matrix is not tridiagonal, so we draw $\theta$ using the MCFA and transform to obtain a draw of $\tilde{\gamma}$. The density of $V,W|\tilde{\gamma},y$ is too complicated to draw from directly, as was the case when we used the scaled disturbances. In this case, the full conditional distribution of $W$ is the same as its distribution when we condition on the states while the density of $V|\tilde{\gamma},y$ is once again difficult to draw from. The density of $V|W,\tilde{\gamma},y$ is easier to work with, at least in the local level model example in Section \ref{sec:LLM}. 
\begin{alg*}[WSD]Wrongly-Scaled Disturbance Sampler\label{alg:DLMwdist}
\begin{enumerate}
\item Use MCFA to draw $\theta \sim p(\theta|V,W,y)$.
\item Transform $\theta$ to $\tilde{\gamma}$.
\item Draw $V \sim p(V|W,\tilde{\gamma},y)$.
\item Draw $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
\end{enumerate}
\end{alg*}\noindent
Now the third step is difficult and we demonstrate how to accomplish it in the local level model in the supplementary materials (Appendix E). We could switch the order in which $V$ and $W$ are drawn in this algorithm so that we can draw $W$ before transforming $\theta$ to $\tilde{\gamma}$. This would make each iteration slightly cheaper and probably would not affect the mixing and convergence properties of the algorithm. However, we are more interested in comparing the mixing and convergence properties of the various samplers, so we always sample $V$ before $W$ when we cannot sample them jointly.

The {\it wrongly-scaled error sampler} (Algorithm \nameref{alg:DLMwerror}) is closely related to both the wrongly-scaled disturbance sampler and the scaled error sampler. The density of $\tilde{\psi}|V,W,y$ is Gaussian with a tridiagonal precision matrix, so the simulation smoothing step can be accomplished using the MCFA. The density $p(V,W|\tilde{\psi},y)$ is from the same class as $p(W,V|\tilde{\gamma},y)$ so that $V$ and $W$ essentially switch places when we condition on $\tilde{\psi}$ instead of $\tilde{\gamma}$. In particular, $V|W,\tilde{\psi},y$ has an inverse Wishart density and the density of $W|V,\tilde{\psi},y$ is from the same class as that of $V|W,\tilde{\gamma},y$.
\begin{alg*}[WSE]Wrongly-Scaled Error Sampler\label{alg:DLMwerror}
\begin{enumerate}
\item Use MCFA to draw $\tilde{\psi} \sim p(\theta|V,W,y)$.
\item Draw $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$.
\item Draw $W \sim p(W|V,\tilde{\psi},y)$
\end{enumerate}
\end{alg*}

%The densities $p(V,W|\gamma,y)$ and $p(V,W|\tilde{\psi}$ are both complex to work with and both imply the same full conditional inverse Wishart distribution for $V$, but they differ in their treatment of $W$. Because of the symmetry between the correctly and wrongly-scaled versions of each density, something similar can be said for $p(V,W|\tilde{\gamma},y)$ and $p(V,W|\psi,y)$. In Section \ref{sec:LLM}, we will show how closely the two distributions are related in the local level model. Now given these five DA algorithms, we can move on to alternating and interweaving algorithms which combine two or more of the various DAs in interesting ways.

\subsection{Alternating algorithms}\label{sec:Algs:alt}
Using the full conditional distributions defined in Section \ref{sec:Algs:base}, we can construct several alternating algorithms based on any two of the DA algorithms defined there. The algorithms have the form of Algorithm \nameref{alg:Alt}. For example, the {\it State-SD alternating sampler} which alternates between the states and the scaled disturbances, obtains the $k+1$'st iteration of $(V,W)$ from the $k$'th as follows:
\begin{align*}
&[\theta|V^{(k)},W^{(k)}] \to [V^{(k+0.5)},W^{(k+0.5)}|\theta] \to\\
&[\gamma|V^{(k+0.5)},W^{(k+0.5)}] \to [V^{(k+1)}|W^{(k+0.5)},\gamma] \to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
The first line is an iteration of the state sampler while the second line is an iteration of the scaled disturbance sampler. No additional work is necessary to link up the two iterations --- we simply plug in the values of $V$ and $W$ obtained from the state sampler iteration into the draw of $\gamma$ from step one of the scaled disturbance sampler iteration.

Each other alternating algorithm is analogous and can be constructed without any additional complications. The order in which the base algorithms are used within an alternating algorithm could in principle affect the convergence properties of the algorithm. In practice we find this typically is not important. There is one additional wrinkle added by considering alternating algorithms which combine more than two base samplers. This class of samplers is not any more complicated other than adding a couple extra steps. 

%For example, the {\it State-SD-SE} or {\it Triple alternating sampler}, which alternates between the states, the scaled errors, and the scaled disturbances, is as follows:
%\begin{align*}
%&[\theta|V^{(k)},W^{(k)}] \to [V^{(k+1/3)},W^{(k+1/3)}|\theta] \to\\
%&[\gamma|V^{(k+1/3)},W^{(k+1/3)}] \to [V^{(k+2/3)}|W^{(k+1/3)},\gamma] \to [W^{(k+2/3)}|V^{(k+2/3)%},\gamma]\to \\
%&[\psi|V^{(k+2/3)},W^{(k+2/3)}] \to [V^{(k+1)}|W^{(k+2/3)},\psi] \to [W^{(k+1)}|V^{(k+1)},\psi].
%\end{align*}
%Once again, each of the required conditional distributions are defined in Section \ref{sec:Algs:base} so there is no real extra difficulty.

The naming convention we use for these algorithms is to list each DA in the order in which they appear in the alternating sampler, separated by hyphens. We shorten the scaled disturbances to ``SD'', the scaled errors to ``SE'', and the wrongly-scaled version of each to ``WSD'' and ``WSE'' respectively. So for example, the alternating sampler which alternates between the scaled disturbances and the wrongly-scaled disturbances, in that order, we call the {\it SD-WSD alternating sampler} or {\it SD-WSD Alt}. The main purpose of these algorithms is to use as a baseline for evaluating the GIS algorithms based on the same sets of DAs.

\subsection{GIS Algorithms}\label{sec:Algs:GIS}
We can use the various DAs of Section \ref{sec:DAs} in order to construct interweaving algorithms in addition to the alternating algorithms of the previous section. We will start with Algorithm \nameref{alg:GIS} on page \pageref{alg:GIS}. Given the full conditional distributions listed in Section \ref{sec:Algs:base}, the only additional ingredients we need are the densities of one DA conditional on the other and the data -- for example, $p(\gamma|\theta,y)$ and $p(\psi|\theta,y)$. In practice, it is easier to use Algorithm \nameref{alg:GIS2} and obtain a joint draw from $V$, $W$ and the new DA, e.g. from $(V,W,\gamma|\theta,y)$.

So the only additional requirement in order to construct a GIS algorithm is to use the definitions of the various available DAs in order to perform the one-to-one transformations from any one DA to another. For example, in the {\it State-SD GIS sampler} we obtain $(V^{(k+1)},W^{(k+1)})$ from $(V^{(k)},W^{(k)})$ as follows:
\begin{align*}
&[\theta|V^{(k)},W^{(k)}] \to [W^{(k+0.5)},V^{(k+0.5)}|\theta] \to\\
&[\gamma|V^{(k+0.5)},W^{(k+0.5)},\theta] \to [V^{(k+1)}|W^{(k+0.5)},\gamma] \to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
The first step of the second line is deterministic: we transform $\theta$ to $\gamma$ by means of the defining equations for $\gamma$: $\gamma_0=\theta_0$ and $\gamma_t = L_W^{-1}(\theta_t - G_t\theta_{t-1})$ for $t=1,2,\cdots,T$ where $L_W$ is the Cholesky decomposition of $W$.

There are often some small improvements that can be made simply by thinking clearly about what the GIS algorithm is doing. For example in the above version of the State-SD GIS sampler, the draw of $V$ in step two of line one and the draw of $V$ in step two of line two are redundant --- they come from the same distribution and only the last one is ever used in later steps. As a result, we can remove the second draw in order to save computational time so that the State-SD GIS sampler is as follows:
\begin{alg*}[State-SD GIS]State-Scaled Disturbance GIS Sampler
\label{alg:DLMstateerror}
\begin{enumerate}
\item Use the MCFA to sample $\theta \sim p(\theta|V,W,y)$.
\item Sample $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$ and $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$ independently.
\item Transform $\theta$ to $\gamma$.
\item Sample $W \sim p(W|V,\gamma,y)$.
\end{enumerate}
\end{alg*}\noindent

%As another example, the {\it SD-SE GIS sampler} is as follows:
%\begin{alg*}[SD-SE GIS]Scaled Disturbance-Scaled Error GIS Sampler
%\label{alg:DLMdisterror}
%\begin{enumerate}
%\item Use the MCFA to sample $\theta \sim p(\theta|V,W,y)$.
%\item Sample $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$.
%\item Transform $\theta$ to $\gamma$.
%\item Sample $W \sim p(W|V,\gamma,y)$.
%\item Transform $\gamma$ to $\psi$.
%\item Sample $V \sim p(V|W,\psi,y)$.
%\item Sample $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
%\end{enumerate}
%\end{alg*}\noindent
%Note that like in the scaled disturbance algorithm, we draw $V$ in step two before transforming from $\theta$ to $\gamma$ in step three since the conditional distribution of $V$ is unaffected by this transformation, and it is less costly the compute the hyperparameters of $V$ inverse Wishart distribution from $\theta$.

The naming convention for GIS algorithms is similar to that of alternating algorithms --- DAs appear in the name in the order that they appear in the algorithm, separated by hyphens, e.g. a GIS algorithm based on the states, scaled disturbances and scaled errors, in that order, would be called the {\it State-SD-SE GIS sampler}. There is no additional difficulty encountered by using a GIS with greater than two DAs. Also like alternating algorithms, the performance of GIS algorithms may depend on the order in which the DAs are used. \citet{yu2011center} note that this tends not to make much difference, which is consistent with our own experience.

\subsection{CIS algorithms}\label{sec:Algs:CIS}
Next we consider CIS algorithms which have the form of Algorithm \nameref{alg:CIS}. The advantage of using CIS is that it is sometimes possible to find an AA-SA pair of DAs for each part of the parameter vector even when no such pair of DAs exist for the entire parameter vector. From Section \ref{sec:DAs}, we know that the scaled disturbances and the wrongly-scaled disturbances form an AA-SA pair for $W|V$ while the scaled errors and the wrongly-scaled errors form an AA-SA pair for $V|W$.

A CIS sampler based on these AA-SA pairs obtains $(V^{(k+1)},W^{(k+1)})$ from $(V^{(k)},W^{(k)})$ as follows:
\begin{align*}
&[\psi|V^{(k)},W^{(k)}] \to [V^{(k+0.5)}|W^{(k)},\psi] \to [\tilde{\psi}|V^{(k+0.5)},W^{(k)},\psi] \to [V^{(k+1)}|W^{(k)},\tilde{\psi}]\to\\
&[\tilde{\gamma}|V^{(k+1)},W^{(k)},\tilde{\psi}] \to [W^{(k+0.5)}|V^{(k)},\tilde{\gamma}] \to [\gamma|V^{(k+1)},W^{(k+0.5)},\tilde{\gamma}]\to [W^{(k+1)}|V^{(k+1)},\gamma].
\end{align*}
The first line is essentially a Gibbs step for drawing $V$ that interweaves between $\psi$ and $\tilde{\psi}$ while the second line is essentially a Gibbs step for drawing $W$ that interweaves between $\gamma$ and $\tilde{\gamma}$. In the second line we use the SA before the AA in order to minimize the number of transformations we have to make in every iteration. Notice that each time one of the wrongly-scaled DAs appears in the CIS sampler, it would make no difference if the states were used instead because $p(V|W,\tilde{\psi},y)=p(V|W,\theta,y)$ and $p(W|V,\tilde{\gamma},y)=p(W|V,\theta,y)$ --- in both cases the density is inverse Wishart. This is despite the fact that we know from Section \ref{sec:DAs} the states are only an SA for $W|V$ but not for $V|W$. Using this fact we obtain a slightly different version of the CIS sampler in Algorithm \nameref{alg:DLMcis}.
\begin{alg*}[CIS]Componentwise Interweaving Sampler\label{alg:DLMcis}
\begin{enumerate}
\item Use the MCFA to sample $\psi \sim p(\psi|V,W,y)$.
\item Sample $V \sim p(V|W,\psi,y)$.
\item Transform $\psi$ to $\theta$.
\item Sample $V \sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right)$.
\item Sample $W \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)$.
\item Transform $\theta$ to $\gamma$.
\item Sample $W \sim p(W|V,\gamma,y)$.
\end{enumerate}
\end{alg*}

\jarad{matt: do we need this or can we just jump to the latter algorithm?} We can rearrange the order in which the Gibbs steps for $V$ and $W$ are taken within the CIS algorithm and change the order in which the AAs and SAs are used with the Gibbs steps to obtain another version of the CIS algorithm:
\begin{align*}
&[\gamma|V^{(k)},W^{(k)}] \to [W^{(k+0.5)}|V^{(k)},\gamma] \to [\theta|V^{(k)},W^{(k+0.5)},\gamma] \to [W^{(k+1)}|V^{(k)},\theta]\to \\
&[V^{(k+0.5)}|W^{(k+1)},\theta] \to [\psi|V^{(k+0.5)},W^{(k+1)},\theta] \to [V^{(k+1)}|W^{(k+1)},\psi].
\end{align*}
Since the draw of $V$ is from the same density whether we condition on $\theta$ or $\gamma$ and similar for $W$ given $\theta$ or $\psi$ and since $V$ and $W$ are conditionally independent given $\theta$ and $y$, we can move the draw of $V^{(k+0.5)}$ to an earlier step in the iteration and the draw of $W^{(k+1)}$ to a later step in the iteration without changing the MCMC properties of the algorithm in order to obtain:
\begin{align*}
&[\gamma|V^{(k)},W^{(k)}] \to [V^{(k+0.5)}|W^{(k)},\gamma] \to[W^{(k+0.5)}|V^{(k+0.5)},\gamma] \to \\
&[\psi|V^{(k+0.5)},W^{(k+0.5)}]\to [V^{(k+1)}|W^{(k+0.5)},\psi]\to [W^{(k+1)}|V^{(k+1)},\psi].
\end{align*}
This is a slightly rearranged version of the SD-SE GIS sampler. Since in most cases simply rearranging the steps of an MCMC sampler does little to impact the convergence and mixing properties of the sampler, we expect the CIS and SD-SE GIS samplers to perform about the same along this axis so that computational cost per iteration and ease of implementation are the only real considerations involved in choosing between the two.

In our original definition of the CIS sampler for the DLM we used the scaled disturbances as the AA for $W$ and the scaled errors and the AA for $V$. We could have reversed this or used the same AA for both $V$ and $W$ since both the scaled errors and scaled disturbances are AAs for $(V,W)$, or we can have used $\theta$ as the AA for $V$. In each of these cases, the resulting algorithm would reduce to either the state sampler or a {\it partial CIS} algorithm, also introduced by \citet{yu2011center}. A partial CIS algorithm is similar to a CIS algorithm except it uses an ordinary DA step instead of an interweaving step for one of the parameter blocks. So given the DAs we have available, we have one CIS algorithm to go along with 5 base samplers and a large number of GIS and alternating samplers. We now desire to characterize the efficiency of these samplers, both in terms of computational cost and in terms of the mixing and convergence of the Markov chain. We will do this in the next section using class of DLMs known as the local level model.

\section{Application: The Local Level Model}\label{sec:LLM}

\subsection{The local level model and its DAs}

The local level model (LLM) is a DLM with univariate data $y_t$ for $t=1,2,\cdots,T$ and a univariate latent state $\theta_t$ for $t=0,1,\cdots,T$. In the general DLM notation, $F_t=1=G_t=1$ for all $t$. As a result, $V$ and $W$ are scalar. We can write the model as
\begin{align}
  y_t |\theta,V,W& \stackrel{ind}{\sim} N(\theta_t,V), &
  \theta_t |\theta_{0:t-1},V,W& \sim N(\theta_{t-1},W)
\end{align}
for $t=1,2,\cdots,T$. The priors on $(\theta_0,V,W)$ from Section \ref{sec:DLM} simplify to $\theta_0\sim N(m_0,C_0)$, $V\sim IG(\alpha_V,\beta_V)$ and $W\sim IG(\alpha_W,\beta_W)$ with $\theta_0$, $V$ and $W$ mutually independent and $IG(\alpha,\beta)$ is the inverse gamma distribution with shape parameter $\alpha$ and rate parameter $\beta$.

Now we can define the various DAs from Section \ref{sec:DAs} in the context of the local level model. The usual DA, the states, is simply $\theta$. From the states we obtain the scaled disturbances as $\gamma_0=\theta_0$ and $\gamma_t = (\theta_t - \theta_{t-1})/\sqrt{W}$ for $t=1,2,\cdots,T$. Similarly, the scaled errors are $\psi_0=\theta_0$ and $\psi_t = (y_t - \theta_t)/\sqrt{V}$ for $t=1,2,\cdots,T$. The wrongly-scaled disturbances are then $\tilde{\gamma}_{0}=\theta_0$ and $\tilde{\gamma}_t = (\theta_t - \theta_{t-1})/\sqrt{V}$ while the wrongly-scaled errors are $\tilde{\psi}_0=\theta_0$ with $\tilde{\psi}_t = (y_t - \theta_t)/\sqrt{W}$ for $t=1,2,\cdots,T$. %As in previous sections, we will drop the subscript whenever we refer to the entire vector, i.e. $\theta=\theta_{0:T}$ and $y=y_{1:T}$.

\subsection{Full conditionals}\label{sec:LLM:fullcond}

Here we list the full conditional distributions required to construct each of the MCMC samplers in Section \ref{sec:Algs} for the LLM. Derivations of most of these conditionals can be found in the supplementary materials (Appendix B). For all algorithms, we use the MCFA to draw the DA or to draw $\theta$ and then transform to the DA. For $V$, $p(V|W,\theta,y)=p(V|W,\gamma,y)=p(V|W,\tilde{\psi},y)$ so that in each case $V$ has an $IG(a_V,b_V)$ distribution where $a_V = \alpha_V + T/2$ and $b_V = \beta_V + \sum_{t=1}^T(y_t - \theta_t)^2/2$. Similarly for $W$, $p(W|V,\theta,y)=p(W|V,\psi,y)=p(W|V,\tilde{\gamma},y)$ so that in each case $W$ has an $IG(a_W, b_W)$ distribution where $a_W = \alpha_W + T/2$ and $b_W = \beta_W + \sum_{t=1}^T(\theta_t - \theta_{t-1})^2/2$.


In order to construct the algorithms based on $\gamma$, we also need $p(W|V,\gamma,y)$. Similarly to construct the algorithms baseD on $\psi$, we need $p(V|W,\psi,y)$. In Appendix B of the supplementary materials, we show that both of these densities have the form $p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b\sqrt{x} -c/x\right]$. For $p(W|V,\gamma,y)$, $\alpha=\alpha_W$, $a=a_\gamma \equiv \sum_{t=1}^T(\sum_{j=1}^t\gamma_j)^2/2V$, $b=b_\gamma \equiv\sum_{t=1}^T(y_t-\gamma_0)(\sum_{j=1}^t\gamma_j)/V$, and $c=\beta_W$. Similarly, for $p(V|W,\psi,y)$, $\alpha=\alpha_V$, $a=a_{\psi}\equiv\sum_{t=1}^T(\mathcal{L}\psi_t)^2/2W$, $b=b_{\psi}\equiv\sum_{t=1}^T(\mathcal{L}\psi_t\mathcal{L}y_t)/W$, and $c=\beta_V$ where we define $\mathcal{L}y_t=y_t-y_{t-1}$ for $t=2,3,\cdots,T$, $\mathcal{L}y_1=y_1 - \psi_0$, $\mathcal{L}\psi_t = \psi_t - \psi_{t-1}$ for $t=2,3,...,T$ and $\mathcal{L}\psi_1=\psi_1-0$. \jarad{matt: check appendix for Ls} In Appendix C of the supplementary materials, we show that this density is often log concave in which case adaptive rejection sampling \citep{gilks1992adaptive} works well. When the density is not log concave, we use a $t$ approximation in a rejection sampler for $\log(x)$.

The constructions of Algorithms \nameref{alg:DLMwdist} and \nameref{alg:DLMwerror} also require $p(W|V,\tilde{\psi},y)$ and $p(V|W,\tilde{\gamma},y)$ respectively. Both densities have the form $p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b/\sqrt{x} -c/x\right]$, which is closely related to the difficult density from the correctly scaled samplers above.  For $p(V|W,\tilde{\gamma},y)$ we show in Appendix B of the supplementary materials that $\alpha=\alpha_V$, $a = a_{\tilde{\gamma}}\equiv\frac{1}{2W}\sum_{t=1}^T\tilde{\gamma}_t^2$, $b =b_{\tilde{\gamma}}\equiv \sum_{t=1}^T(y_t - \tilde{\gamma}_0)\sum_{s=1}^t\tilde{\gamma}_s$, and $c =c_{\tilde{\gamma}}\equiv \beta_V + \frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\gamma}_0)^2$ while for $p(W|V,\tilde{\psi},y)$ we show that $\alpha=\alpha_W$,   $a =a_{\tilde{\psi}}\equiv \frac{1}{2V}\sum_{t=1}^T\tilde{\psi}_t^2$,  $b =b_{\tilde{\psi}}\equiv \sum_{t=1}^T\mathcal{L}\tilde{y}_t\mathcal{L}\tilde{\psi}_t$, and $c =c_{\tilde{\psi}}\equiv \beta_W + \frac{1}{2}\sum_{t=1}^T\mathcal{L}\tilde{y}_t^2$. This density is harder to sample from because adaptive rejection sampling does not work very well, so we construct a rejection sampler on the log scale using a $t$ approximation in Appendix F of the supplementary materials.

%Given the densities discussed above, all of the pieces are in place to construct each of the base, alternating and interweaving algorithms from Section \ref{sec:Algs} for the LLM. In the next secion, we explore the efficacy of each of these algorithms through simulations.

\subsection{Simulation Setup}

In order to test these algorithms, we simulated data from the local level model for various choices of $V$, $W$, and $T$. We created a full factorial design with $V$ and $W$ each taking the values $10^{i/2}$ where $i=-4,-3,\cdots,4$ and with $T$ taking the values $10, 100, 1000$. Then for each dataset, we fit the local level model using a variety of the algorithms discussed in this paper. We used the same rule for constructing priors for each model: $\theta_0\sim N(0,10^7)$, $V\sim IG(5, 4V^*)$, and $W\sim IG(5, 4W^*)$, mutually independent where $(V^*,W^*)$ are the true values of $V$ and $W$ used to simulate the time series. Note that the prior means are equal to the true values of $V$ and $W$, so both the prior and likelihood and thus the posterior roughly agree about the likely values of $V$ and $W$. Ultimately, the reason for varying the prior with the true parameter values is that the results are useful \jarad{useful?} --- intuitively, it turns out the behavior of each of these samplers depends on where in the parameter space the posterior distribution puts most of its mass.

For each dataset and each sampler, we obtained $n=3000$ draws and threw away the first $500$. The chains were started at the true values used to simulate the time series, so we can examine the behavior of the chains to determine how well they mix but not how quickly they converge. Define the effective sample proportion ($ESP=ESS/n$) for a scalar component of the chain as the effective sample size ($ESS$) \citep{gelman2013bayesian} of the component divided by the actual sample size $n$. When $ESP=1$ the Markov chain is behaving as if it obtains iid draws from the posterior. It is possible to obtain $ESP>1$ if the draws are negatively correlated and this occasionally happens in our simulations. When it does happen the correlation is weak and we round down to one for plotting purposes.

\subsection{Simulation results}

The top panel of Figure \ref{ESplot} contains plots of ESP for $V$ and $W$ in each chain of each base samplers for $T=100$ --- we omit the $T=10$ and $T=1000$ plots for brevity, but they can be found in the supplementary materials (Appendix H). The State sampler has a low ESP for $V$ and a high ESP for $W$ when the true signal-to-noise ratio, $R^*=W^*/V^*$, is larger than one. In contrast, when $R^*<1$ the State sampler has a low ESP for $W$ and a high ESP for $V$. To summarize, the state sampler mixes poorly for whichever of $V$ or $W$ is smaller. The WSD and WSE samplers are similar to the state sampler except WSD has lower ESP for $V$ and WSE has lower ESP for $W$. The SD sampler has low ESP for both $V$ and $W$ when $R^*>1$, and it is particularly low for $W$. The SE sampler behaves almost exactly opposite ---it has low ESP for both when $R^*<1$ and in particular for $V$. We omit the results here, but as $T$ increases, in all samplers the region of the parameter space with high ESP shrinks and in the low ESP regions, ESP drops closer to zero. We summarize some of the above results for convenience in the top panel of Table \ref{tab:stnmix}. In Appendix G of the supplementary materials, we discuss how the pattern of correlations between various quantities in the posterior distribution roughly determines the pattern of ESPs we see in Figure \ref{ESplot}. \jarad{can we state anything succinctly?}

\begin{figure}[!h]
\centering
\includegraphics[width=0.75\textwidth]{basecisESplot100}
\includegraphics[width=0.49\textwidth]{altintESplotV100}
\includegraphics[width=0.49\textwidth]{altintESplotW100}
\caption{Effective sample proportion in the posterior sampler for a time series of length $T=100$, for $V$ and $W$ in the each sampler. The top panel contains ESP for $V$ and $W$ for the base and CIS samplers, the bottom left panel contains ESP for $V$ in the Alt and GIS samplers, and the bottom right panel contains ESP for $W$ in the Alt and GIS samplers. $X$ and $Y$ axes indicate the true values of $V$ and $W$ respectively for the simulated data. Note that the signal-to-noise ratio is constant moving up any diagonal. In the upper left the signal is high, in the lower right the noise is high.}
\label{ESplot}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{lccccc}\hline
    Parameter & State & SD & SE & WSD & WSE \\\hline
    V & $R^* < 1$ & $R^* < 1$ & $R^* > 1$ & $R^* < 1$ & $R^* < 1$\\
    W & $R^* > 1$ & $R^* < 1$ & $R^* > 1$ & $R^* > 1$ & $R^* > 1$ \\
    \hline
      &           &          &           &           &           \\\hline
    Parameter & State-SD        & State-SE       & SD-SE        & Triple            & CIS \\\hline
    V         & $R^* < 1$           & $R^* \not\approx 1$ & $R^* \not\approx 1$ & $R^* \not\approx 1$ & $R^* \not\approx 1$ \\
    W         & $R^* \not\approx 1$ & $R^* > 1$           & $R^* \not\approx 1$ & $R^* \not\approx 1$ & $R^* \not\approx 1$\\\hline
  \end{tabular}
  \caption{Rule of thumb for when each sampler has a high ESP for each variable as a function of the true signal-to-noise ratio, $R^*=W^*/V^*$. Note that as the length of the time series increases, the farther away from one $R^*$ has to be for the sampler to have a high ESP.}
  \label{tab:stnmix}
\end{table}

We fit the LLM to the simulated datasets using several GIS samplers and a CIS sampler as well. Since the wrongly-scaled samplers behaved similarly to the state sampler and neither of the underlying DAs were a SA for $V$ and $W$ jointly, we ignored them in the construction of the GIS samplers. Instead, we constructed the State-SD, State-SE, SD-SE, and Triple (State-SD-SE) GIS samplers, as well as the CIS sampler. The bottom two panels of Figure \ref{ESplot} has plots of ESP for each of the GIS algorithms while the top panel has the plots of ESP for the CIS sampler. First, the State-SD GIS algorithm has high ESP for $W$ except for a narrow band where $R^*$ is near one while its mixing behavior for $V$ appears identical to the original State sampler --- high ESP when $R^* < 1$ and poor ESP when $R^* > 1$. So this algorithm takes advantage of the fact that the state and SD DA algorithms make up a ``beauty and the beast'' pair for $W$ and thus improves mixing in the marginal chain for $W$. Similarly the State-SE GIS algorithm's ESP for $W$ is essentially identical to the State and SE algorithms' ESP for $W$ while its ESP for $V$ is high ESP when $R^*$ is far enough away from one. The SD-SE GIS algorithm combines the best of both worlds --- when $R^*$ is not too close to one it has high ESP for both $V$ and $W$, though it has extra trouble with $W$ when $R^*$ is less than one but not small enough. 

We showed in Section \ref{sec:Algs:CIS} that the CIS and the SD-SE GIS algorithms consist of the same steps, just rearranged. This suggests that they should perform similarly and in fact the SD-SE GIS algorithm behaves essentially identically to the CIS and Triple GIS algorithms. The second of these is somewhat surprising --- we had some hope that the Triple GIS algorithm would improve upon the SD-SE GIS algorithm somewhat by further breaking the correlation between iterations in the Markov chain.

We also fit each Alt sampler that corresponds to one of the GIS samplers we fit, that is the State-SD, State-SE, SD-SE and Triple (State-SD-SE) Alt samplers. The bottom two panels of Figure \ref{ESplot} also contains the ESP of these samplers for both $V$ and $W$. There appears to be little practical difference between the two categories of algorithms --- both take advantage of the beauty and the beast nature of their underlying DAs, but apparently dependence between the two DAs prevents the GIS version of the sampler from improving on the Alt version. All of the alternating and interweaving results are summarized in the bottom panel of Table \ref{tab:stnmix}. We include simulations with differing sizes of $T$ using these samplers in the supplementary materials (Appendix H), but the upshot is that, like with the base samplers, increasing the length of the time series worsens ESP for both $V$ and $W$ in all samplers and in particular shrinks the area of the parameter space in which ESP is high.

\subsection{Computational time}\label{sec:LLM:time}

From a practical standpoint a more important question than how well the chain mixes is the full computational time required to adequately characterize the target posterior distribution. In order to investigate this, we compute the natural log of the average time in minutes required for each sampler to achieve an effective sample size of 1000 --- in other words the log minutes per 1000 effective draws. All simulations were performed on a server with Intel Xeon X5675 3.07 GHz processors. While different systems will yield different absolute times, the relative times should be similar. Figure \ref{baseinttimeplot} contains plots of the log minutes per 1000 effective draws for both $V$ and $W$ and for each of the samplers.

For $T=100$ the pattern we saw for ESP also appears for log minutes per 1000 effective draws. The State sampler becomes slow to reach 1000 effective draws for $V$ when $R^*>1$ and for $W$ when $R^*<1$. The SD and SE samplers behave as expected --- the SD sampler is slow for both $V$ and $W$ when $R^*>1$ while the SD sampler is slow for both $V$ and $W$ when $R^*<1$. The SD-SE GIS, Triple GIS and CIS algorithms appear to be the big winners here and are almost indistinguishable. All three algorithms are slightly slower for both $V$ and $W$ when $R^*$ is near one, though for larger $T$,  when $R^*$ is near or below one all three are slow for $W$ (plots available in Appendix H of the supplementary materials). Compared to the state sampler, all three offer large gains over most of the parameter space. There appears to be no difference between a GIS algorithm and the corresponding alternating algorithm in terms of log time per 1000 effective draws, so the SD-SE Alt and Triple Alt algorithms are both just as efficient as the best interweaving algorithms. We favor the SD-SE GIS and Alt algorithms because they are the easiest to implement.

\begin{figure}[!h]
\centering
\includegraphics[width=0.75\textwidth]{basecistimeplot100}
\includegraphics[width=0.49\textwidth]{altgisVtimeplot100}
\includegraphics[width=0.49\textwidth]{altgisWtimeplot100}
\caption{Log of the time in minutes per 1000 effective draws in the posterior sampler for $V$ and $W$, for $T=100$ in each sampler. Log times larger than three $\log \mathrm{min}$ are rounded down to three for plotting purposes.}
\label{baseinttimeplot}
\end{figure}

\section{Discussion}\label{sec:Discuss}

\jarad{matt: ideally our work is 1 paragraph}

The importance of reparameterization in DA algorithms is well known \citep{papaspiliopoulos2007general}, both in term of reparameterizing the model parameters and reparameterizing the DA. In order to explore reparameterizing the DA in dynamic linear models, we start with two DAs --- the usual DA, called the latent states, and the scaled disturbances, often called the non-centered disturbances, obtained by centering and scaling the latent states to create a pivotal quantity. In addition, we introduce three new DAs for the DLM: the scaled errors, the wrongly-scaled disturbances, and the wrongly-scaled errors. We introduce these DAs for the purpose of applying the interweaving strategies of \citet{yu2011center}. In particular we apply GIS, obtained by interweaving between two separate DAs in a manner similar to an alternating strategy, and CIS, obtained by applying GIS in a Gibbs-like structure to sub-blocks of the parameter vector. We also seek to apply ASIS, which is a GIS where the two DAs are required to be a sufficient augmentation and an ancillary augmentation. However, we find under reasonable assumptions that any sufficient augmentation for a general class of DLMs yields a full conditional distribution for the model parameters that is as difficult to sample from as the target posterior distribution of the model parameters. So ASIS probably is not a fruitful sampling strategy in the DLM, though perhaps with some creativity a suitable SA can be found.

With available DAs, we construct each possible DA algorithm, several GIS algorithms and their corresponding Alt algorithms, and a CIS algorithm for the general DLM. The full conditional posterior of the model parameters given any of the scaled DAs turns out to be complicated and difficult to sample from. As a result, we are forced to draw the system variance and observation variance in two steps instead of one. One of the these two draws is still difficult, but at least in the local level model it is manageable. Using the local level model as an example, we explore the mixing properties of many of the available DA, GIS, Alt, and CIS algorithms by fitting the model to simulated datasets. We find that the true signal-to-noise ratio, $R^*=V^*/W^*$, is important for determining when each algorithm performs well, and in addition that there appears to be no substantive difference between a GIS algorithm an its corresponding Alt algorithm. In fact, the three best performing algorithms under most circumstances are the SD-SE GIS algorithm, the SD-SE Alt algorithm and the CIS algorithm. In some regions of the parameter space all three algorithms are substantially faster than the commonly used State sampler, though in other regions the difference is small.

\jarad{matt: signal-to-noise down to 1 paragraph}

The importance of the true signal-to-noise ratio in DLMs to the mixing and convergence properties of various MCMC algorithms has been anticipated in the literature. In the AR(1) plus noise model, \citet{pitt1999analytic} find that the signal-to-noise ratio along with the AR(1) coefficient determine the convergence rate of a Gibbs sampler. In addition, they find that the convergence rate decreases as the length of the time series increases, which is consistent with our empirical findings in the local level model. When \citet{fruhwirth2004efficient} study the dynamic regression model with a stationary AR(1) process on the regression coefficient, they use both the states and the scaled disturbances (non-centered disturbances) and several other DAs motivated by some results for Gibbs samplers in the hierarchical model literature. When they examine the behavior of the resulting DA algorithms, \citet{fruhwirth2004efficient} find that the relative behavior of the SD sampler and the State sampler depends on a function of the true signal-to-noise ratio that also depends on the true value of the autocorrelation parameter and the distribution of the covariate. In addition none of the other DA algorithms they consider are more efficient than both the state sampler and the SD sampler. This holds true both when they assume that the autocorrelation parameter is known and when it is assumed unknown and must be estimated. Their result is encouraging since the additional model complexity from adding a new parameter could alter the properties of the DA algorithms based on an AA, but it appears the differences are relatively small.

While it is clear that the signal-to-noise ratio is the key quantity for determining how well one of MCMC algorithms we discussed will perform in the local level model, in the general DLM we do not know whether the signal-to-noise ratio has any relevance. However, given the results of previous work discussed above it is likely that the signal-to-noise ratio will in some way determine how well each algorithm performs. The precise manner in which convergence and mixing behavior depends on the true signal-to-noise ratio is unknown, but it is likely a consequence of the relevance of the Bayesian fraction of missing information and the related EM fraction of missing information to the performance of the DA and EM algorithms (see \citet{van2001art} for a good explication of both concepts).

%\subsection{Improving computational efficiency}

\jarad{matt: one paragraph on bottleneck and one paragraph on other priors}

A major computational bottleneck in most of our algorithms occurs when we have to draw from $p(W|V,\gamma,y)$, $p(V|W,\psi,y)$, $p(V|W,\tilde{\gamma},y)$ or $p(W|V,\tilde{\psi},y)$. The densities $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$ have the form
\[
p(x)\propto x^{-\alpha-1}\exp\left[-ax + b\sqrt{x} - c/x\right],
\]
while the densities $p(W|V,\tilde{\psi},y)$ and $p(V|W,\tilde{\gamma},y)$ have the form
\[
p(x)\propto x^{-\alpha-1}\exp\left[ -ax + b/\sqrt{x} -c/x\right]
\]
where $\alpha,a,c>0$ and $b\in\Re$. When $b=0$ we have a special case of the generalized inverse Gaussian (GIG) distribution, so perhaps the methods used to speed up draws from the GIG can be used here \citep{jorgensen1982statistical,dagpunar1989easily,devroye2012random}. On the other hand, it might be worth putting effort into drawing $V$ and $W$ jointly conditional on the (wrongly) scaled disturbances or the (wrongly) scaled errors. Using the scaled disturbances, the conditional distribution of $V$ given $W$ is inverse gamma in the LLM and inverse Wishart in the general DLM, so it is easy to derive the marginal density $p(W|\gamma,y)$ up to a proportionality constant. In our LLM example, this density turns out to be very difficult to sample from and, in particular, it is not easy to come up with a generally good approximation for rejection sampling or for a Metropolis step.

The main problem is that the augmented data likelihood and the prior for the variance we used to scale the states combine to create a complicated full conditional for that variance, e.g. $p(y,\gamma|V,W)$ and $p(W)$, combine to create a complicated full conditional for $W$. We chose inverse Wishart priors for $V$ and $W$ partially because they are standard and partially because their conditional conjugacy with the states is computationally convenient, but outside of the state sampler there may be a more convenient prior. In addition, there are well known inferential problems with the inverse Wishart prior in the hierarchical model literature, e.g. \citet{gelman2006prior} and \citet{alvarez2014cov}, though it is unclear whether this transfers over to DLMs or more generally any time series model. An alternative is to use the conditionally conjugate prior conditional on the scaled disturbances, or whichever DA we prefer.

In the LLM, the conditionally conjugate prior for $\sqrt{W}$ using the scaled disturbances as the DA is a Gaussian distribution --- strictly speaking this prior is on $\pm \sqrt{W}$. If we use this prior for $\pm\sqrt{V}$ as well, the $V$ step in the scaled disturbance sampler becomes a draw from the generalized inverse Gaussian distribution. This prior has been used by \citet{fruhwirth2011bayesian} and \citet{fruhwirth2008bayesian} to speed up computation while using the scaled disturbances in hierarchical models and by \citet{fruhwirth2010stochastic} for time series models with a DA similar to the scaled disturbances (the latent states are scaled, but not centered). We omit the results here, but using this prior on both variances does not alter our mixing results for any of the MCMC samplers --- effective sample sizes were basically the same with either prior. There is a trade-off in computation time to consider, however. For example when using the scaled disturbances, the draw of $W|V,\gamma,y$ is sped up by using the Gaussian prior on $\pm\sqrt{W}$ since it becomes a Gaussian draw while the $V|W,\gamma,y$ is slower since it becomes a generalized inverse Gaussian draw instead of an inverse gamma. The gains outweigh the costs, at least in the local level model.

In the general DLM, however, it is unclear whether this will hold. In particular in the scaled disturbance sampler, when $V$ is a matrix its full conditional becomes a matrix analogue of the generalized inverse Gaussian distribution while when $W$ is a matrix, the nonzero elements of the Cholesky decomposition of $W$ now have a Gaussian prior and Gaussian full conditional posterior. The former is likely difficult to draw from efficiently and the latter requires additional matrix computations during every iteration. On the other hand with the inverse Wishart priors, when $W$ is a matrix the full conditional of $W$ becomes a matrix analogue of the density in Section \ref{sec:LLM:fullcond}, and this is more complicated than the matrix analogue of the GIG distribution.

\newpage
\section{Supplementary Materials}\label{sec:Supp}
All supplementary materials are available in a single \verb0.zip0 file.
\begin{description}
\item[appendices.pdf:] This file provides 7 appendices cited in the main article:
\begin{itemize}
\item[] Appendix A: Proof of lemma 1.
\item[] Appendix B: Derivations of joint distributions and full conditional distributions for each DA in the DLM.
\item[] Appendix C: Mixed Cholesky factorization algorithm for simulation smoothing.
\item[] Appendix D: Further augmentation for non-invertible $F_t$.
\item[] Appendix E: Efficiently drawing from $p(W|V,\gamma,y)$ and $p(V|W,\psi,y)$ in the LLM.
\item[] Appendix F: Efficiently drawing from $p(W|V,\tilde{\gamma},y)$ and $p(V|W,\tilde{\psi},y)$ in the LLM.
\item[] Appendix G: Using posterior correlations to understand patterns of ESP.
\item[] Appendix H: Further plots of ESP and log time per 1000 effective draws for different values of $T$.
\end{itemize}

\item[mcfa.R:] This file contains R code to perform the MCFA for obtaining a draw from $p(\theta|V,W,y)$ and from $p(\psi|V,W,y)$ in the local level model.

\item[dlmasisfun.R:] This file contains several R functions for obtaining the simulations from a variety of MCMC algorithms in the local level model, summarizing those simulations appropriately, and for simulating data from the model.

\item[dlmasismixrun.R:] This file contains R code that uses the functions in \verb0dlmasisfun.R0 and \verb0mcfa.R0 to obtain the simulations used in the local level model example from Section \ref{sec:LLM}.

\item[plots.R:] This file contains R code that uses the output from \verb0dlmasismixrun.R0 to create the various plots in the main document and the appendices.

\end{description}


\clearpage
\bibliographystyle{MWS_jasa}  % proper bibliography style for ASA
\bibliography{dlmasis}
\end{document}
