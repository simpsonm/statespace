\documentclass{article}

\usepackage{JASA_manu} %formats document like ASA wants
\usepackage{jasa_harvard} %formats citations like ASA wants
\usepackage{amssymb, amsmath, amsthm, graphics, color, fullpage}
%\usepackage[authoryear]{natbib} %numbers instead of authoryear for [1] instead of [1980]

\newtheorem{alg}{Algorithm}
\newtheorem{thm}{Theorem}[subsection]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\B}{B}
\DeclareMathOperator{\vech}{vech}
\DeclareMathOperator{\vect}{vec}

\graphicspath{{plots/}}
\newcommand{\matt}[1]{{\color{red} Matt: #1}}
\newcommand{\jarad}[1]{{\color{red} Jarad: #1}}


\begin{document}


\title{Ancillarity-Sufficiency or not; Interweaving to Improve Markov Chain Monte Carlo (MCMC) Estimation of the Dynamic Linear Model (DLM)}
\author{Matt Simpson}
\maketitle

\begin{abstract}
In DLMs, MCMC sampling can often be very slow for accurately estimating the posterior density --- especially for longer time series. In particular, in some regions of the parameter space the standard data augmentation algorithm can mix very slowly. Using some of the insights from the data augmentation for multilevel models literature, we explore several alternative data augmentations for a general class of DLMs and we show that no ``practical'' centered parameterization (or sufficient augmentation in the terminology of \citeasnoun{yu2011center}) exists. In addition, we utilize these augmentations to construct several interweaving algorithms --- though we cannot construct an ancillary-sufficient interweaving algorithm (ASIS) since no sufficient augmentation exists, we find two ancillary augmentations and are able to construct a componentwise interweaving algorithm (CIS) that uses ASIS for each model parameter conditional on the rest. Using the local level DLM, we show how to construct several of these algorithms and conduct a simulation study in order to discern their properties. We find that several algorithms that outperform the usual ``state sampler'' for many values of the population parameters, though there is room for improvement.
\end{abstract}


\section{Introduction}\label{sec:Intro}

Ever since the seminal article by \citeasnoun{tanner1987calculation}, data augmentation has become a common strategy for constructing MCMC algorithms to sample from intractable probability distributions. Suppose $p(\phi|y)$ is the target density, in this case the posterior distribution of some parameter $\phi$ given data $y$. We use $p(.)$ to denote the probability density of the enclosed random variables. Then the data augmentation (DA) algorithm adds a data augmentation $\theta$ with joint distribution $p(\phi,\theta|y)$ such that $\int_{\Theta}p(\phi,\theta|y)d\theta = p(\phi|y)$. Then the DA algorithm is similar to a Gibbs sampler, except it constructs a Markov chain for $\phi$ instead of $(\phi, \theta)$. The DA algorithm obtains the $k+1$'st state of $\phi$ from the $k$'th state as follows (we implicitly condition on the data $y$ in all algorithms and only superscript the previous and new draws of the model parameters of interest):
\begin{alg}Data Augmentation.\label{alg:DA}
  \begin{align*}
  [\theta|\phi^{(k)}] \to [\phi^{(k+1)}|\theta]
\end{align*}
\end{alg} 
\noindent Here $[\theta|\phi^{(k)}]$ means a draw of $\theta$ from $p(\theta|\phi^{(k)},y)$ and $[\phi^{(k+1)}|\theta]$ means a draw from $p(\phi^{(k+1)}|\theta,y)$. The DA, $\theta$, need not be interesting in any scientific sense --- it can be viewed purely as a computational construct. However for cases where the DA is intrinsically interesting, the DA algorithm does incidentally obtain joint draws from $p(\phi,\theta|y)$. For our purposes, $\theta$ is a nuisance parameter.

The EM algorithm of \citeasnoun{dempster1977maximum} and its variants are closely analogous to DA algorithms --- the DA algorithm can be viewed as a stochastic version of the EM algorithm. In fact there is a long history of using methods typically used to speed up one algorithm to speed up the other; \citeasnoun{van2010cross} shows how much overlap in the two literatures exists. The main advantage of DA and EM algorithms is their ease of implementation, but much of this work is necessary because DA and EM algorithms can often be prohibitively slow. One well known method of improving mixing and convergence in MCMC samplers is reparameterizing the model. \citeasnoun{papaspiliopoulos2007general} is a good summary. Most of the work in some way focuses on what are called centered and noncentered parameterizations. In our general notation where $\phi$ is the parameter, $\theta$ is the DA and $y$ is the data, the parameterization $(\phi,\theta)$ is a {\it centered parameterization} (CP) if $p(y|\theta,\phi)=p(y|\theta)$. The parameterization is a {\it noncentered parameterization} (NCP) if $p(\theta|\phi)=p(\theta)$. When $(\phi,\theta)$ is a CP, $\theta$ is called a {\it centered augmentation} (CA) for $\phi$ and when $(\phi,\theta)$ is a NCP, $\theta$ is called a {\it noncentered augmentation} (NCA) for $\phi$. A centered augmentation is sometimes called a {\it sufficient augmentation} (SA) and a noncentered augmentation is sometimes called an {\it ancillary augmentation} (AA)\cite{yu2011center}. Like \citeasnoun{yu2011center}, we prefer the latter terminology because it immediately suggests the intuition that a sufficient augmentation is like a sufficient statistic while an ancillary augmentation is like an ancillary statistic and hence Basu's theorem suggests that they are conditionally independent given $\phi$. 

The key reasoning behind the emphasis on SAs and AAs is that typically when the DA algorithm based on the SA has nice mixing and convergence properties the DA algorithm based on the AA has poor mixing and convergence properties and vice-versa. This property suggests that there might be a way to combine the two DA algorithms, or the two underlying parameterizations, in order to construct an improved sampler. Some work focuses on using partially noncentered parameterizations that are a sort of bridge between the CP and NCP, e.g. \citeasnoun{papaspiliopoulos2007general} for general hierarchical models and \citeasnoun{fruhwirth2004efficient} in the context of a particular DLM --- a dynamic univariate regression with a stationary AR(1) coefficient. Another suggestion by \citeasnoun{papaspiliopoulos2007general} is to alternate between the two augmentations within a Gibbs sampler. Suppose we have a second distinct DA $\gamma$ such that $\int_\Gamma p(\phi,\gamma|y)d\gamma = p(\phi|y)$. Then the {\it alternating} algorithm for sampling from $p(\phi|y)$ is as follows:
\begin{alg}Alternating.\label{alg:Alt} \\
  \begin{center}
    \begin{tabular}{lllllll}
  $[\theta|\phi^{(k)}]$& $\to$& $[\phi|\theta]$& $\to$& $[\gamma|\phi]$& $\to$& $[\phi^{(k+1)}|\gamma]$
    \end{tabular}
  \end{center}
\end{alg}
\noindent Essentially, one iteration of the alternating algorithm consists of one iteration of the DA algorithm based on $\theta$ followed by one iteration of the DA algorithm based on $\gamma$. 

Recall that a typical problem with slow MCMC is that there is high autocorrelation in the Markov chain for $\phi$, $\{\phi^{(k)}\}_{k=1}^K$, leading to imprecise estimates of $\mathrm{E}[f(\phi)|y]$ for some function $f$ integrable with respect to the posterior of $\phi$. Our goal is to reduce this dependence. In the usual DA algorithm (Algorithm \ref{alg:DA}) when $\phi$ and $\theta$ are highly dependent in the joint posterior the draws from $p(\theta|\phi,y)$ and then from $p(\phi|\theta,y)$ will hardly move the chain which results in high autocorrelation. In an alternating algorithm, there are essentially two chances to substantially move the chain -- one for $\theta$ in the draws from $p(\theta|\phi,y)$ and from $p(\phi|\theta,y)$ and one for $\gamma$ in the draws from $p(\gamma|\phi,y)$ and from $p(\phi|\gamma,y)$.  In particular, when $\theta$ is an SA and $\gamma$ is an AA or vice versa, then it is often the case that there is low dependence either between $\phi$ and $\theta$ or between $\phi$ and $\gamma$. As a result, either steps 1 and 2 or steps 3 and 4 will substantially move the chain in the alternating algorithm, which limits how poorly the algorithm can perform.

One recent advance that is similar to an alternating strategy is the notion of {\it interweaving} the two DAs together \cite{yu2011center}. Suppose that given $(\phi,\theta,\gamma,y)$ we further require that they have a full joint but possibly singular distribution and that the conditional distribution $\mu_y$ of $(\phi,\theta,\gamma|y)$ is defined almost everywhere such that for any measurable $A\in \Phi$, $\int_{\Theta \times \Gamma \times A}d\mu_y = \int_Ap(\phi|y)d\phi$. Then a general interweaving strategy (GIS) is an MCMC algorithm that obtains $\phi^{(k+1)}$ from $\phi^{(k)}$ as follows:
\begin{alg}GIS.\label{alg:GIS}
  \begin{align*}
    [\theta|\phi^{(k)}] \to [\gamma|\theta] \to [\phi^{(k+1)}|\gamma].
  \end{align*}
\end{alg}
\noindent The GIS algorithm obtains the $k+1$st iteration of the parameter vector $\phi$ from the $k$th iteration in three steps. First, it draws the DA $\theta$ conditional on $\phi^{(k)}$ (and the data). Next, it draws the DA $\gamma$ conditional on $\theta$. Finally, it draws $\phi^{(k+1)}$ conditional on $\gamma$. This looks similar to the usual DA algorithm, except a second DA is ``weaved'' in between the draw of the first DA and of the parameter vector. Step two of the GIS algorithm is typically accomplished by sampling $\phi|\theta,y$ and then $\gamma|\theta,\phi,y$. In addition, $\gamma$ and $\theta$ are often, but not always, one-to-one transformations of each other conditional on $(\phi,y)$, i.e. $\gamma = M(\theta;\phi,y)$ where $M(.;\phi,y)$ is a one-to-one function. This is the source of the potential singularity in $\mu_y$. If we expand out step two, then the algorithm becomes:
\begin{alg}GIS expanded.\label{alg:GIS2}\\
  \begin{center}
    \begin{tabular}{lllllll}
      $[\theta|\phi^{(k)}]$& $\to$& $[\phi|\theta]$& $\to $&$[\gamma|\theta,\phi]$& $\to$& $[\phi^{(k+1)}|\gamma]$
    \end{tabular}
  \end{center}
\noindent \end{alg}
When $\gamma$ is a one-to-one transformation of $\theta$, step 3 is an update $\gamma=M(\theta;\phi,y)$. The key difference between GIS and the correpsonding alternating algorithm (Algorithm \ref{alg:Alt}) can be seen in step 3 of Algorithm \ref{alg:GIS2}: instead of drawing from $p(\gamma|\phi,y)$, the GIS algorithm draws from $p(\gamma|\theta,\phi,y)$, ``weaving'' the two DAs together, while the alternating algorithm keeps them separate.

\citeasnoun{yu2011center} call a GIS approach where one of the DAs is an SA and the other is an AA an {\it ancillary sufficient interweaving strategy}, or an ASIS. They show that the GIS algorithm has a geometric rate of convergence no worse than the worst of the two underlying algorithms and in some cases better than the the corresponding alternating algorithm. In particular, their Theorem 1 suggests that the weaker the dependence between two data augumentations in the posterior, the more efficient the GIS algorithm. With a posteriori independent data augmentations, the GIS algorithm will even obtain iid draws from the posterior density of the model parameter. This helps motivate their focus on ASIS --- conditional on the model parameter, an SA and an AA are independent under the conditions of Basu's theorem \cite{basu1955statistics}, which suggests that the dependence between the two DAs will be limited in the posterior. In fact, when the prior on $\phi$ is nice in some sense, \citeasnoun{yu2011center} show that the ASIS algorithm is the same as the optimal PX-DA algorithm of \citeasnoun{meng1999seeking}, \citeasnoun{liu1999parameter}, \citeasnoun{van2001art} and \citeasnoun{hobert2008theoretical}. \matt{TALK IN MORE DETAIL ABOUT THIS -- VIVEK'S CRITICISMS AND EXPLAINING WHAT PX-DA IS} Their results suggest that ASIS and interweaving generally is a promising approach to improve the speed of MCMC in a variety of models no matter what region of the parameter space the posterior is concentrated. The exploitation of ASIS in particular looks more promising than the well known alternating algorithms.

To gain some intuition about why interweaving works, recall that a typical problem with slow MCMC is that there is high autocorrelation in the Markov chain for $\phi$, $\{\phi^{(k)}\}_{k=1}^K$, leading to imprecise estimates of $\mathrm{E}[f(\phi)|y]$ for some function $f$ integrable with respect to the posterior of $\phi$. Our goal is to reduce this dependence. In the usual DA algorithm (Algorithm \ref{alg:DA}) when $\phi$ and $\theta$ are highly dependent in the joint posterior the draws from $p(\theta|\phi,y)$ and then from $p(\phi|\theta,y)$ will hardly move the chain. The upshot is high autocorrelation in the chain. Interweaving helps break this autocorrelation in two ways. First, by inserting the extra step, the chain gets an additional chance to move in a single iteration thereby weakening the autocorrelation. This is a feature of an alternating algorithm as well, but \citeasnoun{yu2011center} show that the corresponding interweaving algorithm is often even more efficient. The key is the second point --- when the posterior dependence between the two DAs is low, step 2 in Algorithm \ref{alg:GIS} (i.e. steps 2 and 3 in Algorithm \ref{alg:GIS2}) is enough to almost completely break the dependence in the chain. For the alternating algorithm, it is typically not feasible to find a data augmentation such that step 2 or step 3 of Algorithm \ref{alg:Alt} completely breaks the dependence in the chain --- this would require finding a DA such that the model parameter and the DA are essentially independent which, in turn, would likely mean that drawing from the conditional posterior of the parameter given the DA is nearly as difficult as drawing from the marginal posterior of the model parameter.
 
Aside from the intuition of finding a posteriori (nearly) independent DAs, both alternating and interweaving strategies suggest looking for a ``beauty and the beast'' pair of DAs --- specifically both strategies for combining DAs will tend to do better, all else equal, when the two underlying DA algorithms are efficient in opposite regions of the parameter space. \matt{Is this really true? Imagine two DA algorithms that perform well in the same region, will interweaving them help?}

The particular method of interweaving discussed above is called a GIS or a {\it global} interweaving strategy since interweaving occurs globally across the entire parameter vector. It is possible to define a {\it componentwise} interweaving strategy (CIS) that interweaves within specific steps of a Gibbs sampler as well. A CIS algorithm for $\phi=(\phi_1, \phi_2)$ essentially employs interweaving for each block of $\phi$ separately, e.g.
\begin{alg}CIS.\label{alg:CIS}\\
  \begin{center}
    \begin{tabular}{llllll}
      $[\theta_1|\phi_1^{(k)},\phi_2^{(k)}]$ & $\to$  & $[\gamma_1|\phi_2^{(k)},\theta_1]$ & $\to$ & $[\phi_1^{(k+1)}|\phi_2^{(k)},\gamma_1]$ &$\to$ \\
      $[\theta_2|\phi_1^{(k+1)},\phi_2^{(k)},\gamma_1]$ &$\to$ & $[\gamma_2|\phi_1^{(k+1)},\theta_2]$ & $\to$ & $[\phi_2^{(k+1)}|\phi_1^{(k+1)},\gamma_2]$ &
    \end{tabular}
  \end{center}
\noindent \end{alg}
where $\theta_i$ and $\gamma_i$ are distinct data augmentations for $i=1$ and $i=2$, but potentially $\gamma_1=\theta_2$  or $\gamma_2=\theta_1$. The first line draws $\phi_1$ conditional on $\phi_2$ using interweaving in a Gibbs step, while the second line does the same for $\phi_2$ conditional on $\phi_1$. The algorithm can easily be extended to greater than two blocks within $\phi$. The main attraction of CIS is that it is often easier to find an AA--SA pair of DAs for $\phi_1$ conditional on $\phi_2$ and another pair for $\phi_2$ conditional on $\phi_1$ than it is to find and AA--SA pair for $\phi=(\phi_1,\phi_2)$ jointly. 

Most of the work on constructing and using DAs has focused on multilevel models --- e.g. \citeasnoun{van2001art} and \citeasnoun{papaspiliopoulos2007general}, but relatively little attention has been paid to time series models despite strong similarities between some time series models and the hierarchical models typically studied. We seek to improve DA schemes in a particular class of models --- linear, Gaussian statespace models, a.k.a. dynamic linear models (DLMs). Some examples of papers which do explore reparameterization in time series models include \citeasnoun{strickland2008parameterisation}, \citeasnoun{fruhwirth2006auxiliary}, \citeasnoun{bos2006inference}, \citeasnoun{fruhwirth2008heston}, \citeasnoun{kastner2013ancillarity}, and \citeasnoun{fruhwirth2004efficient}. Most of these papers focus on non-Gaussian statespace models --- in particular the stochastic volatility model, but few directly work with the class of DLMs we consider.\matt{STILL NEED SUMMARY OF WHAT THESE DO} We characterize this class of models in the next section.

The rest of the paper is organized as follows. In Section \ref{sec:DLMs} we introduce the dynamic linear model, discuss the subclass of DLMs we consider, and explore some of the key properties of the model. Section \ref{sec:DAs} explores several possible DAs for our class of DLMs, including several new DAs, and shows that any SA for the DLM is likely to be difficult to use. Section \ref{sec:Algs} discusses the various MCMC strategies available for the DLM, including several DA algorithms, alternating algorithms, and interweaving algorithms. Section \ref{sec:LLM} applies these algorithms to a particular DLM --- the local level model --- and discusses some interesting problems which arise during the construction of these algorithms before using a simulation to empirically examine how well various algorithms perform across different regions of the parameter space. Finally, Section \ref{sec:Discuss} discusses these results and what they might mean for more general DLMs.

\section{Dynamic linear models} \label{sec:DLMs} \matt{references to West and Harrison, Petris et al, and Prado and West}

The general DLM is
\begin{align}
y_t &= F_t\theta_t + v_t && v_t \stackrel{ind}{\sim} N_k(0,V_t) && (\mbox{observation equation}) \label{dlmtdobseq}\\
 \theta_t &= G_t\theta_{t-1} + w_t && w_t \stackrel{ind}{\sim} N_p(0,W_t) && (\mbox{system equation}) \label{dlmtdsyseq}
\end{align}
where $N_d(\mu,\Sigma)$ is a $d$-dimensional multivariate normal distribution with mean $\mu$ and covariance $\Sigma$ and the observation errors, $v_{1:T}$, and system disturbances, $w_{1:T}$, are assumed independent. The observed data is $y_{1:T} = (y_1',y_2,\cdots y_T')'$ while $\theta_{0:T}=(\theta_0',\theta_1',\theta_2',\cdots \theta_T')'$ is called the latent states and is the usual DA for this model. However, we will treat $\theta_0$ as a model parameter and $\theta_{1:T}$ as the DA. For each $t=1,2,\cdots,T$, $F_t$ is a $k\times p$ matrix and $G_t$ is a $p\times p$ matrix. Let $\phi$ denote the vector of unknown parameters in the model. Then possibly $F_{t}$, $G_{t}$, $V_{t}$, and $W_{t}$ are all functions of $\phi$ for $t=1,2,\cdots,T$. 

The subclass of DLMs we will focus on sets $V_t=V$ and $W_t=W$ and treats $F_{t}$ and $G_{t}$ as known for all $t$. Our results can be extended when $V_t$ or $W_t$ is time-varying or when $F_t$ or $G_t$ depend on unknown parameters, but we ignore those cases for simplicity. When $\phi=(\theta_0,V,W)$ is our unknown parameter vector and we can write the model as
\begin{align}
  y_t|\theta_{0:T} \stackrel{ind}{\sim} & N(F_t\theta_t,V) &
  \theta_t|\theta_{0:t-1}  \sim & N(G_t\theta_{t-1},W) \label{dlmbotheqs}
\end{align}
for $t=1,2,\cdots T$ To complete the model specification in a Bayesian context, we need priors on $\theta_0$, $V$, and $W$. We will use the standard approach and assume that they are mutually independent a priori and that $\theta_0 \sim N(m_0, C_0)$, $V \sim IW(\Lambda_V, \lambda_V)$ and $W \sim IW(\Lambda_W, \lambda_W)$ where $m_0$, $C_0$, $\Lambda_V$, $\lambda_V$, $\Lambda_W$, and $\lambda_W$ are known hyperparameters and $IW(\Lambda, \lambda)$ denotes the inverse Wishart distribution with degrees of freedom $\lambda$ and positive definite scale matrix $\Lambda$.

From equation \eqref{dlmbotheqs} we can rewrite the model by recursive substitution:
\begin{align*}
  y_t &= v_t + F_t\left(w_t + G_tw_{t-1} + G_tG_{t-1}w_{t-2} + ... + G_tG_{t-1}\cdots G_{2}w_1 + G_tG_{t-1}\cdots G_1\theta_0\right).
\end{align*}
Here we see that $\theta_0$ is given a special status relative to the other elements of the data augmentation which helps motivate treating it as a model parameter rather than part of the DA. Now each $y_t$ is a linear combination of normal random variables conditional on $\phi=(\theta_0,V,W)$, so $y_{1:T}$ has a normal distribution after marginalizing out $\theta_{1:T}$ such that
\begin{align*}
  \mathrm{E}[y_t|\phi] =  F_t\left[\prod_{s=t}^1G_s\right]\theta_0,\qquad
  \mathrm{Var}[y_t|\phi] =  V + F_tH_tWH_t'F_t',\qquad
  \mathrm{Cov}[y_s,y_t|\phi] = F_sH_sWH_t'F_t'
\end{align*}
where $\prod_{s=t}^1G_s = G_tG_{t-1}\cdots G_1$ and $H_t = I_p + G_t + G_tG_{t-1} + \cdots + G_tG_{t-1}\cdots G_2$. Next define $D_t = F_tG_tG_{t-1}\cdots G_1$. Then let
\begin{align*}
D_{Tp\times Tk} &= \begin{bmatrix} 
D_1 & 0 & \ddots & 0\\
0 & D_2 & \ddots & 0\\
\ddots & \ddots & \ddots & \ddots\\
0 & 0 & \ddots & D_T 
\end{bmatrix}, &
\tilde{V}_{Tk\times Tk} & = \begin{bmatrix} 
V & 0 & \ddots & 0\\
0 & V & \ddots & 0\\
\ddots & \ddots & \ddots & \ddots\\
0 & 0 & \ddots & V 
\end{bmatrix},
\intertext{ }
\mu_{Tp\times 1} &= \begin{bmatrix} \theta_0 \\ \theta_0 \\ \vdots \\ \theta_0\end{bmatrix}, & \tilde{W}_{Tp\times Tp} &= \begin{bmatrix} F_1H_1 \\ F_2H_2 \\ \vdots \\ F_TH_T \end{bmatrix} W \begin{bmatrix} H_1'F_1' & H_2'F_2' & \hdots H_T'F_T' \end{bmatrix}.
\end{align*} 

Then we have the data model for $y_{1:T}$ without any data augmentation:
\begin{align}
  y_{1:T} \stackrel{ind}{\sim} N_{k}(D\mu, \tilde{V} + \tilde{W}) \label{margmodel}
\end{align}
for $t=1,2,\cdots,T$. Given a prior $p(\phi)$, the posterior density we are interested in is $p(\phi|y_{1:T})\propto p(y_{1:T}|\phi)p(\phi)$.

Equation \eqref{dlmbotheqs} motivates a strong analogy with a commonly studied hierarchical model:
\begin{align*}
  y_{ij}|\beta_{1:J} \stackrel{ind}{\sim} & N(X_{ij}\beta_j,V) &
  \beta_j \stackrel{ind}{\sim} & N(Z_j\lambda,W)
\end{align*}
for $i=1,2,\cdots,I_j$ and $j=1,2,\cdots,J$. Here, $ij$ indicates observation $i$ within group $j$. The response vector $y_{ij}$ is related to the matrix of covariates $X_{ij}$ through the group-specific regression coefficient $\beta_j$ while the group specific regression coefficient is related to the group-level matrix of covariates $Z_j$ through the group level regression coefficient $\lambda$. There are three key differences between this model and the class of DLMs we consider. First, in the DLM $I_j=1$ for $j=1,2,\cdots J$ so that each observation comes from its own group. Second, the $y_{1j}$'s are no longer exchangeable and instead exhibit a temporal ordering. Finally, the $\beta_j$'s are given a special kind of dependence by forcing the mean of the distribution of $\beta_j$ to depend on $\beta_{j-1}$ instead of $\lambda$, where we define $\beta_0=\lambda$. This combined with equation \eqref{margmodel} helps motiviate our decision to treat $\theta_0$ as a model parameter rather than part of the DA --- it is analogous to the top level mean parameter in a hierarchical regression model and in addition $\theta_0$ requires its own prior distribution rather than being an implicit prior in the construction of the model, like $\theta_{1:T}$ in the DLM or $\beta_{1:J}$ is the hierarchical model.

\section{Augmenting the DLM}\label{sec:DAs}
The latent states $\theta_{1:T}$ defined in the definition of the DLM form the usual DA. We will explore several alternative DAs found through manipulating $\theta_{1:T}$ in intuitive ways. The purpose of these constructions is primarily to use as grist for the interweaving mill.

\subsection{The scaled disturbances} \matt{Isn't this augmentation implied by Durbin's simulation smoother?}

The next step is to apply the ideas of interweaving to sampling from the posterior of the dynamic linear model. \citeasnoun{papaspiliopoulos2007general} note that typically the usual parameterization results in an SA for the parameter $\phi$. All that would be necessary for an ASIS algorithm, then, is to construct an AA for $\phi$. We immediately run into a problem because the standard DA for a DLM is the latent states $\theta_{0:T}$. From equation \eqref{dlmbotheqs} and \eqref{dlmbotheqs} we see that $V$ is in the observation equation so that $\theta_{0:T}$ is not an SA for $(V,W)$ while $W$ is in the system equation so that $\theta_{0:T}$ is not an AA for $(V,W)$ either. In order to find an SA we need to somehow move $V$ from the observation equation to the system equation while leaving $W$ in the system equation. We also need to find an AA by somehow moving $W$ from the system equation to the observation equation while leaving $V$ in the observation equation. A naive thing to try is to condition on the disturbances instead of the states and see if the disturbances for an SA or an AA for $(V,W)$. The disturbances $w_{0:T}$ are defined by $w_t = \theta_t - G_t\theta_{t-1}$ for $t=1,,2,\cdots,T$ and and $w_0=\theta_0$. However it is easy to see that the conditional distributions $p(V,W|\theta_{0:T},y_{1:T})$ and $p(V,W|w_{0:T},y_{1:T})$ are identical (we omit these details) so the DA algorithm based on the $w_t$'s is identical to the algorithm based on the $\theta_t$'s.

\citeasnoun{papaspiliopoulos2007general} suggest that in order to obtain an ancillary augmentation for a variance parameter, we must scale the sufficient augmentation by the square root of that parameter. Based on this intuition, note that if we hold $V$ constant then $\theta_{0:T}$ is an SA for $W$ from equation \eqref{dlmbotheqs}, i.e. we say $\theta_{0:T}$ is an SA for $W$ given $V$, or for $W|V$. Similarly $\theta_{0:T}$ is an AA for $V|W$. This suggests that if we scale $\theta_{t}$ by $W$ appropriately for all $t$ we'll have an ancillary augmentation for $V$ and $W$ jointly. The same intuition suggests scaling $w_{t}=\theta_{t}-G_t\theta_{t-1}$ by $W$ appropriately for all $t$ in order to find an ancillary augmentation for $(V,W)$. We will work with the latter case since it has already been used in the literature. In fact it follows \citeasnoun{papaspiliopoulos2007general}'s suggestion to construct a pivotal quantity in order to find an ancillary augmentation which incidentally also buttresses the case for the terminology ``ancillary'' and ``sufficient'' augmentations rather than ``centered'' and ``non-centered''. In some DLMs the DA algorithm based on scaling $w_t$ and the DA algorithm based on scaling $\theta_t$ will be the same, but this is not generally true --- and even fails to hold for some of the simplest DLMs.

To define the scaled disturbances in the general DLM, let $L_W$ denote the Cholesky decomposition of $W$, i.e. the lower triangle matrix $L_W$ such that $L_WL_W' =W$. Then we will define the scaled disturbances $\gamma_{0:T}$ by $\gamma_0=\theta_0$ and $\gamma_t = L_W^{-1}(\theta_t-G_t\theta_{t-1})$ for $t=1,2,\cdots,T$. There are actually $p!$ different versions of the scaled disturbances depending on how we order the elements of $\theta_t$, as \citeasnoun{meng1998fast} note for EM algorithms in a different class of models. We will sidestep the issue of the best ordering of the latent states. No matter which ordering is chosen, we can confirm our intuition that the scaled disturbances are an AA for $V$ and $W$ jointly. The reverse transformation is defined recursively by $\theta_0=\gamma_0$ and $\theta_t=L_W\gamma_t + G_t\theta_{t-1}$ for $t=1,2,\cdots,T$. Then the Jacobian is block lower triangular with the identity matrix and $T$ copies of $L_W$ along the diagonal blocks, so $|J| = |L_W|^T=|W|^{T/2}$. Then from \eqref{dlmjoint} we can write the full joint distribution of $(V,W,\gamma_{0:T},y_{1:T})$ as
 \begin{align}
  p(&V,W,\gamma_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\gamma_0-m_0)'C_0^{-1}(\gamma_0-m_0)\right] \nonumber\\
  &\times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Lambda_WW^{-1}\right)\right] \exp\left[-\frac{1}{2}\gamma_t'\gamma_t\right] |V|^{-(\lambda_V + k + T + 2)/2} \nonumber\\
  &\times \exp\left[-\frac{1}{2}\left(tr\left(\Lambda_VV^{-1}\right) + \sum_{t=1}^T\left[y_t-F_t\theta_t(\gamma_{0:T},W)\right]'V^{-1}\left[y_t-F_t\theta_t(\gamma_{0:T},W)\right]\right)\right] \label{dlmdistjoint}
 \end{align}
where $\theta_t(\gamma_{0:T},W)$ denotes the recursive back transformation defined by the scaled disturbances. So ultimately under the scaled disturbance parameterization we can write the model as
\begin{align}
  y_t|\gamma_{0:T},V,W & \stackrel{ind}{\sim} N\left(F_t\theta_t(\gamma_{0:T},W), V\right)\nonumber\\
  \gamma_t & \stackrel{iid}{\sim}N(0,I_p) \label{dlmdistmodel}
\end{align}
for $t=1,2,\cdots,T$ where $I_p$ is the $p\times p$ identity matrix. Neither $V$ nor $W$ are in the system equation so the scaled disturbances are an AA for $(V,W)$. This parameterization is well known, e.g. \citeasnoun{fruhwirth2004efficient} use it in a dynamic regression model with stationary regression coefficient. 

\subsection{The scaled errors}\label{sec:scalederrors}
The scaled disturbances immediately suggest an analogous augmentation using the scaled errors, i.e. $v_t=y_t - F_t\theta_t$ appropriately scaled by $V$ in the general DLM. Let $L_V$ denote the Cholesky decomposition of $V$, that is $L_VL_V'=V$, then we can define a version of the scaled errors (this time depending on how we order the elements of the vector $y_t$) as $\psi_t = L_V^{-1}(y_t - F_t\theta_t)$ for $t=1,2,\cdots,T$ and $\psi_0 = \theta_0$ for completeness. 

It is not straightforward to write down the model in terms of $\psi_{0:T}$ instead of $\theta_{0:T}$ and determine $p(\psi_{0:T}|V,W)$. When $F_t$ is $k\times k$ (so that $dim(y_t)=k=p=dim(\theta_t)$) and is invertible for $t=1,2,\cdots,T$, $\psi_{0:T}$ is a one-to-one transformation of $\theta_{0:T}$ and the problem is easier. This restriction is not necessary --- $\theta_t$, $y_t$ or both could be augmented in order to construct an $\tilde{F}_t$ which square and invertible using a more complicated data augmentation scheme, but we pass over this issue until the discussion and assume that $F_t$ is $k\times k$ and invertible from now on. Given this assumption, $\theta_t = F_t^{-1}(y_t - L_V\psi_t)$ for $t=1,2,\cdots,T$ while $\theta_0=\psi_0$. The Jacobian of this transformation is block diagonal with a single copy of the identity matrix and the $F_t^{-1}L_V$'s along the diagonal, so $|J|=(\prod_{t=1}^T|F_t|^{-1})|V|^{T/2}$. Then from \eqref{dlmjoint} we can write the joint distribution of $(V, W, \psi_{0:T}, y_{1:T})$ as
\begin{align}
    p(&V,W,\psi_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\psi_0-m_0)'C_0^{-1}(\psi_0-m_0)\right] \nonumber\\
  &\times |V|^{-(\lambda_V + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\psi_t'\psi_t\right] \nonumber\\
   & \times |W|^{-(\lambda_W + k + T + 2)/2}\exp\left[-\frac{1}{2}\left(tr\left(\Lambda_WW^{-1}\right) + (y_t - \mu_t)'(F_tWF_t')^{-1}(y_t-\mu_t)\right)\right]\label{dlmerrorjoint}
\end{align}
where we define $\mu_1 = L_V\psi_1 + F_1G_1\psi_0$ and for $t=2,3,\cdots,T$, $\mu_t =L_V\psi_t + F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{V}\psi_{t-1})$. The $|F_t|^{-1}$'s have been absorbed into the normalizing constant, but if the $F_t$'s depended on some unknown parameter then we could not do this and as a result would have to take them into account in a Gibbs step for $F_t$. Now we can write the model in terms of the scaled error parameterization:
\begin{align*}
  y_t|V,W,\psi_{0:T},y_{1:t-1} &\sim N(\mu_t, F_tWF_t')\\
  \psi_t & \stackrel{iid}{\sim} N(0,I_k)
\end{align*}
for $t=1,2,\cdots,T$ where $I_k$ is the $k\times k$ identity matrix. Now we see immediately that the scaled errors are also an AA for $(V,W)$ since neither $V$ nor $W$ are in the system equation of this model. However, both $V$ and $W$ are in the observation equation so that $\psi_{0:T}$ is not an SA for $(V,W)$ or for either one conditional on the other.

\subsection{The ``wrongly-scaled'' DAs}
We can, however, find a couple more useful DAs to use as grist for the interweaving mill. The scaled disturbances are defined by $\gamma_t = L_W^{-1}(\theta_t - G_t\theta_{t-1})$  and the scaled errors are defined by $\psi_t = L_V^{-1}(y_t - \theta_t)$ for $t=1,2,\cdots,T$ where $L_WL_W' = W$ and $L_VL_V' = V$. Now define $\tilde{\gamma}_t=L_V^{-1}(\theta_t - G_t\theta_{t-1})$ and $\tilde{\psi}_t=L_W^{-1}(y_t - \theta_t)$ for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\tilde{\gamma}_0=\theta_0$. In other words, the ``tilde'' versions of the scaled disturbances and the scaled errors are scaled by the ``wrong'' Cholesky decomposition, hence we call them the wrongly-scaled disturbances and the wrongly-scaled errors respectively. 

First consider the wrongly-scaled disturbances, i.e. $\tilde{\gamma}_t = L_V^{-1}L_W\gamma_t= L_V^{-1}(\theta_t-G_t\theta_{t-1})$ for $t=1,2,\cdots,T$ and $\tilde{\gamma_0}=\gamma_0=\theta_0$. The reverse transformation is $\gamma_t = L_W^{-1}L_V\tilde{\gamma}_t$ and the Jacobian is block diagonal with $L_W^{-1}L_V$ along the diagonal. Thus $|J|=|L_W|^{-T}|L_V|^T=|W|^{-T/2}|V|^{T/2}$. Then from equation \eqref{dlmdistjoint} we can write the joint distribution of $(V,W,\tilde{\gamma}_{0:T},y_{1:T})$ as
 \begin{align}
  p(&V,W,\tilde{\gamma}_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\tilde{\gamma}_0-m_0)'C_0^{-1}(\tilde{\gamma}_0-m_0)\right] |V|^{-(\lambda_V + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Lambda_VV^{-1}\right)\right] \nonumber\\
  &\times  |W|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T\left(y_t - F_t\theta_t(\tilde{\gamma}_{0:T})\right)'V^{-1}\left(y_t - F_t\theta_t(\tilde{\gamma}_{0:T})\right)\right]\nonumber\\
   & \times |W|^{-(\lambda_W + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Lambda_WW^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\gamma}_t'(L_V^{-1}W(L_V^{-1})')^{-1}\tilde{\gamma}_t\right]\label{dlmdisttildejoint}
 \end{align}
Then under $\tilde{\gamma}_{0:T}$ we can write the model as
\begin{align*}
  y_t|\tilde{\gamma}_{0:T},V,W & \stackrel{ind}{\sim} N\left(F_t\theta_t(\tilde{\gamma}_{0:T}), V\right)\\
  \tilde{\gamma}_t & \stackrel{ind}{\sim}N(0,L_V^{-1}W(L_V^{-1})')
\end{align*}
for $t=1,2,\cdots,T$. Since $L_V$ is the Cholesky decomposition of $V$, the observation equation does not contain $W$. So $\tilde{\gamma}_{0:T}$ is an SA for $W|V$. Note also that since $W$ and $L_V$ are both in the system equation, $\tilde{\gamma}_{0:T}$ is not an AA for $V$ nor for $W$. 

Now consider the wrongly-scaled errors, i.e. $\tilde{\psi}_t=L_W^{-1}L_V\psi_t$\matt{=?} for $t=1,2,\cdots,T$ and $\tilde{\psi}_0=\psi_0=\theta_0$. Then $\psi_t = L_V^{-1}L_W\tilde{\psi}_t$ and the Jacobian is block diagonal with $L_V^{-1}L_W$ along the diagonal. So $|J|=|V|^{-T/2}|W|^{T/2}$ and from \eqref{dlmerrorjoint} we can write the joint distribution of $(V, W, \tilde{\psi}_{0:T}, y_{1:T})$ as
\begin{align}
    p(&V,W,\tilde{\psi}_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\tilde{\psi}_0-m_0)'C_0^{-1}(\tilde{\psi}_0-m_0)\right] \nonumber\\
   &\times |V|^{-(\lambda_V + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T\tilde{\psi}_t'(L_W^{-1}V(L_W^{-1})')^{-1}\tilde{\psi}_t\right] \nonumber\\
    & \times |W|^{-(\lambda_W + k + 2)/2}\exp\left[-\frac{1}{2}tr\left(\Lambda_WW^{-1}\right)\right] |V|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - \tilde{\mu}_t)'(F_tWF_t')^{-1}(y_t-\tilde{\mu}_t)\right]\label{dlmerrortildejoint}
 \end{align}
where we define $\tilde{\mu}_1 = L_W\tilde{\psi}_1 - F_1G_1\tilde{\psi_0}$ and for $t=2,3,\cdots,T$ $\tilde{\mu}_t =L_W\tilde{\psi}_t - F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{W}\tilde{\psi}_{t-1})$ In terms of $\tilde{\psi}_{0:T}$, the model is then:
 \begin{align*}
   y_t|V,W,\tilde{\psi}_{0:T},y_{1:t-1} &\sim N(\tilde{\mu}_t, F_tWF_t')\\
   \tilde{\psi}_t & \stackrel{iid}{\sim} N(0,L_W^{-1}V(L_W^{-1})')
\end{align*}
for $t=1,2,\cdots,T$. Since $\tilde{\mu}_t$ only depends on $W$ (through $L_W$) and not on $V$, $V$ is absent from the observation equation. Thus $\tilde{\psi}_{0:T}$ is an SA for $V|W$. Once again, since both $W$ and $V$ are in the system equation $\tilde{\psi}_{0:T}$ is not an AA for either $V$ or $W$.

\subsection{The elusive search for a sufficient augmentation}

Having found two ancillary augmentations for the DLM, we would like to find a sufficient augmentation in order to construct an ASIS for sampling from the posterior distribution. It turns out that this is no easy task. The following theorem clarifies just where the difficulty lies.

\begin{lem}\label{noSA}
Suppose $\eta$ is an SA for the DLM such that conditional on $\phi$, $\eta$ and $y$ are joint normally distributed, that is 
\begin{align*}
 \left. \begin{bmatrix}\eta \\ y \end{bmatrix}\right|\phi \sim N\left(\begin{bmatrix} \alpha_\eta \\ D\mu \end{bmatrix}, \begin{bmatrix} 
   \Omega_\eta & \Omega_{y,\eta}' \\
   \Omega_{y,\eta} & \tilde{V} + \tilde{W} \end{bmatrix}\right).
\end{align*}
Then $\tilde{\eta}=\Omega_{y,\eta}'\Omega_{\eta}^{-1}\eta$ is also an SA and
\[
\tilde{\eta}|\phi \sim N(D\mu,\tilde{V} + \tilde{W} - \Sigma)
\]
where $\Sigma=\Omega_{y,\eta}'\Omega_{\eta}^{-1}\Omega_{y,\eta}$, and $\tilde{V} + \tilde{W} - \Sigma$ is functionally independent of $\phi$, and conditional posterior of $\phi$ given $\tilde{\eta}$ can be written as
\[
p(\phi|\tilde{\eta},y) \propto p(\phi)\propto |\tilde{V} + \tilde{W} - \Sigma|^{-1/2}\exp\left[-\frac{1}{2}(\tilde{\eta} - D\mu + b)'(\tilde{V} + \tilde{W} - \Sigma)^{-1}(\tilde{\eta} - D\mu + b)\right].
\]
\end{lem}
To show this, first the normality assumption implies
\begin{align*}
  y|\eta,\phi &\sim N(D\mu + \Omega_{y,\eta}'\Omega_\eta^{-1}(\eta - \alpha_\eta), \tilde{V} + \tilde{W} - \Omega_{y,\eta}'\Omega_{\eta}^{-1}\Omega_{y,\eta})\\
  \eta|\phi &\sim N(\alpha_\eta, \Omega_\eta).
\end{align*}
Now for $\eta$ to be a sufficient augmentation we need $D\mu + \Omega_{y,\eta}'\Omega_\eta^{-1}(\eta - \alpha_\eta)$ and $\tilde{V} + \tilde{W} - \Omega_{y,\eta}'\Omega_{\eta}^{-1}\Omega_{y,\eta}$
to be independent of $\phi$. This requires that
\begin{align*}
  D\mu - \Omega_{y,\eta}'\Omega_\eta^{-1}\alpha_\eta + \Omega_{y,\eta}'\Omega_\eta^{-1}\eta  = b + A\eta
\end{align*}
where $A=\Omega_{y,\eta}'\Omega_\eta^{-1}$ and $b=D\mu - A\alpha_\eta$ must be free of $\phi$. This also implies that $A\alpha_\eta = D\mu - b$.

Then using the second equation, we now require $\Sigma = \tilde{V} + \tilde{W} - A\Omega_{\eta}A'$ free of $\phi$ which in turn implies that $\Omega_{\eta,y}$ must not be the zero matrix. This gives $A\Omega_{\eta}A' = \tilde{V} + \tilde{W} - \Sigma$. Consider $\tilde{\eta}=A\eta$, which is also a sufficient augmentation since it is just a linear transformation by a constant matrix. Then we have
\begin{align*}
y|\tilde{\eta},\phi & \sim N(b + \tilde{\eta}, \Sigma)\\
\tilde{\eta}|\phi & \sim N(D\mu - b, \tilde{V} + \tilde{W} - \Sigma)
\end{align*}
with $b$ and $\Sigma$ free of $\phi$. Thus the posterior density of $\phi$ given $\tilde{\eta}$ can be written as
\begin{align*}
  p(\phi|\tilde{\eta}, y) &\propto p(y|\tilde{\eta},\phi)p(\tilde{\eta}|\phi)p(\phi) \propto p(\tilde{\eta}|\phi)p(\phi) \\
&\propto |\tilde{V} + \tilde{W} - \Sigma|^{-1/2}\exp\left[-\frac{1}{2}(\tilde{\eta} - D\mu + b)'(\tilde{V} + \tilde{W} - \Sigma)^{-1}(\tilde{\eta} - D\mu + b)\right]. \qed
\end{align*}
The posterior desity we wish to sample from is similar to $p(\phi|\tilde{\eta},y)$, except it has $\Sigma$ equal to the zero matrix and $b$ equal to the sero vector. The upshot is that once we find an SA, in order to use it we must obtain draws from a density that appears just as hard to sample from as the posterior density we are already trying to approximate. Using $\eta$ instead of $\tilde{\eta}$ will not result in a simpler density $p(\phi|\eta,y)$. This conditional posterior can be written as
\begin{align*}
p(\phi|\eta,y) &\propto p(\phi) |\Omega_{\eta}|^{-1/2}\exp\left[-\frac{1}{2}(\eta - \alpha_{\eta})'(\Omega_{\eta})^{-1}(\eta - \alpha_{\eta})\right]\\
p(\phi|\eta,y) &\propto p(\phi) |A(\tilde{V} + \tilde{W} - \Sigma)A'|^{-1/2}\exp\left[-\frac{1}{2}(A\eta - D\mu + b)'A[A(\tilde{V} + \tilde{W} - \Sigma)A']^{-1}A'(A\eta - D\mu + b)\right]\\
\end{align*}
where again $A$, $\Sigma$ and $b$ are free of $\phi$. This density is even more complicated than $p(\phi|\tilde{\eta},y)$ unless $A=\Omega_{\eta,y}'\Omega_\eta^{-1}$ is invertible, in which case they are the same density.

Lemma \ref{noSA} does not rule out the existence of a useful sufficient augmentation --- in particular relaxing the joint normality assumption might yield something worthwhile --- but it does suggest that it will be difficult to find one. This result brings to mind \citeasnoun{van2001art}'s contention that there is an art to constructing data augmentation algorithms --- our goal is not only to find an MCMC algorithm that has nice convergence and mixing properties, but also one that is easy to implement, but this second criteria is much more difficult to quantify. 

The problem we run into is unlikely to be unique to the time series setting but rather seems driven by trying to find a sufficient augmentation for a pair of variances, one on the data level and the other on the latent data level. For example, in a hierarchical model we expect there to be similar problems finding an SA when both the observational and hierarchical variance are unknown. There is a similar problem while trying to find two data augmentations that are independent in the posterior which, by \citeasnoun{yu2011center}'s Theorem 1, would guarantee an interweaving algorithm that yields iid draws of from the posterior distribution of the model parameters. We omit the details, but unsurprisingly after making similar assumptions about the nature of the DAs in this case, the conditional posterior of $\phi$ ends of being either identical to or just as complicated as the marginal posterior of $\phi$.

\section{MCMC Strategies for the DLM}\label{sec:Algs}

A well known method to estimate the parameters in a DLM is via data augmentation (DA) \cite{fruhwirth1994data,carter1994gibbs}. The basic idea is to implement a Gibbs sampler with two blocks: 1) $\theta|\phi,y$ and 2) $\phi|\theta,y$. This DA algorithm with parameter $\phi$, augmented data $\theta$, and data $y$ obtains the $k+1$'st state of the Markov chain, $(\phi^{(k+1)},\theta^{(k+1)})$, from the $k$'th state, $\phi^{(k)}$ as follows:
\begin{alg}State Sampler for DLMs.\label{alg:DA}\\
  \begin{center}
    \begin{tabular}{lll}
      $[\theta|\phi^{(k)}]$& $\to$& $[\phi^{(k+1)}|\theta]$
    \end{tabular}
  \end{center}
\noindent \end{alg}
In the context of the DLM, the full joint distribution of $(V,W,\theta_{0:T},y_{1:T})$ is
\begin{align}
  p(&V,W,\theta_{0:T},y_{1:T}) \propto \exp\left[-\frac{1}{2}(\theta_0-m_0)'C_0^{-1}(\theta_0-m_0)\right] \nonumber\\
  &\times   |V|^{-(\lambda_V + k + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_VV^{-1}\right)\right] \exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - F_t\theta_t)'V^{-1}(y_t - F_t\theta_t)\right] \nonumber\\
   & \times |W|^{-(\lambda_W + p + T + 2)/2}\exp\left[-\frac{1}{2}\tr\left(\Lambda_WW^{-1}\right)\right]\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t-G_t\theta_{t-1})'W^{-1}(\theta_t-G_t\theta_{t-1})\right]\label{dlmjoint}
 \end{align}
where $\tr(.)$ is the matrix trace operator. The first block runs a simulation smoother which draws the latent states $\theta$ from their Gaussian conditional posterior distribution given the model parameters via forward filtering, backward sampling (FFBS) \cite{fruhwirth1994data,carter1994gibbs}, or other alternatives \cite{koopman1993disturbance,de1995simulation,mccausland2011simulation}. The second block draws $\phi=(V,W)$ from their joint conditional posterior which in this model turns out to be independent inverse Wishart distributions. In particular
\begin{align}
  V|\theta_{0:T},y_{1:T} &\sim IW\left(\Lambda_V + \sum_{t=1}^Tv_tv_t',\lambda_V + T\right) &
  W|\theta_{0:T},y_{1:T} &\sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right) \label{eq:VWcond}
\end{align}
where $v_t = y_t - F_t\theta_t$ and $w_t = \theta_t - G_t\theta_{t-1}$. We are calling this algorithm the {\it state sampler}.

As we will show in Section \ref{sec:LLMest}, the Markov chain constructed using the state sampler can mix poorly in some regions of the parameter space. For example, in the univariate local level model ($F_t=G_t=1$ for $t=1,2,\cdots,T$) and similar models it is known that if the variance of the latent states is too small relative to the variance of the data, mixing will be poor for $W$ \cite{fruhwirth2004efficient}.

The DA algorithm based on $\psi_{0:T}$ is similar to that of $\gamma_{0:T}$ except we note that simulation smoothing can be accomplished by directly applying the algorithm of \citeasnoun{mccausland2011simulation} because the precision matrix of $\psi_{0:T}$ retains the necessary tridiagonal structure. Also we mention in passing that there is a bit of symmetry here --- the joint conditional posterior of $(V,W)$ given $\gamma_{0:T}$ is from the same family of densities as that of $(W,V)$ given $\psi_{0:T}$ so that $V$ and $W$ essentially switch places. The upshot is that if we can draw from one we can draw from the other, so this part of our work has been essentially halved.


In the case of both wrongly-scaled DA algorithms, the smoothing step can be accomplished in a manner analogous to the ``correctly scaled'' DA algorithms, i.e. the scaled disturbance and scaled error algorithms. The draw from the joint conditional posterior of $(V,W)$ is from a nonstandard density that, like for the correctly scaled DA algorithms, has a certain symmetry property. Specifically $V,W|\tilde{\gamma}_{0:T},y_{1:T}$ and $W,V|\tilde{\psi}_{0:T},y_{1:T}$ have densities from the same family so that by changing which of $\tilde{\gamma}_{0:T}$ or $\tilde{\psi}_{0:T}$ is conditioned on, $V$ and $W$ essentially switch places. This class of densities is different from the correctly scaled DA case, however. We will demonstrate this through an example in Section \ref{sec:LLMest}.

We now have five DAs for general DLMs. For simplicity we'll assume that $dim(y_t)=dim(\theta_t)$ an $F_t$ invertible for $t=1,2,\cdots,T$ so that the scaled errors are easy to work with. \matt{what can we do if it isn't?} The five DAs are the states, $\theta_{0:T}$, the scaled disturbances $\gamma_{0:T}$, the scaled errors $\psi_{0:T}$, the wrongly-scaled disturbances $\tilde{\gamma}_{0:T}$, and the wrongly-scaled errors $\tilde{\psi}_{0:T}$ \matt{We may consider acronyms for these DAs so that we can quickly reference them.} This allows us to construct several GIS algorithms based on Algorithm \ref{alg:GIS2}. \matt{Reintroduce alternating and interweaving here.} The main algorithms we consider are the State-Dist, State-Error, Dist-Error, and Triple GIS algorithms. The State-Dist algorithm, for example, interweaves between the states and the scaled disturbances, while the Triple GIS algorithm interweaves between the states, the scaled disturbances, and the scaled errors. Strictly speaking the order in which we sample the DAs in the algorithm does matter, but \citeasnoun{yu2011center} note that this tends not to make much difference. \matt{Do we have any experience to add to this? It would be great if we could say: like \citeasnoun{yu2011center} we say very little difference when reordering.} We always construct our algorithms so that the DAs are used in the order they were presented earlier in this paragraph \matt{for example...}.

To illustrate the GIS algorithms, algorithm \ref{alg:SDint} is the state-dist GIS algorithm:
\begin{alg}State-Dist GIS for DLMs.\label{alg:SDint}\\
  \begin{center}
    \begin{tabular}{lllllll}
      $[\theta_{0:T}|V^{(k)},W^{(k)}]$&$\to$&$ [V,W|\theta_{0:T}]$&$\to$&$ [\gamma_{0:T}|V,W,\theta_{0:T}]$&$ \to $&$[V^{(k+1)},W^{(k+1)}|\gamma_{0:T}]$.
    \end{tabular}
  \end{center}
\end{alg}
\noindent \matt{Explain all steps.}
The third step is actually a one-to-one transformation from $\theta_{0:T}$ to $\gamma_{0:T}$.  In practice we may want to break up step 4 into two steps if it is easier to draw from the full conditionals of $V$ and $W$ rather than drawing them jointly, though this will cost us both in terms of MCMC efficiency and theoretical tractability \matt{Do we have much for theoretical resuls? Perhaps just don't mention this.} for analyzing the algorithm. 

\matt{Need a transition here. Why are you suddenly talking about a GIS algorithm? I think breaking this up into subsections will help, e.g. Base samplers followed by alternating and then interweaving samplers.} 
None of the GIS algorithms we can construct are ASIS algorithms --- none of the DAs are a SA for $(V,W)$. The states, $\theta_{0:T}$, are a SA for $W|V$ though, so this motivates a CIS algorithm. A partial CIS algorithm is immediate:
\begin{alg}Partial CIS for DLMs.\label{alg:PCIS}\\
  \begin{center}
    \begin{tabular}{lllll}
    $[\theta_{0:T}|V^{(k)},W^{(k)}]$& $\to$& $[V^{(k+1)}|W^{(k)},\theta_{0:T}]$&$\to$ & \\
    $[W|V^{(k+1)},\theta_{0:T}]$& $\to$& $[\gamma_{0:T}|V^{(k+1)},W,\theta_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$
    \end{tabular}
  \end{center}
\end{alg}
\noindent Steps 1-2 of this algorithm correspond to a Gibbs step for $V$ while steps 3-5 correspond to a Gibbs step for $W$. Step 4 is a transformation step since conditional on $V$ and $W$, $\gamma_{0:T}$ is a one-to-one transformation of $\theta_{0:T}$. This algorithm is actually the same as a version of the State-Dist interweaving algorithm with some of the steps rearranged, specifically algorithm \ref{alg:SDint}. So it should be similar in performance to a GIS algorithm.

With a little more work, we can also construct a Full CIS algorithm that also turns out to be essentially the same as another GIS algorithm. Here we employ the wrongly-scaled disturbances $\tilde{\gamma}_{0:T}$ and wrongly-scaled errors $\tilde{\psi}_{0:T}$. Now we already know that $\gamma_{0:T}$ is an AA for $W|V$ and $\tilde{\gamma}_{0:T}$ is an SA for $W|V$, so the two form an AA-SA pair for $W|V$. Similarly,  $\psi_{0:T}$ is an AA for $V|W$ while $\tilde{\psi}_{0:T}$ is an SA for $V|W$ so together they form an AA-SA pair for $V|W$. \matt{Did we make these AA and SA relationships clear previously? We should probably reference the section where we made this clear.} Now we can construct a Full CIS algorithm:
\begin{alg}Full CIS for DLMs, based on wrongly-scaled DAs.\label{alg:FCIS}\\
  \begin{center}
    \begin{tabular}{llllllll}
      $[\tilde{\psi}_{0:T}|V^{(k)},W^{(k)}]$& $\to$& $[V|W^{(k)},\tilde{\psi}_{0:T}]$& $\to$& $[\psi_{0:T}|V,W^{(k)},\tilde{\psi}_{0:T}]$& $\to$& $[V^{(k+1)}|W^{(k)},\psi_{0:T}]$& $\to$\\
      $[\tilde{\gamma}_{0:T}|V^{(k+1)},W^{(k)},\psi_{0:T}]$&$\to$& $[W|V^{(k+1)},\tilde{\gamma}_{0:T}]$& $\to$& $[\gamma_{0:T}|V^{(k+1)},W,\tilde{\gamma}_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$
    \end{tabular}
  \end{center}
\noindent \end{alg}
Steps 1-4 constitute a Gibbs step for $V$ and steps 5-8 constitute a Gibbs step for $W$. Steps 3, 5 and 7 are transformation steps --- the parameter we are drawing is a one-to-one function of the parameters we are conditioning on. It turns out that $p(V|W,\tilde{\gamma}_{0:T},y_{1:T})$ and $p(V|W,\theta_{0:T},y_{1:T})$ are the same density, and also that $p(W|V,\tilde{\psi}_{0:T},y_{1:T})$ and $p(W|V,\theta_{0:T},y_{1:T})$ are the same density. To see this \matt{put in the Appendix?}, from equation \eqref{dlmdistjoint} we can write the full conditional posterior of $V$ given $W$ and $\gamma_{0:T}$ as
 \begin{align*}
  p(&V|W,\gamma_{0:T},y_{1:T}) \propto |V|^{-(\lambda_V + k + T + 2)/2} \exp\left[-\frac{1}{2}\left(tr\left(\Lambda_VV^{-1}\right) + \sum_{t=1}^T\left[y_t-F_t\theta_t(\gamma_{0:T},W)\right]'V^{-1}\left[y_t-F_t\theta_t(\gamma_{0:T},W)\right]\right)\right].
 \end{align*}
Since $\theta_{0:T}$ is a function of $\gamma_{0:T}$ and $W$, this conditional distribution is unchanged from before when conditioning on $\theta_{0:T}$ instead of $\gamma_{0:T}$, so we have 
\begin{align*}
  V|W,\gamma_{0:T},y_{1:T} &\sim IW\left(\Lambda_V + \sum_{t=1}^T(y_t-F_t\theta_t(\gamma_{0:T},W))(y_t-F_t\theta_t(\gamma_{0:T},W))',\lambda_V + T\right),
\end{align*}
which is the same as the conditional distribution of $V|W,\theta_{0:T},y_{1:T}$ from equation \eqref{eq:VWcond}.

Similarly, from equation \eqref{dlmerrorjoint} we have the full conditional distribution of $W$ given $V$ and $\psi_{0:T}$:
\begin{align*}
    p(&W|V,\psi_{0:T},y_{1:T}) \propto  |W|^{-(\lambda_W + k + T + 2)/2}\exp\left[-\frac{1}{2}\left(tr\left(\Lambda_WW^{-1}\right) + (y_t - \mu_t)'(F_tWF_t')^{-1}(y_t-\mu_t)\right)\right]
\end{align*}
where $\mu_1 = L_V\psi_1 + F_1G_1\psi_0$ and for $t=2,3,\cdots,T$, $\mu_t =L_V\psi_t + F_tG_tF_{t-1}^{-1}(y_{t-1} - L_{V}\psi_{t-1})$. Now 
\begin{align*}
  F_1^{-1}(y_1-\mu_1)=F_1^{-1}(y_1-L_V\psi_1 = F_1G_1\psi_0)=w_1
\end{align*}
and for $t=2,3,\cdots,T$, 
\begin{align*}
  F_t^{-1}(y_t-\mu_t)=F_t^{-1}(y_t-L_V\psi_t - F_tG_tF_{t-1}^{-1}[y_{t-1}-L_V\psi_{t-1}])=\theta_t-G_t\theta_{t-1}=w_t
\end{align*}
so that 
\begin{align*}
  W|V,\psi_{0:T},y_{1:T} \sim IW\left(\Lambda_W + \sum_{t=1}^Tw_tw_t',\lambda_{W} + T\right)
\end{align*}
where we treat $w_{1:T}$ as a function of $\psi_{0:T}$, $y_{1:T}$, and $V$. This is once again the same as $p(W|V,\theta_{0:T},y_{1:T})$ from \eqref{eq:VWcond}. The upshot is that step 1 of Algorithm \ref{alg:FCIS} can be replaced with a draw from $p(\theta_{0:T}|V,W,y_{1:T})$, and any time we condition on one of the ``wrongly-scaled'' variables, we can condition on $\theta_{0:T}$ instead, yielding the following version of the same CIS algorithm:
\begin{alg}Full CIS for DLMs, based on states.\label{alg:FCIS2}\\
  \begin{center}
    \begin{tabular}{llllllll}
      $[\theta_{0:T}|V^{(k)},W^{(k)}]$& $\to $& $[V|W^{(k)},\theta_{0:T}]$& $\to$& $[\psi_{0:T}|V,W^{(k)},\theta_{0:T}]$& $\to$& $[V^{(k+1)}|W^{(k)},\psi_{0:T}]$& $\to$\\
      $[\theta_{0:T}|V^{(k+1)},W^{(k)},\psi_{0:T}]$& $\to$& $[W|V^{(k+1)},\theta_{0:T}]$& $\to$& $[\gamma_{0:T}|V^{(k+1)},W,\theta_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\gamma_{0:T}]$
    \end{tabular}
  \end{center}
\end{alg}

Compare this algorithm to the Dist-Error algorithm where we draw $V$ and $W$ separately:
\begin{alg}Dist-Error GIS for DLMs.\label{alg:Dist-Error}\\
  \begin{center}
    \begin{tabular}{llllllll}
      $[\gamma_{0:T}|V^{(k)},W^{(k)}]$& $\to $& $[V|W^{(k)},\gamma_{0:T}]$& $\to$& $[W|V,\gamma_{0:T}]$& $\to$\\
      $[\psi_{0:T}|V,W,\gamma_{0:T}]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$& $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$&
    \end{tabular}
  \end{center}
\end{alg}
\noindent If move the first draw of $W$ to the end make explicit the transformation that would be required from $\psi_{0:T}$ to $\gamma_{0:T}$, we have:
\begin{alg}Dist-Error GIS for DLMs, rearranged.\label{alg:Dist-Error2}\\
  \begin{center}
    \begin{tabular}{llllllll}
      $[\gamma_{0:T}|V^{(k)},W^{(k)}]$& $\to $& $[V|W^{(k)},\gamma_{0:T}]$& $\to$& $[\psi_{0:T}|V,W,\gamma_{0:T}]$& $\to$& $[V^{(k+1)}|W,\psi_{0:T}]$& $\to$\\
      $[W^{(k+1)}|V^{(k+1)},\psi_{0:T}]$& $\to$& $[\gamma_{0:T}|V,W,\psi_{0:T}]$&$\to$& $[W|V,\gamma_{0:T}]$.&&&
    \end{tabular}
  \end{center}
\end{alg}
\noindent Now anytime we draw $V$ conditional on $W$ and $\gamma_{0:T}$, it is the same as drawing $V$ conditional on $W$ and $\theta_{0:T}$, and similarly for $W$ conditional on $V$ and $\psi_{0:T}$ compared to $W$ conditional on $V$ and $\theta_{0:T}$, so we can replace $\gamma_{0:T}$ with $\theta_{0:T}$ in steps 1, 2, and 3, and we can replace $\psi_{0:T}$ with $\theta_{0:T}$ in steps 5 and 6, and insert the appropriate transformation step between step 4 and the new step 5. This gives us the full CIS algorithm, Algorithm \ref{alg:FCIS2}. So since these two algorithms are essentially the same up to rearranging their respective steps, we expect them to perform similarly. In fact, we will verify this intuition in Section \ref{sec:LLMest} with simulations using a particular DLM.

\matt{Need a transition here. What is the bottom line for these algorithms? How many algorithms have we introduced? We started with the 5 base samplers, then we could consider alternating and interweaving among these, so that is (5 choose 2) times 2? And then we could consider the (?) triple? Shouldn't there be 5 choose 2 triple algorithms?}


\clearpage

\bibliographystyle{ECA_jasa}  % proper bibliography style for ASA
\bibliography{dlmasis}
\end{document}




