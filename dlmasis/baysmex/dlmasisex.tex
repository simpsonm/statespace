\documentclass[graybox]{svmult}
\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
%\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage{amssymb, amsmath, graphics, graphicx, color, fullpage}
\usepackage{natbib} 
\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program
\DeclareMathOperator{\logit}{logit}
\begin{document}

\title*{Application of Interweaving in DLMs to an Exchange and Specialization Experiment}
\titlerunning{Application of Interweaving in DLMs}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\author{Matthew Simpson}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{Matthew Simpson \at Department of Economics, Iowa State University, Ames, IA 50011, \email{themattsimpson@gmail.com}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

\abstract{Markov chain Monte Carlo is often particularly challenging in dynamic models. In statespace models, the data augmentation algorithm \citep{tanner1987calculation} is a commonly used approach, e.g. \citet{fruhwirth1994data} and \citet{carter1994gibbs} in dynamic linear models. Using two data augmentations, \citet{yu2011center} introduces a method of ``interweaving'' between the two augmentations in order to construct an improved algorithm. Picking up on this, \citet{simpson2014interweaving} introduces several new augmentations for the dynamic linear model and builds interweaving algorithms based on these augmentations. In the context of a multivariate model using data from an economic experiment intended to study the disequilibrium dynamics of economic efficiency under a variety of conditions, we use these interweaving ideas and show how to implement them simply despite complications that arise because the model has latent states with a higher dimension than the data.}


\section{Introduction}\label{sec:intro}

Several innovations on the original data augmentation (DA) algorithm \citep{tanner1987calculation} have been proposed in the literature, see e.g. \citet{van2001art} for a thorough overview. One such innovation is the notion of interweaving two separate DAs together \citep{yu2011center}. This general idea has been picked up on in the dynamic setting by \citet{kastner2013ancillarity} in stochastic volatility models and \citet{simpson2014interweaving} in dynamic linear models (DLMs). We seek to illustrate the interweaving methods introduced in \citet{simpson2014interweaving} in the context of model that can be expressed either as a hierarchical DLM with equal state and data dimensions or simply a DLM with a state dimension larger than the data dimension.

Throughout this article we will use the notation $p(.|.)$ to denote the potentially conditional density of the enclosed random variables and $x_{1:T}=(x_1,\cdots,x_T)'$ when $x_t$ is a scalar and $x_{1:T}=(x_1',\cdots,x_T')'$ when $x_t$ is a column vector so that $x_{1:T}$ is also a column vector in both cases. The rest of this paper is organized as follows: Section \ref{sec:data} will describe the data which arise from a series of economics experiments, and Section \ref{sec:model} will describe the model we wish to fit to these data. Section \ref{sec:MCMC} will cover how to do MCMC in this model, including a fairly standard DA algorithm and an interweaving algorithm based on the ideas in \citet{simpson2014interweaving} and \citet{yu2011center}. Finally, Section \ref{sec:results} will contain the results of fitting the model using both algorithms and Section \ref{sec:conclusion} will briefly conclude.

\section{Data}\label{sec:data}
Economists are interested in determining the factors that affect the level of economic efficiency within an economy where economic efficiency can roughly be defined as the proportion of maximum possible dollar value of the total benefits to all actors in the economy, also known as Kaldor-Hicks efficiency and based on compensating variation \citep{kaldor1939welfare,mas1995microeconomic}. Studying this in the real world is messy and difficult in part because computing this proportion is nontrivial. In addition, most economic models only allow the analysis of equilibrium efficiency. To the extent that efficiency dynamics are studied, they are typically studied as equilibrium dynamics. Disequilibrium dynamics are difficult to study but potentially important. In order to avoid these difficulties while still learning something about the disequilibrium dynamics of efficiency, a series of laboratory experiments were designed and run by a group of experimental economists in order to explore what factors impact the disequilibrium dynamics of a small laboratory economy \citep{crockett2009exchange,kimbrough2010exchange}. What follows is a brief description of these experiments\footnote{For a more detailed description of the experimental design, see \citet{crockett2009exchange} especially, but also \citet{kimbrough2010exchange}}.

In a single session of the experiment, 2, 4, or 8 subjects are recruited to participate, depending on the treatment. Each subject sits at a computer visually isolated from the rest of the subjects.  On the computer, each subject controls an avatar in a virtual village where they can interact with the other subjects in the experiment.  At any time during the experiment, subjects can communicate with each other by typing into a chat window. Each subject in a given session has control over a house and a field within the village and can view each other subject's house and field.  The experiment runs for 40 periods, each lasting 100 seconds.  Within a period, each subject has to make a production decision and a consumption decision.  Every seventh period is a `rest' period where no production or consumption takes place, but the subjects can still communicate.  This results in 35 periods of production and consumption.

There are two types of goods in this world, each produced in a subject's field: {\it red} and {\it blue}, and two types of subjects: {\it odd} and {\it even}.  Half of the subjects are {\it odd} and half are {\it even}.  Both {\it odd} and {\it even} subjects can produce both types of goods and earn money for consuming both types of goods, but they produce and consume in different ways.  {\it odd} subjects must {\it red} and {\it blue} in a fixed proportion of 1 {\it red} for every 3 {\it blue} to earn U.S. cents. {\it even} subjects, on the other hand, must consume 2 {\it red} for every 1 {\it blue} to earn U.S. cents. However, {\it even} subjects are more effective at producing {\it blue} while {\it odd} subjects are more effective at producing {\it red}. Production occurs in the first 10 seconds of a period where each subject must decide how much of that time to devote to producing {\it red} and {\it blue} respectively using a slider on their screen. The last 90 seconds of the period is reserved for trading and consumption, though subjects have to discover that they may trade by noticing that they can use their mouse to drag and drop red and/or blue icons (representing one unit of {\it red} or {\it blue} respectively) onto another subject's house.  The maximum level of village wide production takes place when each subject spends 100\% of their time producing the good that they can produce the most efficiently, i.e. {\it odd} subjects produce only {\it red} and {\it even} subjects produce only {\it blue}.  Maximum consumption and thus maximum profit occurs when under maximum production and the subjects trade extensively with each other. In every period, the efficiency level of the village is recorded. 

A wide variety of treatments were applied to the various sessions of this experiment, including variations on group size and group formation, various levels of knowledge about the subject's own production function, allowing theft or not and if so, whether mechanisms for punishing theft are available. See \citet{crockett2009exchange} and \citet{kimbrough2010exchange} for a detailed description of these treatments. Each treatment consists of several replications --- anywhere from four to six. The challenge, then, is to model a time series of proportions that takes into account the nested structure of the replications within the treatments. To deal with the proportions, we simply transform the efficiencies to the real line using the logit transformation, i.e. $\logit(x)=\log(x/(1-x))$. In some replications of some treatments, efficiencies of 100\% or 0\% are obtained, which causes a problem the logit and other plausible transformations. We only consider the Steal treatment of \citet{kimbrough2010exchange} in order to avoid this issue and simplify a bit. This allows for a useful illustration of \citet{simpson2014interweaving} without too much additional complication. 

\section{Model}\label{sec:model}
Let $j=1,2,\cdots,J$ denote the replications of the treatment and $t=1,2,\cdots,T$ denote periods within these replications. Then let $y_{j,t}$ denote the observed logit efficiency of the $j$'th replication in the $t$'th period. Consider the following model
\begin{align}
y_{j,t} =& \mu_t + \theta_{j,t} + v_{j,t}&&\mbox{(observation equation)} \nonumber\\
\theta_{j,t}=&\theta_{j,t-1} + w_{j,t}&&\mbox{(replication level system equation)}\nonumber\\
\mu_t=&\mu_{t-1} + u_t&&\mbox{(treatment level system equation)}\label{eq:hier}
\end{align}
for $j=1,2,\cdots,J$ and $t=1,2,\cdots,T$, where $(v_{1:J,1:T},w_{1:J,1:T},u_{1:T})$ are mutually independent with $v_{j,t}\sim N(0,V_j)$, $w_{j,t}\sim N(0,W_j)$, and $u_t\sim N(0,U)$. The latent treatment level logit efficiency is represented by $\mu_{t}$, and evolves via a random walk. On the replication level, $\theta_{j,t}$ represents replication $j$'s deviation from the the treatment logit efficiency in period $t$ which also evolves over time via a random walk. Then $\mu_t + \theta_{j,t}$ is replication level latent logit efficiency. Finally $y_{j,t}$ represents the observed logit efficiency of replication $j$ in period $t$. The amount replication $j$'s path tends to differ from the treatment level path is controlled by the relative values of $W_j$ and $U$ --- the larger $W_j$ is relative to $U$, the less replication $j$'s path is affected by the treatment level path. Finally, $V_j$ represents how much of the change in logit efficiency is independent of previous changes. The relative size of $V_{j}$ compared to $W_j$ and $U$ tells us how much logit efficiency changes over time due to independent sources of error relative to the replication and treatment level evolutionary processes. So in this sense, $\mu_t + \theta_{j,t}$ can be seen as the portion of replication $j$'s logit efficiency that is carried on into the next period, or sustainable in a certain sense.

Another way to represent this model is by writing it in terms of the replication level latent logit efficiencies, $\phi_{j,t}=\mu_t + \theta_{j,t}$. Under this parameterization, the model is
\begin{align}
y_{j,t} =& \phi_{j,t} + v_{j,t}\nonumber\\
\phi_{j,t}=&\phi_{j,t-1} + w_{j,t} + u_t \label{eq:notrt}
\end{align}
for $j=1,2,\cdots,J$ and $t=1,2,\cdots,T$ where we substitute $u_t$ in for $\mu_t - \mu_{t-1}$. This representation shows us that the replication level latent logit efficiencies evolve according to a correlated random walk where $U$ controls the degree of correlation between the replications. 

Finally, if we let $\theta_t=(\mu_t,\theta_{1:J,t}')'$, $y_t=y_{1:J,t}$, $V=diag(V_1,\cdots,V_J)$, $W=diag(U,W_1,\cdots,W_J)$, and $F=[1_{J\times 1}\  \mathrm{I}_{J\times J}]$, we can write the model as a multivariate DLM:
\begin{align}
y_t|\theta_{0:T}\sim&N_J(F\theta_t,V)\nonumber\\
\theta_t|\theta_{0:(t-1)}\sim&N_{J+1}(\theta_{t-1},W)\label{eq:DLM}
\end{align}
for $t=1,2,\cdots,T$. This representation will be useful for constructing MCMC algorithms for the model. Using this representation, we need priors for the $V_j$'s, $W_j$'s, $U$, and $\theta_0$ to complete the model. We will suppose that they are independent with $\theta_0\sim N_{J+1}(m_0,C_0)$, $V_j\sim IG(a_{V_j},b_{V_j})$, $W_j\sim IG(a_{W_j},b_{W_j})$, and $U\sim IG(a_U,b_U)$. We will set $m_0=0_{J+1}$, $C_0=diag(100)$, $a_{V_j}=a_{W_j}=a_u=1.5$ and $b_{V_j}=b_{W_j}=b_U=0.25$. This prior on the variance parameters has essentially zero mass below $0.02$ and above $2$, which allows for a fairly wide range of parameter estimates relative to the scale of the data. These priors are chosen for convenience in illustrating the MCMC method of \citet{simpson2014interweaving} and for simplicity, but a simple way to use the inverse gamma priors without their well known inferential problems \citep{gelman2006prior} is to put gamma hyperpriors on the $b$ parameters rather than of fixing them. The marginal priors on the standard deviations will then be half-$t$ and in the MCMC samplers we discuss a Gibbs step will have to be added for drawing the $b$'s from a Gamma distribution. This prior is the hierarchical inverse Wishart prior of \citet{huang2013simple} in the scalar case.


\section{Markov chain Monte Carlo}\label{sec:MCMC}
We construct two separate MCMC samplers for this model. One is a naive data augmentation algorithm and the other takes advantage of the interweaving technology of \citet{yu2011center}, particularly the developments of \citet{simpson2014interweaving} for DLMs. We primarily use the DLM representation of the model, equation \eqref{eq:DLM}. 

\subsection{Naive Data Augmentation}\label{sec:gibbs}

The standard DA algorithm characterizes the posterior of $(V,W)$ by using a Gibbs sampler to draw from the posterior distribution of $(V,W,\theta_{0:T})$ \citep{tanner1987calculation}. In this particular case we are also interested in the posterior of $\theta_{0:T}$, which is common in dynamic models, but this does not change the MCMC strategy. The sampler is based on \citet{fruhwirth1994data} and \citet{carter1994gibbs} and consists of two steps, a draw from $p(\theta_{0:T}|V,W,y_{1:T})$ and a draw from $p(V,W|\theta_{0:T},y_{1:T})$. In order to construct this algorithm we need these two densities.

First, from the DLM representation of the model, equation \eqref{eq:DLM}, and the priors we can write the joint posterior density of $V$, $W$, and $\theta_{0:T}$ as
\begin{align}
p(V,W,&\theta_{0:T}|y_{1:T}) \propto |V|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(y_t - F\theta_t)'V^{-1}(y_t - F\theta_t)\right]\nonumber\\
\times&|W|^{-T/2}\exp\left[-\frac{1}{2}\sum_{t=1}^T(\theta_t - \theta_{t-1})'W^{-1}(\theta_t - \theta_{t-1})'\right]\exp\left[-\frac{1}{2}(\theta_0-m_0)'C_0^{-1}(\theta_0-m_0)\right]\nonumber\\
\times& U^{-a_{U} - 1}\exp\left[-\frac{1}{U}b_{U}\right]\prod_{j=1}^JV_j^{-a_{V_j} - 1}\exp\left[-\frac{1}{V_j}b_{V_j}\right]W_j^{-a_{W_j} - 1}\exp\left[-\frac{1}{W_j}b_{W_j}\right].\label{eq:post}
\end{align}

From here we can derive the smoothing density, or conditional posterior density of $\theta_{0:T}$. We use the method of \citet{mccausland2011simulation}, based on \citet{rue2001fast}, for drawing from this density, called the mixed Cholesky factor algorithm (MCFA) by \citet{simpson2014interweaving}. The following derivation closely follows Appendix C of \citet{simpson2014interweaving}. The full conditional density of $\theta_{0:T}$ can be written as
\begin{align*}
p(\theta_{0:T}&|V,W,y_{1:T}) \propto \exp\left[-\frac{1}{2}g(\theta_{0:T})\right]
\end{align*}
where
\begin{align*}
g(\theta_{0:T})&=\sum_{t=1}^T(y_t - F\theta_t)'V^{-1}(y_t - F\theta_t) + \sum_{t=1}^T(\theta_t - \theta_{t-1})'W^{-1}(\theta_t - \theta_{t-1}) + (\theta_0-m_0)'C_0^{-1}(\theta_0-m_0).
\end{align*}
Then $g$ has the form $g(\theta_{0:T}) = \theta_{0:T}'\Omega\theta_{0:T} - 2\theta_{0:T}'\omega + K$ where $K$ is some constant with respect to $\theta_{0:T}$, $\Omega$ is a symmetric matrix of dimension $(J+1)(T+1)$ and $\omega$ is a column vector of dimension $(J+1)(T+1)$. This gives $\theta_{0:T}|V,W,y_{1:T}\sim N_{(J+1)(T+1)}(\Omega^{-1}\omega, \Omega^{-1})$. Further, $\Omega$ is block tridiagonal since there are no cross product terms involving $\theta_t$ and $\theta_{t + k}$ where $|k|>1$. Because of this, the Cholesky factor and thus inverse of $\Omega$ can be efficiently computed leading to the Cholesky factor algorithm (CFA) \citep{rue2001fast}. Instead of computing the Cholesky factor of $\Omega$ all at once before drawing $\theta_{0:T}$ as in the CFA, the same technology can be used to draw $\theta_T$, then $\theta_t|\theta_{(t+1):T}$ recursively in a backward sampling structure, resulting in the MCFA. In simulations, the MCFA has been found to be significantly cheaper than Kalman filter based methods and often cheaper than the CFA \citep{mccausland2011simulation}.

In order to implement the algorithm, we need to first characterize the diagonal and off diagonal blocks of $\Omega$ and the blocks of $\omega$:
\begin{align*}
  \Omega_{0,0} & = C_0^{-1} + G_1'W^{-1}G_1 && \\
  \Omega_{t,t} & = F'V^{-1}F + 2W^{-1} &&  \mathrm{ for }\ \  t=1,2,\cdots T-1\\
  \Omega_{T,T} & = F'V^{-1}F + W^{-1} && \\
  \Omega_{t,t-1} & = - W_t^{-1}=\Omega_{t-1,t} &&  \mathrm{ for }\ \  t=1,2,\cdots T\\
  w_0 & = C_0^{-1}m_0 &&\\
  w_t &= F'V^{-1}y_t &&  \mathrm{ for }\ \  t=1,2,\cdots T.
\end{align*}
Now let $\Sigma_0 = \Omega_{0,0}^{-1}$, $\Sigma_t = (\Omega_{t,t} - \Omega_{t,t-1}\Sigma_{t-1}\Omega_{t-1,t})^{-1}$ for $t=1,2,\cdots,T$, $h_0 = \Sigma_0w_0$, and $h_t = \Sigma_t(w_t - \Omega_{t,t-1}h_{t-1})$ for $t=1,2,\cdots,T$. Then to complete the MCFA we perform the following draws recursively
\begin{align*}
  \theta_T \sim & N(h_T, \Sigma_T) &&\\
  \theta_{t}|\theta_{(t+1):T} \sim & N(h_t - \Sigma_t\Omega_{t,t+1}\theta_{t+1}, \Sigma_t) && \mathrm{for}\ \ t=T-1,T-2,\cdots,0.
\end{align*}

The second step of the DA algorithm requires a draw from $p(V,W|\theta_{0:T},y_{1:T})$. Recalling that $V=diag(V_1,\cdots,V_J)$ and $W=diag(U,W_1,\cdots,W_J)$, this density is
\begin{align*}
p(V,W&|\theta_{0:T},y_{1:T}) \propto U^{-a_{U}-T/2-1}\exp\left[-\frac{1}{U}\left(b_U + \frac{1}{2}\sum_{t=1}^T(\mu_t - \mu_{t-1})^2\right)\right]\\
\times&\prod_{j=1}^J V_j^{-a_{V_j} - T/2 - 1}\exp\left[-\frac{1}{V_j}\left(b_{V_j} + \frac{1}{2}\sum_{t=1}^T(y_{j,t} - \mu_t - \theta_{j,t})^2\right)\right] \\
\times& \prod_{j=1}^JW_j^{-a_{W_j} - T/2 - 1}\exp\left[-\frac{1}{W_j}\left(b_{W_j} + \frac{1}{2}\sum_{t=1}^T(\theta_{j,t} - \theta_{j,t-1})^2\right)\right].
\end{align*}
This is the product of inverse gamma densities, so a draw from this density can easily be accomplished by
\begin{align*}
V_j &\sim IG(\tilde{a}_{V_j},\tilde{b}_{V_j}) &&\mbox{for } j=1,2,\cdots,J\\
W_j &\sim IG(\tilde{a}_{W_j},\tilde{b}_{W_j}) &&\mbox{for } j=1,2,\cdots,J\\
U &\sim IG(\tilde{a}_{U},\tilde{b}_U) &&
\end{align*}
where $\tilde{a}_U = a_U + T/2$, $\tilde{b}_U = b_U + \sum_{t=1}^T(\mu_t - \mu_{t-1})^2/2$, and for $j=1,2,\cdots,J$, $\tilde{a}_{V_j} = a_{V_j} + T/2$, $\tilde{b}_{V_j} = b_{V_j} + \sum_{t=1}^T(y_{j,t} - \mu_t - \theta_{j,t})^2/2$, $\tilde{a}_{W_j}=a_{W_j} + T/2$, and $\tilde{b}_{W_j} = b_{W_j} + \sum_{t=1}^T(\theta_{j,t} - \theta_{j,t-1})^2/2$. So we can write the naive DA algorithm as follows:
\begin{enumerate}
\item Draw $\theta_{0:T}\sim N(\Omega^{-1}\omega, \Omega^{-1})$ using the MCFA.
\item Draw $U\sim IG(\tilde{a}_U,\tilde{b}_U)$. 
\item For $j=1,2,\cdots,J$ draw $V_j\sim IG(\tilde{a}_{V_j},\tilde{b}_{V_j})$ and $W_j\sim IG(\tilde{a}_{W_j},\tilde{b}_{W_j})$.
\end{enumerate}
Note that step 2 and the $2J$ substeps of step 3 can be parallelized since the draws are all independent.

\subsection{Interweaving}\label{sec:inter}
The basic idea of interweaving is to use two separate data augmentations (DAs) and ``weave'' them together \citep{yu2011center}. Suppose were have the DAs $\gamma_{0:T}$ and $\psi_{0:T}$. Then an alternating algorithm for our model consists of four steps:
\begin{align*}
[\gamma_{0:T}|V,W,y_{1:T}] \to [V,W|\gamma_{0:T},y_{1:T}] \to [\psi_{0:T}|V,W,y_{1:T}] \to [V,W|\psi_{0:T},y_{1:T}].
\end{align*}
The first two steps are simply the two steps of the DA algorithm based on $\gamma_{0:T}$ while the last two steps are the two steps of the DA algorithm based on $\psi_{0:T}$. A global interweaving strategy (GIS) using these two augmentations is very similar:
\begin{align*}
[\gamma_{0:T}|V,W,y_{1:T}] \to [V,W|\gamma_{0:T},y_{1:T}] \to [\psi_{0:T}|V,W,\gamma_{0:T},y_{1:T}] \to [V,W|\psi_{0:T},y_{1:T}].
\end{align*}
The only difference is that in step 3, we condition on $\gamma_{0:T}$ as well as $V$, $W$, and $y_{1:T}$. Often, this is a transformation using the definition of $\gamma_{0:T}$ and $\psi_{0:T}$, and not a random draw. When step 3 is a transformation, this reduces the computational cost relative to the alternating algorithm and in any case, depending on the properties of the data augmentations used, changing step 3 in this manner can also drastically improve the behavior of the Markov chain \citep{yu2011center}.

\citet{simpson2014interweaving} defines several DAs for the DLM, including the following two --- the scaled disturbances, defined by $\gamma_t = L_W^{-1}(\theta_t - \theta_{t-1})$, and the scaled errors, defined by $\psi_t = L_V^{-1}(y_t - F\theta_t)$ for $t=1,2,\cdots,T$ and $\psi_0=\gamma_0=\theta_0$ where $L_X$ denotes the lower triangular Cholesky factor of the symmetric and positive definite matrix $X$. Since the dimension of $y_t$ and $\theta_t$ are not the same, the scaled errors cannot be directly used without some additional augmentation. Another option is to use representation of the model which removes the treatment level states, given in equation \eqref{eq:notrt}. Using this is unwieldy because the full conditional posterior of $(W_{1:J},U)$ becomes complicated since the $\phi_{j,t}$'s are correlated across groups. Instead of either of those, we will take a particularly simple approach. Consider the hierarchical representation of the model given in equation \eqref{eq:hier}. For $j=1,2,\cdots,J$ define the replication level scaled disturbances as $\gamma_{j,t}=(\theta_t - \theta_{t-1})/\sqrt{W_j}$ for $t=1,2,\cdots,T$ and $\gamma_{j,0}=\theta_{j,0}$ and the replication level scaled errors as $\psi_{j,t}=(y_{j,t} - \mu_t - \theta_{j,t})\sqrt{V_j}$ for $t=1,2,\cdots,T$ and $\psi_{j,0}=\theta_{j,0}$. Now let $\gamma_t=(\mu_t,\gamma_{1:J,t}')'$ and $\psi_t=(\mu_t,\psi_{1:J,t}')'$ Then we can easily interweave between $\gamma_{0:T}$ and $\psi_{0:T}$ since these are one-to-one transformations of each other. Specifically the GIS algorithm we seek to construct is
\begin{enumerate}
\item Draw $\gamma_{0:T} \sim p(\gamma_{0:T}|V_{1:J},W_{1:J},U,y_{1:T})$.
\item Draw $(V_{1:J},W_{1:J},U) \sim p(V_{1:J},W_{1:J},U|\gamma_{0:T},y_{1:T})$
\item Transform $\gamma_{0:T}$ to $\psi_{0:T}$ and draw $(V_{1:J},W_{1:J},U) \sim p(V_{1:J},W_{1:J},U|\psi_{0:T},y_{1:T})$
\end{enumerate}
In order to complete this algorithm, we need to characterize the relevant full conditionals. First, consider the transformation from $\theta_{j,0:T}$ to $\gamma_{j,0:T}$. The Jacobian is triangular with a one and $T$ copies of $\sqrt{W_j}$ along the diagonal. So the joint posterior of $V_{1:T},W_{1:J},U$, and $\gamma_{0:T}$ is
\begin{align*}
p(&V_{1:T},W_{1:J},U,\gamma_{0:T}|y_{1:T}) \propto U^{-a_U-T/2-1}\exp\left[-\frac{1}{U}\left(b_U + \frac{1}{2}\sum_{t=1}^T(\mu_t - \mu_{t-1})^2\right)\right]\\
\times& \prod_{j=1}^JV_j^{-a_{V_j}-T/2-1}\exp\left[-\frac{1}{V_j}\left(b_{V_j} + \frac{1}{2}\sum_{t=1}^T\left(y_{j,t} - \mu_t - \gamma_{j,t} - \sqrt{W_j}\sum_{s=1}^t\gamma_{j,s}\right)^2\right)\right]W_j^{-a_{W_j}-1}\exp\left[-\frac{1}{W_j}b_{W_j}\right]\\
\times&\exp\left[-\frac{1}{2}\sum_{j=1}^J\sum_{t=1}^T\gamma_{j,t}^2\right]\exp\left[-\frac{1}{2}(m_0 - \gamma_0)'C_0^{-1}(m_0 - \gamma_0)\right].
\end{align*}
The full conditional of $\gamma_{0:T}$ is a bit more complicated than that of $\theta_{0:T}$, but we can just use the MCFA to draw from $\theta_{0:T}$'s full conditional and transform to $\gamma_{0:T}$. The full conditional of $(V_{1:J},W_{1:J},U)$ is 
\begin{align*}
p(&V_{1:T},W_{1:J},U|\gamma_{0:T},y_{1:T})=p(U|\gamma_{0:T},y_{1:T})\prod_{j=1}^Jp(V_j,W_j|\gamma_{0:T},y_{1:T}).
\end{align*}
Here $p(U|\gamma_{0:T},y_{1:T})=p(U|\theta_{0:T},y_{1:T})$, i.e. the same inverse gamma distribution as when we conditioned on $\theta_{0:T}$. However, $p(V_j,W_j|\gamma_{0:T},y_{1:T})$ is complicated and difficult to sample from efficiently. Instead of drawing $V_j$ and $W_j$ jointly, we draw from their full conditionals. It turns out that $V_j|W_j,\gamma_{0:T},y_{1:T} \sim IG(\tilde{a}_{V_j},\tilde{b}_{V_j})$, as when we conditioned on $\theta_{0:T}$. The full conditional density is of $W_j$ is still rather complicated:
\begin{align*}
p(W_j|V_j\gamma_{0:T},y_{1:T}) \propto W_j^{-a_{W_j}-1}\exp\left[-b_{W_j}\frac{1}{W_j} + c_{W_j}\sqrt{W_j} - d_{W_j}W_j\right]
\end{align*}
where
\begin{align*}
c_{W_j}=\frac{\sum_{t=1}^T(y_{j,t} - \mu_t - \gamma_{j,0})\sum_{s=1}^t\gamma_{j,t}}{V_j}\in\Re,&&d_{W_j} = \frac{\sum_{t=1}^T\left(\sum_{s=1}^t\gamma_{j,s}\right)^2}{2V_j} >0.
\end{align*}
We follow \citet{simpson2014interweaving} (Appendix E) and use an adaptive rejection sampling approach \citep{gilks1992adaptive} when this density is log concave, and otherwise we use a Cauchy approximation in a rejection sampling scheme for the density of $\log(W_j)$.

Now we need to characterize the full conditionals given $\psi_{0:T}$. The Jacobian matrix of the transformation from $\theta_{j,0:T}$ to $\psi_{j,0:T}$ is diagonal with a one and $T$ copies of $\sqrt{V_j}$ along the diagonal. So the joint posterior of $V_{1:T},W_{1:J},U$, and $\psi_{0:T}$ is
\begin{align*}
p(&V_{1:T},W_{1:J},U,\psi_{0:T}|y_{1:T}) \propto U^{-a_U-T/2-1}\exp\left[-\frac{1}{U}\left(b_U + \frac{1}{2}\sum_{t=1}^T(\mu_t - \mu_{t-1})^2\right)\right]\\
\times& \prod_{j=1}^JW_j^{-a_{W_j}-T/2-1}\exp\left[-\frac{1}{W_j}\left(b_{W_j} + \frac{1}{2}\sum_{t=1}^T\left(\Delta y_{j,t} - \Delta\mu_{t} - \sqrt{V_j}\Delta\psi_{j,t}\right)^2\right)\right]V_j^{-a_{V_j}-1}\exp\left[-\frac{1}{V_j}b_{V_j}\right]\\
\times&\exp\left[-\frac{1}{2}\sum_{j=1}^J\sum_{t=1}^T\psi_{j,t}^2\right]\exp\left[-\frac{1}{2}(m_0 - \psi_0)'C_0^{-1}(m_0 - \psi_0)\right]
\end{align*}
where we define $\Delta x_{j,t}=x_{j,t} - x_{j,t-1}$ for $t=2,3,\cdots,T$ and $\Delta x_{j,1} = x_{j,1}$ for any variable $x_{j,t}$ except in the case of $x_{j,t}=y_{j,t}$ where we define $\Delta y_{j,1} = y_{j,1} - \psi_{j,1}$. Now again
\begin{align*}
p(&V_{1:T},W_{1:J},U|\psi_{0:T},y_{1:T})=p(U|\psi_{0:T},y_{1:T})\prod_{j=1}^Jp(V_j,W_j|\psi_{0:T},y_{1:T}).
\end{align*}
Once again $p(U|\psi_{0:T},y_{1:T})=p(U|\theta_{0:T},y_{1:T})$, which is the same inverse gamma draw. In fact, the parameters $\tilde{a}_{U}$ and $\tilde{b}_U$ do not change from the $\gamma$ step to the $\psi$ step, so the second draw of $U$ is redundant and can be removed from the algorithm. The conditional density $p(V_j,W_j|\psi_{0:T},y_{1:T})$ is once again complicated and has the same form as $p(W_j,V_j|\gamma_{0:T},y_{1:T})$, i.e. it switches the positions of $V_j$ and $W_j$. So again we draw $V_j$ and $W_j$ in separate Gibbs steps, and $W_j|V_j,\psi_{0:T},y_{1:T}$ has the same inverse gamma density as $W_j|\theta_{0:T},y_{1:T}$. The density of $V_j|W_j,\psi_{0:T},y_{1:T}$ has the form
\begin{align*}
p(V_j|W_j\psi_{0:T},y_{1:T}) \propto V_j^{-a_{V_j}-1}\exp\left[-b_{V_j}\frac{1}{V_j} + c_{V_j}\sqrt{V_j} - d_{V_j}V_j\right]
\end{align*}
where
\begin{align*}
c_{V_j}=\frac{\sum_{t=1}^T\Delta\psi_{j,t}(\Delta y_{j,t} - \Delta \mu_t)}{W_j}\in\Re,&&d_{V_j} = \frac{\sum_{t=1}^T\Delta \psi_{j,t}^2}{2W_j} >0.
\end{align*}
This density has the same form as $p(W_j|V_j,\gamma_{0:T},y_{1:T})$ so the same rejection sampling strategy can be used to sample from it.

Finally we can write the GIS algorithm as follows:
\begin{enumerate}
\item Draw $\theta_{0:T}\sim N(\Omega^{-1}\omega, \Omega^{-1})$ using the MCFA.
\item Draw $U\sim IG(\tilde{a}_U,\tilde{b}_U)$. 
\item For $j=1,2,\cdots,J$:
\begin{enumerate}
\item Draw $V_j \sim IG(\tilde{a}_{V_j}, \tilde{b}_{V_j})$
\item Transform $\theta_{j,0:T}\to \gamma_{j,0:T}$ and draw $W_j\sim p(W_j|V_j,\gamma_{0:T},y_{1:T})$.
\item Transform $\gamma_{j,0:T}\to \psi_{j,0:T}$ and draw $V_j\sim p(V_j|W_j,\psi_{0:T},y_{1:T})$.
\item Draw $W_j \sim IG(\tilde{a}_{W_j}, \tilde{b}_{W_j})$.
\end{enumerate}
\end{enumerate}
Since $(U,V_1,\cdots,V_J,W_1,\cdots,W_J)$ are conditionally independent in the posterior no matter which of the DAs we use, Step 3 can be parallelized and step 2 can come before or after step 3. Steps 3.b and 3.c can both be accomplished using the rejection sampling method described from Appendix E of \citet{simpson2014interweaving}, briefly described above.

\section{Results}\label{sec:results}

We fit the model using both MCMC algorithms, running five chains for each algorithm at diverse starting points for $20,000$ iterations per chain. For both algorithms, convergence appeared to be attained for all parameters in all chains in the first $5,000$ iterations according to both trace plots and the Gelman-Rubin diagnostic \citep{brooks1998general}, so we throw away those initial draws as burn in. The GIS algorithm appeared to converge slightly slower according to the Gelman-Rubin diagnostic for some of the parameters, though this difference was not apparent in trace plots. 

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrrrrrrrrr}
  \hline
 & $V_1$ & $V_2$ & $V_3$ & $V_4$ & $V_5$ & $V_6$ & $U$ & $W_1$ & $W_2$ & $W_3$ & $W_4$ & $W_5$ & $W_6$ \\ 
  \hline
DA $n_{eff}$ & 24633 & 20656 & 20558 & 18883 & 21003 & 24897 & 14583 & 15072 & 18713 & 15137 & 10609 & 13228 & 29458 \\ 
GIS $n_{eff}$   & 44894 & 43659 & 35400 & 43843 & 23364 & 40913 & 19571 & 23706 & 23560 & 22768 & 15051 & 17753 & 29729 \\ 
DA Time & 3.08 & 3.68 & 3.70 & 4.02 & 3.62 & 3.05 & 5.21 & 5.04 & 4.06 & 5.02 & 7.16 & 5.74 & 2.58 \\ 
GIS Time & 4.85 & 4.98 & 6.15 & 4.96 & 9.31 & 5.32 & 11.12 & 9.18 & 9.24 & 9.56 & 14.46 & 12.26 & 7.32 \\ 
   \hline
\end{tabular}
\caption{Effective sample size ($n_{eff}$) and time in seconds per $1,000$ effective draws (Time) for each MCMC algorithm computed after burn in for all chains. Actual sample size is $60,000$ for each algorithm.}
\label{tab:neff}
\end{table}

There was, however, significant differences in mixing between the two algorithms. Table \ref{tab:neff} contains the effective sample size, $n_{eff}$ \citep{gelman2013bayesian}, for each parameters as well as the time in seconds to achieve an effective sample size of $1,000$ for each parameter, computed for both MCMC algorithms using all $60,000$ post burn-in iterations. The GIS algorithm has higher $n_{eff}$ for all parameters. For some parameters, e.g. $V_5$ and $W_6$, this difference is rather small. For others, such as $V_1$ and $V_2$, the GIS algorithm has an $n_{eff}$ roughly twice as large as the DA algorithm. In time per $1,000$ effective draws, however, the GIS algorithm underperforms across the board. The expensive steps to draw from $p(W_j|V_j,\gamma_{0:T},y_{1:T})$ and $p(V_j|W_j,\psi_{0:T},y_{1:T})$ are the main culprits. As the number of periods in the experiment increases, however, the GIS algorithm should look strong relative to the DA algorithm since the algorithm is able to use adaptive rejection sampling more often and the relative advantage of the improved mixing becomes more important \citep{simpson2014interweaving}. 

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr|rrrrr}
  \hline
 & Mean & 50\% & 2.5\% & 97.5\% && Mean & 50\% & 2.5\% & 97.5\% \\  
  \hline
  $V_1$ & 0.144 & 0.136 & 0.070 & 0.263 & $W_1$ & 0.101 & 0.092 & 0.042 & 0.216 \\ 
  $V_2$ & 0.086 & 0.080 & 0.040 & 0.163 & $W_2$ & 0.083 & 0.075 & 0.035 & 0.171 \\ 
  $V_3$ & 0.116 & 0.106 & 0.045 & 0.248 & $W_3$ & 0.078 & 0.072 & 0.035 & 0.158 \\ 
  $V_4$ & 0.102 & 0.095 & 0.046 & 0.196 & $W_4$ & 0.104 & 0.095 & 0.043 & 0.216 \\  
  $V_5$ & 0.208 & 0.196 & 0.075 & 0.415 & $W_5$ & 0.110 & 0.096 & 0.038 & 0.258 \\ 
  $V_6$ & 0.162 & 0.153 & 0.077 & 0.296 & $W_6$ & 0.085 & 0.076 & 0.034 & 0.188 \\ 
  &&&&& $U$ & 0.044 & 0.041 & 0.023 & 0.079 \\  
   \hline
\end{tabular}
\caption{Parameter estimates, including the posterior mean, posterior median, and a 95\% credible interval for each parameter.}
\label{tab:parests}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{code/phiplot.pdf}
\caption{Plots by replication of the observed logit efficiency ($y_{j,t}$), posterior median latent replication logit efficiency ($\phi_{j,t}$), and posterior median latent treatment logit efficiency ($\mu_t$).}
\label{fig:phiplot}
\end{figure}

Table \ref{tab:parests} contains the parameter estimates for the model. The treatment level variance appears to be smaller than both the replication and observation level variances, suggesting that changes in logit efficiency over time are driven less by treatment level dynamics and more by random noise and replication level dynamics. Figure \ref{fig:phiplot} also contains plots of each replication's observed logit efficiency trajectory, each replication's posterior median latent logit efficiency trajectory, and the treatment wide posterior median latent efficiency trajectory. The replication level latent logit efficiency follows the observed logit efficiency very closely in each case --- it is essentially a smoothed version of the observed logit efficiency. The treatment latent logit efficiency follows the observed logit efficiencies of replications 2, 4, 5, and 6 fairly closely, but replication 3 consistently underperforms the treatment average while replication 1 consistently overperforms, at least in the latter half of periods.


\section{Conclusion}\label{sec:conclusion}
\citet{simpson2014interweaving} explored the interweaving algorithms of \citet{yu2011center} for DLMs, but only implemented them in the univariate local level model. We use their approach in a model that can be represented as independent local level models conditional on a univariate sequence of latent states, or as a slightly more complicated DLM with $J$ dimensional data and $J+1$ dimensional state. This poses some problems with directly applying the methods in \citet{simpson2014interweaving}, but we show that they are easily overcome. The resulting sampler has similar convergence and improved mixing properties compared to the standard data augmentation algorithm with this particular dataset. In terms of end user time required to adequately characterize the posterior, the DA algorithm is a bit faster for this particular problem despite worse mixing, but this is largely due to an inefficient rejection sampling step in the interweaving algorithm that likely can be improved \citep{simpson2014interweaving}. This step also tends to become relatively more efficient in problems with more data as well less important relative to improved mixing so that the interweaving algorithm will eventually, with enough data, outperform the DA algorithm.

\bibliographystyle{apalike}
\bibliography{baysm.bib}
\end{document}
